<search><entry><title>Jackson实现入参多态</title><url>https://1162492411.github.io/docs/post/jackson%E5%AE%9E%E7%8E%B0%E5%85%A5%E5%8F%82%E5%A4%9A%E6%80%81/</url><categories><category>实战</category></categories><tags><tag>Spring</tag><tag>实战</tag></tags><content type="html"> 需求 在Spring项目中,我们在接收入参时,一般使用@RequestBody,将HTTP报文反序列化为具体的入参对象。但有时情况会略微复杂一些，需要将入参解析为不同的子类对象。这里我们以经典的学校人员管理系统为例，讲解如何实现该功能。 具体的需求为：该系统中的人员类型分为学生/老师，在信息录入接口中需要根据人员类型,执行不同的信息录入方法。 在各序列化框架中调研后，发现Jackson中的@JsonTypeInfo可以用来进行多态的序列化和反序列化，该注解作用在接口/类上，被用来开启多态类型的处理，对基类/接口和子类/实现类都有效。
代码实现 这里我们先给出一个简单方案。
// 定义一个抽象的父类,在这里指定入参与子类的映射关系 import com.fasterxml.jackson.annotation.JsonSubTypes; import com.fasterxml.jackson.annotation.JsonTypeInfo; import lombok.*; @Data @AllArgsConstructor @NoArgsConstructor @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = &amp;#34;type&amp;#34;, visible = true, defaultImpl = InfoReqBaseParam.class, include = JsonTypeInfo.As.EXISTING_PROPERTY) @JsonSubTypes({ @JsonSubTypes.Type(value = StudentInfoParam.class, name = &amp;#34;code_student&amp;#34;), @JsonSubTypes.Type(value = TeacherInfoParam.class, name = &amp;#34;code_teacher&amp;#34;) }) public class InfoReqBaseParam { private String type; } // 定义子类 import lombok.*; @Data @EqualsAndHashCode(callSuper = true) @AllArgsConstructor @NoArgsConstructor public class StudentInfoParam extends InfoReqBaseParam{ private String name; private String no; } //定义子类 import lombok.*; @Data @EqualsAndHashCode(callSuper = true) @AllArgsConstructor @NoArgsConstructor public class TeacherInfoParam extends InfoReqBaseParam{ private String name; private Integer classNumber; } //使用转换换后的子类 import com.fasterxml.jackson.core.JsonProcessingException; import com.fasterxml.jackson.databind.ObjectMapper; import lombok.extern.slf4j.Slf4j; import org.springframework.util.StringUtils; import org.springframework.web.bind.annotation.RequestBody; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; import java.util.Objects; @RestController @RequestMapping(&amp;#34;demo&amp;#34;) @Slf4j public class IDemoController implements IDemoApi{ @Override public void receive(@RequestBody InfoReqBaseParam req) throws JsonProcessingException { log.info(&amp;#34;接收到入参:{}&amp;#34;,new ObjectMapper().writeValueAsString(req)); if(Objects.isNull(req) || StringUtils.isEmpty(req.getType())){ return; } switch (req.getType()){ case &amp;#34;code_student&amp;#34; : log.info(&amp;#34;mockStudentMethod,object type:{}&amp;#34;, req.getClass()); break; case &amp;#34;code_teacher&amp;#34; : log.info(&amp;#34;mockTeacherMethod,object type:{}&amp;#34;, req.getClass()); break; default: log.info(&amp;#34;no method match&amp;#34;); break; } } } 这样，我们就可以通过在入参中为InfoReqBaseParam的type指定不同的值，来自动转换为不同的子类：type=code_student时将入参转换为StudentInfoParam.class;type=code_teacher时将入参转换为TeacherInfoParam.class;默认情况下将入参转换为InfoReqBaseParam。这个转换关系我们在@JsonTypeInfo与@JsonSubTypes中进行维护
入参解析代码改进 上述方案已经基本满足我们的需求,但是在实际的业务场景中，也许会出现子类对象也有type字段，为了解决这种情况，我们可以通过如下方式来满足需求：我们将type字段放在另外一个大对象中，根据大对象中的reqType字段，来将入参转换为reqParam的不同子类。
//定义父类 import com.fasterxml.jackson.annotation.JsonTypeInfo; import lombok.AllArgsConstructor; import lombok.Data; import lombok.NoArgsConstructor; @Data @AllArgsConstructor @NoArgsConstructor public class InfoReqDto&amp;lt;T extends InfoReqBaseParam&amp;gt; { private String reqType; @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, property = &amp;#34;reqType&amp;#34;, visible = true, defaultImpl = com.study.jackson.sample.resolveBySelf.InfoReqBaseParam.class, include = JsonTypeInfo.As.EXTERNAL_PROPERTY ) private T reqParam; } //定义映射关系 import com.fasterxml.jackson.annotation.JsonSubTypes; import lombok.AllArgsConstructor; import lombok.Data; import lombok.NoArgsConstructor; @Data @JsonSubTypes({ @JsonSubTypes.Type(value = StudentInfoParam.class, name = &amp;#34;code_student&amp;#34;), @JsonSubTypes.Type(value = TeacherInfoParam.class, name = &amp;#34;code_teacher&amp;#34;) }) public class InfoReqBaseParam { } // 定义子类 import lombok.*; @Data @EqualsAndHashCode(callSuper = true) @AllArgsConstructor @NoArgsConstructor public class StudentInfoParam extends InfoReqBaseParam { private String name; private String no; } // 定义子类 import lombok.*; @Data @EqualsAndHashCode(callSuper = true) @AllArgsConstructor @NoArgsConstructor public class TeacherInfoParam extends InfoReqBaseParam { private String name; private Integer classNumber; } //使用 import com.fasterxml.jackson.core.JsonProcessingException; import com.fasterxml.jackson.databind.ObjectMapper; import lombok.extern.slf4j.Slf4j; import org.springframework.util.StringUtils; import org.springframework.web.bind.annotation.RequestBody; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; import java.util.Objects; @Slf4j @RestController @RequestMapping(&amp;#34;&amp;#34;) public class IOtherController implements IOtherApi { @Override public void receiveDto(@RequestBody InfoReqDto req) throws JsonProcessingException { log.info(&amp;#34;接收到入参:{}&amp;#34;,new ObjectMapper().writeValueAsString(req)); if(Objects.isNull(req) || StringUtils.isEmpty(req.getReqType())){ return; } switch (req.getReqType()){ case &amp;#34;code_student&amp;#34; : log.info(&amp;#34;mockStudentMethod,object type:{}&amp;#34;, req.getReqParam().getClass()); break; case &amp;#34;code_teacher&amp;#34; : log.info(&amp;#34;mockTeacherMethod,object type:{}&amp;#34;, req.getReqParam().getClass()); break; default: log.info(&amp;#34;no method match&amp;#34;); break; } } } 子类映射代码改进 以上两种方式已经可以使用在实际业务场景中了，但是仍然存在一个不太优雅的地方：每次增加一个子类，就要修改@JsonSubTypes，这违反了“开闭原则”(对扩展开放、对修改封闭)，那么有什么方法可以改进吗？有以下两种方式可以实现我们想要的效果
方式一：扫描子类并注册 现在我们是对@JsonSubTypes进行硬编码的，只要想办法将其改为动态编码即可,我们可以拆成三步实现：在子类添加某个注解；使用工具扫描包含有该注解的类，生成一个class列表；将该列表注册到Jackson中
//这里只给出第三步的demo代码 ObjectMapper om = new ObjectMapper(); classList.stream().foreach(item -&amp;gt; om.registerSubTypes(item)); 方式二：自定义TypeIdResolver 仍然借助Jackson的@JsonTypeInfo，使用JsonTypeInfo.Id.CUSTOM策略，然后自定义解析规则(思路来自网友实现)
//在抽象父类添加该代码 @JsonTypeInfo(use = JsonTypeInfo.Id.CUSTOM, property = &amp;#34;type&amp;#34;) @JsonTypeIdResolver(JacksonTypeIdResolver.class) //自定义一个TypeIdResolver public class JacksonTypeIdResolver implements TypeIdResolver {private JavaType baseType; @Override public void init(JavaType javaType) { baseType = javaType;} @Override public String idFromValue(Object o) {return idFromValueAndType(o, o.getClass());}/** * 序列化时填充什么对象 */ @Override public String idFromValueAndType(Object o, Class&amp;lt;?&amp;gt; aClass) {//有出现同名类时可以用这个来做区别 JsonTypeName annotation = aClass.getAnnotation(JsonTypeName.class);if (annotation != null) {return annotation.value();} String name = aClass.getName(); String[] splits = StringUtils.split(name, &amp;#34;.&amp;#34;); String className = splits[splits.length - 1];return className;}/** * idFromValueAndType 是序列化的时候告诉序列化器怎么生成标识符 * &amp;lt;p&amp;gt; * typeFromId是反序列化的时候告诉序列化器怎么根据标识符来识别到具体类型，这里用了反射,在程序启动时，把要加载的包通过Reflections加载进来 */ @Override public JavaType typeFromId(DatabindContext databindContext, String type) { Class&amp;lt;?&amp;gt; clazz = getSubType(type); if (clazz == null) { throw new IllegalStateException(&amp;#34;cannot find class &amp;#39;&amp;#34; + type + &amp;#34;&amp;#39;&amp;#34;); } return databindContext.constructSpecializedType(baseType, clazz); } public Class&amp;lt;?&amp;gt; getSubType(String type) { Reflections reflections = ReflectionsCache.getReflections(); Set&amp;lt;Class&amp;lt;?&amp;gt;&amp;gt; subTypes = reflections.getSubTypesOf((Class&amp;lt;Object&amp;gt;) baseType.getRawClass()); for (Class&amp;lt;?&amp;gt; subType : subTypes) { JsonTypeName annotation = subType.getAnnotation(JsonTypeName.class);if (annotation != null &amp;amp;&amp;amp; annotation.value().equals(type)) {return subType;} else if (subType.getSimpleName().equals(type) || subType.getName().equals(type)) {return subType;}}return null;} @Override public String idFromBaseType() {return idFromValueAndType(null, baseType.getClass());} @Override public String getDescForKnownTypeIds() {return null;} @Override public JsonTypeInfo.Id getMechanism() {return JsonTypeInfo.Id.CUSTOM;} } //使用-父类定义 @Data @JsonTypeInfo(use = JsonTypeInfo.Id.CUSTOM, property = &amp;#34;type&amp;#34;) @JsonTypeIdResolver(JacksonTypeIdResolver.class) public class Animal {private String name; } //使用-子类(无需添加任何注解) @Data public class Dog extends Animal { private int age; } 另一种思路-自定义参数解析器 之前的思路都是在使用Jackson的前提下进行技术改造，那么对于不使用Jackson的项目，我们可以通过自行实现参数解析器来达到我们想要的效果，这里只给出来实现思路，具体代码自行实现：继承AbstractMessageConverterMethodArgumentResolver，覆写resolveArgument方法
import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter; import lombok.extern.slf4j.Slf4j; import org.apache.commons.io.IOUtils; import org.springframework.core.Conventions; import org.springframework.core.MethodParameter; import org.springframework.validation.BindingResult; import org.springframework.web.bind.MethodArgumentNotValidException; import org.springframework.web.bind.WebDataBinder; import org.springframework.web.bind.support.WebDataBinderFactory; import org.springframework.web.context.request.NativeWebRequest; import org.springframework.web.method.support.ModelAndViewContainer; import org.springframework.web.servlet.mvc.method.annotation.AbstractMessageConverterMethodArgumentResolver; import javax.servlet.http.HttpServletRequest; import java.io.IOException; import java.nio.charset.StandardCharsets; import java.util.Collections; import java.util.stream.Stream; /** * 自定义参数解析 */ @Slf4j public class xxxModelArgumentResolver extends AbstractMessageConverterMethodArgumentResolver { public SettlementApplyModelArgumentResolver() { super(Collections.singletonList(new FastJsonHttpMessageConverter())); } @Override public boolean supportsParameter(MethodParameter parameter) { return parameter.hasParameterAnnotation(xxxBaseModel.class); } @Override public Object resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) throws Exception { SettlementApplyReqModel model = resolveModel(webRequest); parameter = parameter.nestedIfOptional(); String name = Conventions.getVariableNameForParameter(parameter); WebDataBinder binder = binderFactory.createBinder(webRequest, model, name); if (model != null) { // 使用 Spring Validator 进行参数校验 this.validateIfApplicable(binder, parameter); if (binder.getBindingResult().hasErrors() &amp;amp;&amp;amp; this.isBindExceptionRequired(binder, parameter)) { throw new MethodArgumentNotValidException(parameter, binder.getBindingResult()); } } mavContainer.addAttribute(BindingResult.MODEL_KEY_PREFIX + name, binder.getBindingResult()); return model; } /** * 解析请求参数中的场景码，反序列化特定的 Model 类型 */ private SettlementApplyReqModel resolveModel(NativeWebRequest webRequest) throws IOException { HttpServletRequest request = webRequest.getNativeRequest(HttpServletRequest.class); String body = IOUtils.toString(request.getInputStream(), StandardCharsets.UTF_8); log.info(&amp;#34;参数解析请求体：{}&amp;#34;, body); JSONObject jsonObject = JSON.parseObject(body); Integer sceneCode = jsonObject.getInteger(&amp;#34;sceneCode&amp;#34;); AssertUtil.isNotNull(sceneCode, &amp;#34;场景码为必传参数！&amp;#34;); //下面根据不同的sceneCode解析不同的代码 return jsonObject.toJavaObject(xxxx.class); } }</content></entry><entry><title>MapStruct简要指南</title><url>https://1162492411.github.io/docs/post/mapstruct%E7%AE%80%E8%A6%81%E6%8C%87%E5%8D%97/</url><categories><category>实战</category></categories><tags><tag>Spring</tag><tag>实战</tag><tag>数据转换</tag></tags><content type="html"> MapStruct主要用于对象转换
原理：大致原理与lombok类似，通过注解处理器，在代码编译时为带有MapStruct注解的接口生成实现类，这种实现方式在性能方面要高于基于反射一类对象转换组件，如BeanUtils。 使用方法：引入依赖，创建接口，在接口的方法中添加注解，编译代码即可生成完整的转换代码。 下面我们以生活中常见的订单为例子进行讲解。 简单接入 引入依赖，创建接口，添加注解。我们以Maven为例，添加依赖包与编译插件
&amp;lt;properties&amp;gt; &amp;lt;org.mapstruct.version&amp;gt;1.5.2.Final&amp;lt;/org.mapstruct.version&amp;gt; &amp;lt;maven.compiler.source&amp;gt;1.8&amp;lt;/maven.compiler.source&amp;gt; &amp;lt;maven.compiler.target&amp;gt;1.8&amp;lt;/maven.compiler.target&amp;gt; &amp;lt;/properties&amp;gt; &amp;lt;dependencies&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.mapstruct&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;mapstruct&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${org.mapstruct.version}&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.mapstruct&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;mapstruct-jdk8&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${org.mapstruct.version}&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.mapstruct&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;mapstruct-processor&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${org.mapstruct.version}&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.projectlombok&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;lombok&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.18.12&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;4.13.1&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;/dependencies&amp;gt; 下面我们创建几个实体类
@Data public class Order{ private Long id; private Long no; private Date createDate; } @Data public class OrderRes { private Long id; private Long orderNo; private Date orderCreateDate; } @Mapper public interface OrderMapper { OrderMapper INSTANCE = Mappers.getMapper(OrderMapper.class); @Mappings({ @Mapping(source = &amp;#34;no&amp;#34;, target = &amp;#34;orderNo&amp;#34;) }) OrderRes dbToRes(Order order); } 然后我们使用单元测试，查看代码运行效果
public class OrderMapperTest { @Test public void personDTOToPerson() { //准备入参 OrderMapper orderMapper = OrderMapper.INSTANCE; Order order = new Order(); order.setId(222L); order.setNo(1L); order.setCreateDate(new Date()); //转换 OrderRes orderRes = orderMapper.dbToRes(order); //判断出参 Assert.assertEquals(order.getId(),orderRes.getId()); Assert.assertEquals(order.getNo(),orderRes.getOrderNo()); Assert.assertNull(orderRes.getOrderCreateDate()); } } 从上面的例子可以看出，使用MapStruct定义一个对象转换器，分为以下几步
创建一个对象转换接口，使用@Mapper注解 定义转换方法，设置需要转换的对象作为参数，返回值是转换后的对象 使用@Mapping注解方法，设置转换对应的属性，如果属性名相同，则不需要设置。 接口中定义一个属性，使用Mappers.getMapper方获取对应的实现，方便使用。 通过上面4步，就可以定义出一个对象转换器。 获取Mapper的方式 //方式一:直接使用INSTANCE @Mapper public abstract class CarMapper { public static final CarMapper INSTANCE = Mappers.getMapper(CarMapper.class); @Mapping(source = &amp;#34;numberOfSeats&amp;#34;, target = &amp;#34;seatCount&amp;#34;) CarDto carToCarDto(Car car); } //方式二:交给DI托管,但这种在Mockito的时候似乎有问题 @Mapper(componentModel = &amp;#34;spring&amp;#34;) public interface CarInjectMapper { @Mapping(source = &amp;#34;numberOfSeats&amp;#34;, target = &amp;#34;seatCount&amp;#34;) CarDto carToCarDto(Car car); } Mapping常用属性 @Repeatable(Mappings.class) @Retention(RetentionPolicy.CLASS) @Target({ElementType.METHOD, ElementType.ANNOTATION_TYPE}) public @interface Mapping { /** * 指的是目标对象或者出参对象的某字段 **/ String target(); /** * 指的是入参对象或者入参对象的某字段 **/ String source() default &amp;#34;&amp;#34;; /** * 用于进行日期格式化,将Date转换为String **/ String dateFormat() default &amp;#34;&amp;#34;; String numberFormat() default &amp;#34;&amp;#34;; /** * 用于指定出参对象的某字段的值为某一个固定的值 * eg. target=&amp;#34;name&amp;#34;,constant=&amp;#34;哈哈&amp;#34;,那么出参对象的name字段的值就是&amp;#34;哈哈&amp;#34; **/ String constant() default &amp;#34;&amp;#34;; /** * 返回值会被赋给出参对象的某字段，注意expression中的值最好采用全类名 * eg. target=&amp;#34;createDate&amp;#34;,expression=&amp;#34;java(new java.util.Date())&amp;#34;,那么出参对象的createDate字段的值就是此时此刻 **/ String expression() default &amp;#34;&amp;#34;; String defaultExpression() default &amp;#34;&amp;#34;; /** * 用于指定入参对象的某个字段是否不赋值给出参对象, * eg. source=&amp;#34;product.name&amp;#34;,ignore=&amp;#34;true&amp;#34;,那么入参对象的product对象的name字段将不会赋值给出参对象 **/ boolean ignore() default false; /** * 返回值会被赋给出参对象的某字段 **/ Class&amp;lt;? extends Annotation&amp;gt;[] qualifiedBy() default {}; /** * 返回值会被赋给出参对象的某字段 **/ String[] qualifiedByName() default {}; Class&amp;lt;?&amp;gt; resultType() default void.class; String[] dependsOn() default {}; /** * **/ String defaultValue() default &amp;#34;&amp;#34;; NullValueCheckStrategy nullValueCheckStrategy() default NullValueCheckStrategy.ON_IMPLICIT_CONVERSION; NullValuePropertyMappingStrategy nullValuePropertyMappingStrategy() default NullValuePropertyMappingStrategy.SET_TO_NULL; Class&amp;lt;? extends Annotation&amp;gt; mappingControl() default MappingControl.class; } 常用场景 基本的转换 @Data public class Order{ private Long id; private Long no; private Date createDate; } @Data public class OrderRes { private Long id; private Long orderNo; private Date orderCreateDate; } @Mapper public interface OrderMapper { OrderMapper INSTANCE = Mappers.getMapper(OrderMapper.class); @Mappings({ @Mapping(source = &amp;#34;no&amp;#34;, target = &amp;#34;orderNo&amp;#34;) }) OrderRes dbToRes(Order order); } @Generated( value = &amp;#34;org.mapstruct.ap.MappingProcessor&amp;#34;, date = &amp;#34;2022-09-06T22:43:33+0800&amp;#34;, comments = &amp;#34;version: 1.4.2.Final, compiler: javac, environment: Java 1.8.0_262 (AdoptOpenJDK)&amp;#34; ) public class OrderMapperImpl implements OrderMapper { @Override public OrderRes dbToRes(Order order) { if ( order == null ) { return null; } OrderRes orderRes = new OrderRes(); orderRes.setOrderNo( order.getNo() ); orderRes.setId( order.getId() ); return orderRes; } } @Mapper注解作用是：在build-time时，MapStruct会自动生成一个实现PersonMapper接口的类。 接口中定义的方法，在自动生成时，默认会将source对象（比如PersonDTO）中所有可读的属性拷贝到target（比如Person）对象中相关的属性，转换规则主要有以下两条：
当target和source对象中属性名相同，则直接转换,部分情况下可以自动转换，日期Date转String需要在@Mapping中使用dateFormat进行格式化，long转int会出现精度丢失，包装类型转换为基础数据类型会进行null判断，详细的转换规则见官方文档 当target和source对象中属性名不同，名字的映射可以通过@Mapping注解来指定。比如上面no映射到orderNo属性上。 多个入参对象转换为一个出参对象 MapStruct支持将多个入参对象的属性合并到一个出参对象中。例如我们需要将下单用户的信息和订单信息整合到一个订单详情对象中
@Data public class Merchant { private Long id; private String name; private Byte sex; } @Data public class Order{ private Long id; private Long no; private Date createDate; private Long recyclerId; } @Data public class OrderRes { private Long id; private Long orderNo; private Date orderCreateDate; private Long recyclerId; private String recyclerName; } @Mapper public interface OrderMapper { OrderMapper INSTANCE = Mappers.getMapper(OrderMapper.class); @Mappings({ @Mapping(source = &amp;#34;order.no&amp;#34;, target = &amp;#34;orderNo&amp;#34;), //这里注意:出参对象中有id字段,入参对象有两个id字段,因此需要显式指定映射关系 //如果出参对象中不需要id,可以先任意指定一个id映射然后添加ignore=true @Mapping(source = &amp;#34;order.id&amp;#34;, target = &amp;#34;id&amp;#34;), @Mapping(source = &amp;#34;mer.id&amp;#34;, target = &amp;#34;recyclerId&amp;#34;), @Mapping(source = &amp;#34;mer.name&amp;#34;, target = &amp;#34;recyclerName&amp;#34;) }) OrderRes dbToRes(Order order,Merchant mer); } @Generated( value = &amp;#34;org.mapstruct.ap.MappingProcessor&amp;#34;, date = &amp;#34;2022-09-06T22:43:33+0800&amp;#34;, comments = &amp;#34;version: 1.4.2.Final, compiler: javac, environment: Java 1.8.0_262 (AdoptOpenJDK)&amp;#34; ) public class OrderMapperImpl implements OrderMapper { @Override public OrderRes dbToRes(Order order, Merchant mer) { if ( order == null &amp;amp;&amp;amp; mer == null ) { return null; } OrderRes orderRes = new OrderRes(); if ( order != null ) { orderRes.setOrderNo( order.getNo() ); orderRes.setId( order.getId() ); } if ( mer != null ) { orderRes.setRecyclerId( mer.getId() ); orderRes.setRecyclerName( mer.getName() ); } return orderRes; } } public class OrderMapperTest { @Test public void personDTOToPerson() { //准备入参 OrderMapper orderMapper = OrderMapper.INSTANCE; Order order = new Order(); order.setId(222L); order.setNo(1L); order.setCreateDate(new Date()); Merchant merchant = new Merchant(); merchant.setId(333L); merchant.setName(&amp;#34;张三&amp;#34;); //转换 OrderRes orderRes = orderMapper.dbToRes(order,merchant); //判断出参 Assert.assertEquals(order.getId(),orderRes.getId()); Assert.assertEquals(order.getNo(),orderRes.getOrderNo()); Assert.assertNull(orderRes.getOrderCreateDate()); Assert.assertEquals(merchant.getId(),orderRes.getRecyclerId()); Assert.assertEquals(merchant.getName(),orderRes.getRecyclerName()); } } 将方法的返回值映射到字段 有的时候，某个字段的值无法通过简单的映射来解决，需要经过一定的逻辑，这种情况下MapStrut也是支持的
@Data public class Order{ private Long id; private Long no; } @Data public class OrderRes { private Long id; private Long orderNo; private String orderDay; private Integer status; private Date createDate; } public enum EnumOrderStatus { UN_PAYED(1, &amp;#34;待支付&amp;#34;), PAYED(2, &amp;#34;已支付&amp;#34;); //... } public class BUtil { public static String extractFromNo(Long input){ String inputString = input.toString(); return inputString.length() &amp;gt;= 8 ? inputString.substring(0,8) : &amp;#34;20220101&amp;#34;; } } @Mapper public interface OrderMapper { OrderMapper INSTANCE = Mappers.getMapper(OrderMapper.class); @Mappings({ @Mapping(source = &amp;#34;no&amp;#34;, target = &amp;#34;orderNo&amp;#34;), @Mapping(target = &amp;#34;orderDay&amp;#34;, expression = &amp;#34;java(com.study.mapstruct.sample.methodReturnToField.BUtil.extractFromNo(order.getNo()))&amp;#34;), @Mapping(target = &amp;#34;status&amp;#34;, expression = &amp;#34;java(com.study.mapstruct.sample.methodReturnToField.EnumOrderStatus.PAYED.getStatus())&amp;#34;), @Mapping(target = &amp;#34;createDate&amp;#34;, expression = &amp;#34;java(new java.util.Date())&amp;#34;) }) OrderRes dbToRes(Order order); } @Generated( value = &amp;#34;org.mapstruct.ap.MappingProcessor&amp;#34;, date = &amp;#34;2022-09-06T22:43:33+0800&amp;#34;, comments = &amp;#34;version: 1.4.2.Final, compiler: javac, environment: Java 1.8.0_262 (AdoptOpenJDK)&amp;#34; ) public class OrderMapperImpl implements OrderMapper { @Override public OrderRes dbToRes(Order order) { if ( order == null ) { return null; } OrderRes orderRes = new OrderRes(); orderRes.setOrderNo( order.getNo() ); orderRes.setId( order.getId() ); orderRes.setOrderDay( com.study.mapstruct.sample.methodReturnToField.BUtil.extractFromNo(order.getNo()) ); orderRes.setStatus( com.study.mapstruct.sample.methodReturnToField.EnumOrderStatus.PAYED.getStatus() ); orderRes.setCreateDate( new java.util.Date() ); return orderRes; } } public class OrderMapperTest { @Test public void personDTOToPerson() { //准备入参 OrderMapper orderMapper = OrderMapper.INSTANCE; Order order = new Order(); order.setId(222L); order.setNo(1L); //转换 OrderRes orderRes = orderMapper.dbToRes(order); //判断出参 Assert.assertEquals(order.getId(),orderRes.getId()); Assert.assertEquals(order.getNo(),orderRes.getOrderNo()); } } 关于这种需求，也有一些其他方式可以做到，详见执行自定义方法、向Mapper添加自定义方法、调用其他映射器、映射前后执行自定义方法， 当重名方法很多时，可以通过@Qualifer、@Named等注解来指定唯一方法
映射集合字段 MapStruct支持对集合的映射，当入参对象中的某个字段为集合时，会遍历该集合，将该字段拷贝到出参对象的字段中,Java中常用的List、Map、Set、Steam都是支持的，MapStruct对集合接口类的默认实现见官方文档-6.3。
@Data public class Order{ private Long id; private Long no; private List&amp;lt;String&amp;gt; saleItemNameList; private Set&amp;lt;String&amp;gt; subNoSet; } @Data public class OrderRes{ private Long id; private Long orderNo; private List&amp;lt;String&amp;gt; saleItemNames; private Set&amp;lt;String&amp;gt; subNoSet; } @Mapper public interface OrderMapper { OrderMapper INSTANCE = Mappers.getMapper(OrderMapper.class); @Mappings({ @Mapping(source = &amp;#34;no&amp;#34;, target = &amp;#34;orderNo&amp;#34;), @Mapping(source = &amp;#34;saleItemNameList&amp;#34;, target = &amp;#34;saleItemNames&amp;#34;) }) OrderRes dbToRes(Order order); } @Generated( value = &amp;#34;org.mapstruct.ap.MappingProcessor&amp;#34;, date = &amp;#34;2022-09-06T22:43:33+0800&amp;#34;, comments = &amp;#34;version: 1.4.2.Final, compiler: javac, environment: Java 1.8.0_262 (AdoptOpenJDK)&amp;#34; ) public class OrderMapperImpl implements OrderMapper { @Override public OrderRes dbToRes(Order order) { if ( order == null ) { return null; } OrderRes orderRes = new OrderRes(); orderRes.setOrderNo( order.getNo() ); List&amp;lt;String&amp;gt; list = order.getSaleItemNameList(); if ( list != null ) { orderRes.setSaleItemNames( new ArrayList&amp;lt;String&amp;gt;( list ) ); } orderRes.setId( order.getId() ); Set&amp;lt;String&amp;gt; set = order.getSubNoSet(); if ( set != null ) { orderRes.setSubNoSet( new HashSet&amp;lt;String&amp;gt;( set ) ); } return orderRes; } } public class OrderMapperTest { @Test public void personDTOToPerson() { //准备入参 OrderMapper orderMapper = OrderMapper.INSTANCE; Order order = new Order(); order.setId(222L); order.setNo(1L); order.setSaleItemNameList(Arrays.asList(&amp;#34;商品1&amp;#34;,&amp;#34;商品2&amp;#34;,&amp;#34;商品3&amp;#34;)); order.setSubNoSet(new HashSet&amp;lt;&amp;gt;(Arrays.asList(&amp;#34;sub001&amp;#34;,&amp;#34;sub002&amp;#34;))); //转换 OrderRes orderRes = orderMapper.dbToRes(order); //判断出参 Assert.assertEquals(order.getId(),orderRes.getId()); Assert.assertEquals(order.getNo(),orderRes.getOrderNo()); } } 在Map映射方面，MapStruct提供的功能较为有限，需要通过@MapMapping注解来完成
public @interface MapMapping { String keyDateFormat() default &amp;#34;&amp;#34;; String valueDateFormat() default &amp;#34;&amp;#34;; String keyNumberFormat() default &amp;#34;&amp;#34;; String valueNumberFormat() default &amp;#34;&amp;#34;; Class&amp;lt;? extends Annotation&amp;gt;[] keyQualifiedBy() default { }; String[] keyQualifiedByName() default { }; Class&amp;lt;? extends Annotation&amp;gt;[] valueQualifiedBy() default { }; String[] valueQualifiedByName() default { }; Class&amp;lt;?&amp;gt; keyTargetType() default void.class; Class&amp;lt;?&amp;gt; valueTargetType() default void.class; NullValueMappingStrategy nullValueMappingStrategy() default NullValueMappingStrategy.RETURN_NULL; Class&amp;lt;? extends Annotation&amp;gt; keyMappingControl() default MappingControl.class; Class&amp;lt;? extends Annotation&amp;gt; valueMappingControl() default MappingControl.class; } public interface SourceTargetMapper { @MapMapping(valueDateFormat = &amp;#34;dd.MM.yyyy&amp;#34;) Map&amp;lt;String, String&amp;gt; longDateMapToStringStringMap(Map&amp;lt;Long, Date&amp;gt; source); } 映射前后执行自定义方法 有的时候，在MapsStruct进行字段简单映射的基础上，我们还想要在映射前或映射后执行一些自定义的方法，这种也是支持的，可以通过在映射中传递上下文参数，借助@AfterMapping来实现。例如我们想要在映射后额外计算一下订单中的商品数量和是否包含子订单。
@Data public class Order{ private Long id; private Long no; private List&amp;lt;String&amp;gt; saleItemNameList; private Set&amp;lt;String&amp;gt; subNoSet; } @Data public class OrderRes{ private Long id; private Long orderNo; private List&amp;lt;String&amp;gt; saleItemNames; private Integer saleItemCount; private Set&amp;lt;String&amp;gt; subNoSet; private Boolean containsSubNo; } @Mapper public interface OrderMapper { OrderMapper INSTANCE = Mappers.getMapper(OrderMapper.class); @Mappings({ @Mapping(source = &amp;#34;no&amp;#34;, target = &amp;#34;orderNo&amp;#34;), @Mapping(source = &amp;#34;saleItemNameList&amp;#34;, target = &amp;#34;saleItemNames&amp;#34;) }) OrderRes dbToRes(Order order); @AfterMapping default void quickReqToSearchModelAfter(Order ord, @MappingTarget OrderRes des) { //设置商品数量 if(CollectionUtils.isEmpty(ord.getSaleItemNameList())){ des.setSaleItemCount(0); }else{ des.setSaleItemCount(ord.getSaleItemNameList().size()); } //设置是否包含子订单 //既然已经进行过字段映射了，其实也可以使用出参对象中的字段了 des.setContainsSubNo(!CollectionUtils.isEmpty(des.getSubNoSet())); } } @Generated( value = &amp;#34;org.mapstruct.ap.MappingProcessor&amp;#34;, date = &amp;#34;2022-09-07T22:07:41+0800&amp;#34;, comments = &amp;#34;version: 1.4.2.Final, compiler: javac, environment: Java 1.8.0_262 (AdoptOpenJDK)&amp;#34; ) public class OrderMapperImpl implements OrderMapper { @Override public OrderRes dbToRes(Order order) { if ( order == null ) { return null; } OrderRes orderRes = new OrderRes(); orderRes.setOrderNo( order.getNo() ); List&amp;lt;String&amp;gt; list = order.getSaleItemNameList(); if ( list != null ) { orderRes.setSaleItemNames( new ArrayList&amp;lt;String&amp;gt;( list ) ); } orderRes.setId( order.getId() ); Set&amp;lt;String&amp;gt; set = order.getSubNoSet(); if ( set != null ) { orderRes.setSubNoSet( new HashSet&amp;lt;String&amp;gt;( set ) ); } quickReqToSearchModelAfter( order, orderRes ); return orderRes; } } public class OrderMapperTest { @Test public void personDTOToPerson() { //准备入参 OrderMapper orderMapper = OrderMapper.INSTANCE; Order order = new Order(); order.setId(222L); order.setNo(1L); order.setSaleItemNameList(Arrays.asList(&amp;#34;商品1&amp;#34;,&amp;#34;商品2&amp;#34;,&amp;#34;商品3&amp;#34;)); order.setSubNoSet(new HashSet&amp;lt;&amp;gt;(Arrays.asList(&amp;#34;sub001&amp;#34;,&amp;#34;sub002&amp;#34;))); //转换 OrderRes orderRes = orderMapper.dbToRes(order); //判断出参 System.out.println(JSON.toJSONString(orderRes)); Assert.assertEquals(order.getId(),orderRes.getId()); Assert.assertEquals(order.getNo(),orderRes.getOrderNo()); } } 映射枚举 不知道怎么搞，官网链接有枚举A的值映射到枚举B
映射嵌套对象/对象引用 对于嵌套对象的映射也是支持的，在@Mapping的source中使用入参对象中嵌套对象的名称即可
@Data public class Order{ private Long id; private Long no; private Long recyclerId; private Merchant me; } @Data public class Merchant { private Long id; private String name; private Byte sex; } @Data public class OrderRes { private Long id; private Long orderNo; private Date orderCreateDate; private Long recyclerId; private String recyclerName; private Merchant merchantInfo; } @Mapper public interface OrderMapper { OrderMapper INSTANCE = Mappers.getMapper(OrderMapper.class); @Mappings({ @Mapping(source = &amp;#34;no&amp;#34;, target = &amp;#34;orderNo&amp;#34;), @Mapping(source = &amp;#34;me.id&amp;#34;, target = &amp;#34;recyclerId&amp;#34;), @Mapping(source = &amp;#34;me.name&amp;#34;, target = &amp;#34;recyclerName&amp;#34;), @Mapping(source = &amp;#34;me&amp;#34;, target = &amp;#34;merchantInfo&amp;#34;) }) OrderRes dbToRes(Order order); } @Generated( value = &amp;#34;org.mapstruct.ap.MappingProcessor&amp;#34;, date = &amp;#34;2022-09-07T22:33:29+0800&amp;#34;, comments = &amp;#34;version: 1.4.2.Final, compiler: javac, environment: Java 1.8.0_262 (AdoptOpenJDK)&amp;#34; ) public class OrderMapperImpl implements OrderMapper { @Override public OrderRes dbToRes(Order order) { if ( order == null ) { return null; } OrderRes orderRes = new OrderRes(); orderRes.setOrderNo( order.getNo() ); orderRes.setRecyclerId( orderMeId( order ) ); orderRes.setRecyclerName( orderMeName( order ) ); orderRes.setMerchantInfo( order.getMe() ); orderRes.setId( order.getId() ); return orderRes; } private Long orderMeId(Order order) { if ( order == null ) { return null; } Merchant me = order.getMe(); if ( me == null ) { return null; } Long id = me.getId(); if ( id == null ) { return null; } return id; } private String orderMeName(Order order) { if ( order == null ) { return null; } Merchant me = order.getMe(); if ( me == null ) { return null; } String name = me.getName(); if ( name == null ) { return null; } return name; } } public class OrderMapperTest { @Test public void personDTOToPerson() { //准备入参 OrderMapper orderMapper = OrderMapper.INSTANCE; Order order = new Order(); order.setId(222L); order.setNo(1L); order.setRecyclerId(333L); Merchant merchant = new Merchant(); merchant.setId(333L); merchant.setName(&amp;#34;张三&amp;#34;); merchant.setSex(new Byte(&amp;#34;1&amp;#34;)); order.setMe(merchant); //转换 OrderRes orderRes = orderMapper.dbToRes(order); //判断出参 System.out.println(JSON.toJSONString(orderRes)); Assert.assertEquals(order.getId(),orderRes.getId()); Assert.assertEquals(order.getNo(),orderRes.getOrderNo()); Assert.assertNull(orderRes.getOrderCreateDate()); Assert.assertEquals(merchant.getId(),orderRes.getRecyclerId()); Assert.assertEquals(merchant.getName(),orderRes.getRecyclerName()); } } 批量调用已有的映射方法 有的时候我们已经写好了单个入参对象转换为出参对象的方法，但是还需要一个批量的入参对象转换为出参对象的功能，这种情况下，直接List&amp;lt;出参对象&amp;gt; methodName(List&amp;lt;入参对象&amp;gt; xxList)即可实现。
转换支持builder构建的对象 如果builder存在，mapstruct会使用builder构建对象（默认使用set方法），父类不支持，就只会转换当前对象。 解决方法是 1.让父类支持builder ，例如使用lombok的时候加上superBuilder注解 2.用下面方法指定mapstruct不用builder
@Mapper(builder = @Builder(disableBuilder = true)) public interface TaskSearchMapping { TaskSearchMapping INSTANCE = Mappers.getMapper(TaskSearchMapping.class); TaskSearchModel dto2Model(TaskSearchDto dto); TaskSearchModel h5DtoModel(H5TaskSearchDto dto); } 其他使用场景 将Map对象转换为Object对象 通过Builder映射 通过构造器映射 逆向映射 有条件的映射 关于异常 映射规则 字段名相同的情况下，如果字段类型相同，无需代码显式声明就可以自动映射；字段类型不同时，大部分也可以自动转换 字段名不同的情况下，可以通过@Mapping注解来直接映射关系 简单的字段和集合类字段都是可以直接映射的 支持将方法的返回值映射到一个字段上，并且方式有很多种 一个字段或对象可以进行多次映射 多个入参对象转换为一个出参对象，或者一个入参对象中存在多个或多层的嵌套对象，在@Mapping注解中使使用&amp;quot;入参名.xxx&amp;quot;就可以进行映射，也可以使用&amp;rdquo;.&amp;ldquo;代替很多目标字段 多个入参对象转换为一个出参对象时，默认情况下只有所有入参对象都为空时才不会进行转换(当然，这点可以修改配置来解决) 在映射前后也可以执行自定义方法 参考文档 官方文档-v1.5.2 MapStruct使用手册 MapStruct原理解析</content></entry><entry><title>Feign源码分析</title><url>https://1162492411.github.io/docs/post/feign%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url><categories><category>Feign</category></categories><tags><tag>Java</tag><tag>Feign</tag><tag>源码分析</tag></tags><content type="html"> Feign源码分析-初始化流程和执行流程 概述 Feign是一个面向对象的http客户端, 这里主要介绍Feign是如何初始化的, 并如何进行http请求的. 省略部分不重要的代码. 剥离了hystrix和ribbon`的相关逻辑.
Feign大致经历了以下的发展历程
Feign : Netflix开源维护的声明式REST客户端
spring-cloud-feign ：Spring-Cloud-1.x基于Feign，支持SpringMvc注解
OpenFeign ：Netflix不再维护Feign，交由社区维护，更名为OpenFeign
spring-cloud-openfeign ：Spring-Cloud-2.x基于OpenFeign，支持SpringMvc注解
使用方式 1.引入框架依赖 &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-cloud-starter-openfeign&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; 2.开启feign功能 //在启动类添加@EnableFeignClients注解 @EnableFeignClients @SpringBootApplication public class ConsumerApplication { //.... } 3.定义业务Client //业务client @FeignClient(value = &amp;#34;businessA-client&amp;#34;,url = &amp;#34;http://localhost:8081/service-a.address&amp;#34;, configuration=BusinessAFeignConfiguration.class) public interface BusinessAFeignClient { @GetMapping(&amp;#34;/v1/user/{id}&amp;#34;) ResponseEntity&amp;lt;User&amp;gt; getUserById(@PathVariable Long id, @RequestParam(required = false) String name); @PostMapping(&amp;#34;/v1/user&amp;#34;) ResponseEntity&amp;lt;User&amp;gt; createUser(@RequestBody User user); } //业务client的feign配置类 public class BusinessAFeignConfiguration { @Bean Retryer feignRetryer() { return Retryer.NEVER_RETRY; } @Bean Request.Options requestOptions() { int ribbonReadTimeout = 2000; int ribbonConnectionTimeout = 500; return new Request.Options(ribbonConnectionTimeout, ribbonReadTimeout); } } 4.使用业务Client @RestController public class FeignController { //注入AFeignClient @Autowired private BusinessAFeignClient aFeignClient; @GetMapping(&amp;#34;/v1/user/query&amp;#34;) public ResponseEntity&amp;lt;User&amp;gt; queryUser(Long id,String name) { //调用AFeignClient的方法 User user = aFeignClient.getUserById(id,name); return ResponseEntity.ok(user); } } 初始化流程 流程概览 在feign的调用过程中，针对不同的服务为其创建一个上下文，我们可以将这个上下文理解为沙箱。执行调用所需要的资源是从各自的沙箱环境中取。整体的职责分工如下图所示
扫描注册子流程 ​ 通过前面的DEMO可以发现，使用 Feign 最核心的应该就是 @EnableFeignClients 和 @FeignClient 这两个注解，@FeignClient 加在客户端接口类上，@EnableFeignClients 加在启动类上，就是用来扫描加了 @FeignClient 接口的类。我们研究源码就从这两个入口开始。
@Import(FeignClientsRegistrar.class) // 引入 FeignClientsRegistrar public @interface EnableFeignClients { ... } @EnableFeignClients注解引入了FeignClientsRegistrar, 类实现了ImportBeanDefinitionRegistrar接口, 然后重写了registerBeanDefinitions方法. 说明加入到了Spring生命周期的管理中.
// org.springframework.cloud.openfeign.FeignClientsRegistrar // 实现了 ImportBeanDefinitionRegistrar 接口, 注册到 Spring 生命周期中 class FeignClientsRegistrar implements ImportBeanDefinitionRegistrar, ResourceLoaderAware, EnvironmentAware { @Override public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { // 1. 注册全局默认feign配置 registerDefaultConfiguration(metadata, registry); // 2. 创建 FeignClientFactoryBean 注入到 Spring 容器中 registerFeignClients(metadata, registry); } } 注册全局默认feign配置 registerDefaultConfiguration()方法比较简单，这里直接给出文字版逻辑:
获取 EnableFeignClients 中的属性 注册一个 default.类名.FeignClientSpecification 的 Bean, 作为默认Feign配置 注册FeignClientFactoryBean 逻辑如下
准备扫描路径列表 : 根据 @EnableFeignClients 的 clients 属性或者 value、basePackages、basePackageClasses 属性初始化 basePackages 变量 准备扫描器，并添加过滤，只扫描 @FeignClient 注解修饰的类 for循环basePackages 扫描包下的所有被 @FeignClient 注解修饰的接口 获取@FeignClient注解上的值 获取name : 按 contextId、value、name、serviceId 的优先级顺序获取 name, 用来做下面 bean name 的前缀${name} 注册feignConfigraution到spring core ：注册为${name}.FeignClientSpecification 注册FeignClientFactoryBean到spring core : com.xxx.AClient(别名 ${name}FeignClient) 扫描注册一图流 生成动态代理子流程 ​ FeignClientFactoryBean 实现了 FactoryBean 接口，当一个Bean实现了 FactoryBean 接口后，Spring 会先实例化这个工厂，然后调用 getObject() 创建真正的Bean。
class FeignClientFactoryBean implements FactoryBean&amp;lt;Object&amp;gt;, InitializingBean, ApplicationContextAware { @Override public Object getObject() throws Exception { //核心代码 FeignContext context = applicationContext.getBean(FeignContext.class); Feign.Builder builder = feign(context); Client client = getOptional(context, Client.class); builder.client(client); Targeter targeter = get(context, Targeter.class); return targeter.target(this, builder, context, new HardCodedTarget&amp;lt;&amp;gt;( this.type, this.name, url)); } } 核心组件 在该子流程中,涉及到以下核心组件
FeignContext : 作用是隔离不同服务的各种feign组件,创建一个沙箱环境 Feign.Builder : 作用是准备好每个FeignClient所需要的各种feign配置组件(借助sring存储在不同的springContext),设置了 Logger、Encoder、Decoder、Contract&amp;hellip;.，并读取配置文件中 feign.client.* 相关的配置 Client : 作用是完成http请求，目前有以下实现类 Default ： 默认实现，使用HttpURLConnection Proxied : https实现，使用HttpURLConnection FeignBlockingLoadBalancerClient ： 负载均衡实现，使用ribbon load balance RetryableFeignBlockingLoadBalancerClient ：负载均衡+重试实现，使用spring cloud load balance Target : 作用是桥接不同的动态代理实现类,现在有DefaultTargeter和HystrixTargeter Feign : 作用是生成代理类并借助java生成代理关系,目前有ReflectiveFeign，在这里注入了properties Map&amp;lt;Method,MethodHandler&amp;gt; dispatchMap : 实际请求时通过该数据进行路由 生成代理一图流 代理关系核心代码 //1.Feign.newInstance()具体逻辑 public class ReflectiveFeign extends Feign { public &amp;lt;T&amp;gt; T newInstance(Target&amp;lt;T&amp;gt; target) { //feign自己的内部关系 Map&amp;lt;Method, MethodHandler&amp;gt; methodToHandler = new LinkedHashMap&amp;lt;Method, MethodHandler&amp;gt;(); //借助java反射维护的关系 List&amp;lt;DefaultMethodHandler&amp;gt; defaultMethodHandlers = new LinkedList&amp;lt;DefaultMethodHandler&amp;gt;(); //在factory.create中将fein的invocationHandler注入进去 InvocationHandler handler = factory.create(target, methodToHandler); //借助java反射实现代理拦截效果 T proxy = (T) Proxy.newProxyInstance(target.type().getClassLoader(), new Class&amp;lt;?&amp;gt;[] {target.type()}, handler); for (DefaultMethodHandler defaultMethodHandler : defaultMethodHandlers) { defaultMethodHandler.bindTo(proxy); } return proxy; } } //factory.create()具体逻辑 public interface InvocationHandlerFactory { //调用这里 InvocationHandler create(Target target, Map&amp;lt;Method, MethodHandler&amp;gt; dispatch); interface MethodHandler { Object invoke(Object[] argv) throws Throwable; } static final class Default implements InvocationHandlerFactory { //最终调用到这里 @Override public InvocationHandler create(Target target, Map&amp;lt;Method, MethodHandler&amp;gt; dispatch) { return new ReflectiveFeign.FeignInvocationHandler(target, dispatch); } } } 处理请求流程 ​ feign的设计思路是在初始化流程中生成业务Client和业务Client的动态代理类的绑定关系,那么当代码调用到业务Client方法时,便会转向调用代理类的方法。
处理请求核心代码 上文分析到最终在ReflectiveFeign中生成了代理，那么处理请求时就会调用到该类的invoke()
static class FeignInvocationHandler implements InvocationHandler { //这个在初始化流程中准备好了关系 private final Map&amp;lt;Method, MethodHandler&amp;gt; dispatch; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { return dispatch.get(method).invoke(args); } } ​ 在dispatch中注册进的MethodHandler实际为SynchronousMethodHandler
final class SynchronousMethodHandler implements MethodHandler{ @Override public Object invoke(Object[] argv) throws Throwable { /** 1.根据请求参数构建请求模板 RequestTemplate，这个时候会处理参数中的占位符、拼接请求参数、处理body中的参数等等。 2.将 RequestTemplate 转成 Request : 2.1 用 RequestInterceptor 处理请求模板 2.2 用 Target（HardCodedTarget）处理请求地址，拼接上服务名前缀 2.3 调用 RequestTemplate 的 request 方法获取到 Request 对象 3. 调用 LoadBalancerFeignClient 的 execute 方法来执行请求并得到请求结果 Response 4. 得到 Response 后，就使用解码器 Decoder 解析响应结果，返回接口方法定义的返回类型 */ } } 处理请求一图流 对象关系 ${beanName} : 从@FeignClient注解中按优先级查找context-id/service-id/name/value的值
日常开发问题 Q : @FeignClients中的名字能否一致？如果一致会出现什么问题 A : cloudOpenFeign中存在多个name，name有三种：一种是注册到spring的beanName(从context-id/service-id/name/value取值)，一种是注册到spring的bean的别名aliasName(从qualifiers/qualifier取值)，还有一种name是class的全路径className,这三个名字(beanName,aliasName,className)不仅用于FeignClient自身向spring注册，还用于它专属的Configuration、Options等组件的spring bean名称。因此是否可以重复实际上取决于spring bean配置spring.main.allow-bean-definition-overriding，
Spring Boot 2.0.x 默认是 true. Spring Boot 2.1.x 默认是 false Q ：报错&amp;quot;出现重复的FeignClientSpecification&amp;quot;如何解决 A : 检查@FeignClient中是否存在相同的context-id/service-id/name/value
Q : 报错&amp;quot;AmbiguousMapping&amp;quot;如何解决 A : 原因是xxxClient继承不同服务的xxApi时，xxxApi类级别存在@RequestMapping注解，xxxClient继承后也被springmvc扫描了
解决方案：1）自定义一个RequestMappingHandlerMapping重写isHandler()方法；2）升级到spring6； 3）升级到cloudOpenFeign v3.1.0 v3.0.6 v3.0.5 v2.2.10.RELEASE
Q ： 如何配置使用HTTP连接池？它们是怎么生效的 A : 配置连接池需要两步：1)property配置feign.httpclient.enabled= true 2）maven引入依赖 生效方式方面，主要是feign-client中先进行了client的注册，这样在feign初始化时整个项目的spring中存在了client，被各个feignClient拿去使用
Q ：默认超时配置是多少？ A : 默认是连接超时10s，读超时60s,通过feign.Request.Options()空参构造器实现
可以配置一个全局超时时间 ：feign.client.config. default.connectTimeout= 2000 feign.client.config. default.readTimeout= 60000 可以单独对指定FeignClient配置超时时间 : 1)代码配置 @FeignClient(configuration=xxx.config.class) config类中声明options 2）配置文件配置 feign.client.config.serviceC.connectTimeout=2000 feign.client.config.serviceC.readTimeout=60000 feign和robbin等的超时时间优先级：需要实验一下，一方面,robbin有自己的client,一方面ribbon相关的包可能会先注册options对象从而影响feign的初始化流程 Q ：feignClient的配置优先级？ A : FeignClientFactoryBean.configureFeign()中进行了配置的准备,得出的结论是
如果feign.client.defaultToProperties=true(默认,属性默认赋值为true),生效顺序是全局上下文-&amp;gt;默认上下文→指定FeignClient上下文
如果feign.client.defaultToProperties=false,生效顺序是默认上下文-&amp;gt;指定FeignClient上下文-&amp;gt;全局上下文</content></entry><entry><title>FeignAmbiguousMapping重复映射问题</title><url>https://1162492411.github.io/docs/post/feignambiguousmapping%E9%87%8D%E5%A4%8D%E6%98%A0%E5%B0%84%E9%97%AE%E9%A2%98/</url><categories><category>实战</category></categories><tags><tag>Java</tag><tag>Feign</tag><tag>Feign实战</tag></tags><content type="html"> 现象 demo服务引入了多个依赖的业务系统(businessA/businessB/&amp;hellip;)的sdk后，启动时FeignClient 报错 Ambiguous mapping 重复映射,报错日志如下:
2022-01-06 15:57:20.267 [main] ERROR [bootstrap,,,] org.springframework.boot.SpringApplication - Application startup failed org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name &amp;lsquo;documentationPluginsBootstrapper&amp;rsquo; defined in URL [jar:file:/Users/xxx/.m2/repository/io/springfox/springfox-spring-web/2.9.2/springfox-spring-web-2.9.2.jar!/springfox/documentation/spring/web/plugins/DocumentationPluginsBootstrapper.class]: Unsatisfied dependency expressed through constructor parameter 1; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name &amp;lsquo;webMvcRequestHandlerProvider&amp;rsquo; defined in URL [jar:file:/Users/zyg/.m2/repository/io/springfox/springfox-spring-web/2.9.2/springfox-spring-web-2.9.2.jar!/springfox/documentation/spring/web/plugins/WebMvcRequestHandlerProvider.class]: Unsatisfied dependency expressed through constructor parameter 1; nested exception is org.springframework.beans.factory.BeanCreationException: ===============Error creating bean with name &amp;lsquo;requestMappingHandlerMapping&amp;rsquo; defined in class path resource [org/springframework/boot/autoconfigure/web/WebMvcAutoConfiguration$EnableWebMvcConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalStateException: ===================Ambiguous mapping. ===================Cannot map &amp;lsquo;com.demo.client.feign.UserClient&amp;rsquo; method public abstract com.BusinessBService.DataResponse&amp;lt;java.util.List&amp;lt;com.xxx.response.UserVO&amp;raquo; com.business.b.sdk.api.IUserApi.searchDetail(com.business.b.request.UserDTO) to {[/user/detail],methods=[POST]}: There is already &amp;lsquo;com.demo.client.BusinessBUserClient&amp;rsquo; bean method public abstract com.business.response.DataResponse&amp;lt;java.util.List&amp;lt;com.business.b.response.UserVO&amp;raquo; com.xx.api.IUserApi.searchDetail(com.business.b.request.UserDetailSearchDTO) mapped. at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:749) at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:189) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1198) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1100) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:511) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:481) at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:312) at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:308) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197) at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:761) at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:543) at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:124) at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:693) at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:360) at org.springframework.boot.SpringApplication.run(SpringApplication.java:303) at org.springframework.boot.SpringApplication.run(SpringApplication.java:1118) at org.springframework.boot.SpringApplication.run(SpringApplication.java:1107) at com.xxxx.DemoApplication.main(DemoApplication.java:31)
Maven依赖 :
spring-boot : 1.5.18.RELEASE spring-boot-starter-web : 1.5.18.RELEASE spring-cloud-starter-feign : 1.4.6.RELEASE 业务代码 报错内容表示是在demo服务中有两个不同服务的Client存在同样的URL,导致SpringMvc部分扫描时因为同一个url下存在多个Method/Handler而启动服务失败。
//我们自己写的第一个Client @FeignClient(name=&amp;#34;BusinessAUserClient&amp;#34;,url = &amp;#34;${business.a.address}&amp;#34;) public interface BusinessAUserFeignClient extends com.business.a.UserApi{ //... } //我们第一个Client引用的父类(来自依赖的其他服务BusinessA) @RequestMapping({&amp;#34;/user&amp;#34;}) public interface UserApi{ @PostMapping(&amp;#34;/detail&amp;#34;) List&amp;lt;UserVo&amp;gt; searchDetail(UserDTO userDTO); @GetMapping(&amp;#34;list&amp;#34;) List&amp;lt;UserVo&amp;gt; getList(UserDTO userDTO); } //我们自己写的第二个Client @FeignClient(name=&amp;#34;BusinessBUserClient&amp;#34;,url = &amp;#34;${business.b.address}&amp;#34;) public interface BusinessBUserFeignClient extends com.business.b.UserApi{ //... } //我们第二个Client引用的父类(来自依赖的其他服务BusinessB) @RequestMapping({&amp;#34;/user&amp;#34;}) public interface UserApi{ @PostMapping(&amp;#34;/detail&amp;#34;) List&amp;lt;UserVo&amp;gt; searchDetail(UserDTO userDTO); } 疑问1 : 为什么FeignClient会被SpringMVC扫描?? 疑问2 : 平常我们自己写的各种@FeignClient的类是如何被Spring扫描到的 疑问3 : 如果是所有依赖服务的URL都被当做我们自己服务的URL进行扫描，那么每个服务都存在health/check接口，为什么这些地方不会报错
Feign+SpringMVC扫描 - 相关代码定位 首先我们怀疑一下是SpringMVC的扫描存在问题,从这方面入手,我们查看一下@RequestMapping的类是如何被处理的,省略中间过程,最终查找到如下代码
public class RequestMappingHandlerMapping { /** * {@inheritDoc} * &amp;lt;p&amp;gt;Expects a handler to have either a type-level @{@link Controller} * annotation or a type-level @{@link RequestMapping} annotation. */ @Override protected boolean isHandler(Class&amp;lt;?&amp;gt; beanType) { return (AnnotatedElementUtils.hasAnnotation(beanType, Controller.class) || AnnotatedElementUtils.hasAnnotation(beanType, RequestMapping.class)); } } 那么原因就是各个依赖的UserApi等类存在@RequestMapping注解,所以SpringMVC顺手进行了扫描
Feign+SpringMVC扫描 - 解决思路 既然根源在于被SpringMVC额外扫描了，那么我们可以想办法让其不被SpringMVC扫描(但是最好仍然交给Spring托管xxxFeignClient)
方案一 : 手动写URL //我们自己写的第一个Client(这个类不再继承com.business.a.UserApi) @FeignClient(name=&amp;#34;BusinessAUserClient&amp;#34;,url = &amp;#34;${business.a.address}&amp;#34;,path = &amp;#34;/user&amp;#34;) public interface BusinessAUserFeignClient{ @PostMapping(&amp;#34;/detail&amp;#34;) List&amp;lt;UserVo&amp;gt; searchDetail(UserDTO userDTO); @GetMapping(&amp;#34;/list&amp;#34;) List&amp;lt;UserVo&amp;gt; getList(UserDTO userDTO); } //我们自己写的第二个Client(这个类可以仍然继承com.business.b.UserApi,也可以像BusinessAUserFeignClient一样手写url) @FeignClient(name=&amp;#34;BusinessBUserClient&amp;#34;,url = &amp;#34;${business.b.address}&amp;#34;) public interface BusinessBUserFeignClient extends com.business.b.UserApi{ //... } //我们第二个Client引用的父类(来自依赖的其他服务BusinessB) @RequestMapping({&amp;#34;/user&amp;#34;}) public interface UserApi{ @PostMapping(&amp;#34;/detail&amp;#34;) List&amp;lt;UserVo&amp;gt; searchDetail(UserDTO userDTO); } 这样一来,最多有一个url的Handler被SpringMVC扫描，服务可以正常启动，代码也可以正常调用这几个url。 但是,这样一来,我们就无法享受到依赖服务SDK的好处了,如果依赖服务后期新增/改动了url/改动了url中的请求或相应类,我们这边都要进行相应修改;如果要求 依赖服务方将UserApi的类级别的@RequestMapping移动到各方法中也有些强人所难且不合理
方案二 : SpringMVC扫描时过滤 既然问题出在SpringMVC扫描,那么如果我们修改isHandler()的逻辑,使其在遇到@FeignClient的类时忽略,也能达到我们想要的效果
//代码拷贝自https://github.com/spring-cloud/spring-cloud-netflix/issues/466#issuecomment-257043631 import feign.Feign; import org.springframework.boot.autoconfigure.condition.ConditionalOnClass; import org.springframework.boot.autoconfigure.web.WebMvcRegistrations; import org.springframework.boot.autoconfigure.web.WebMvcRegistrationsAdapter; import org.springframework.cloud.netflix.feign.FeignClient; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.core.annotation.AnnotationUtils; import org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping; /** * 解决带有@FeignClient的类继承了类级别带有@RequestMapping的API类之后被springmvc扫描handlerMapping的问题 * @author xxx */ @Configuration @ConditionalOnClass({Feign.class}) public class FeignMappingDefaultConfiguration { @Bean public WebMvcRegistrations feignWebRegistrations() { return new WebMvcRegistrationsAdapter() { @Override public RequestMappingHandlerMapping getRequestMappingHandlerMapping() { return new FeignFilterRequestMappingHandlerMapping(); } }; } private static class FeignFilterRequestMappingHandlerMapping extends RequestMappingHandlerMapping { @Override protected boolean isHandler(Class&amp;lt;?&amp;gt; beanType) { return super.isHandler(beanType) &amp;amp;&amp;amp; (AnnotationUtils.findAnnotation(beanType, FeignClient.class) == null); } } } 方案三 : 官方解决方案 Spring官方(Spring-framework)相关开发经过讨论,最终给出的解决方案是
2021-10-7 after some cross-team discussions, for the short term the issue will be addressed on the side of Spring Data REST and Spring Cloud. For Spring Framework 6.0, we will also address this in the Spring Framework by no longer considering a class with a type-level @RequestMapping as a candidate for detection, unless there is also @Controller. 经过一些跨团队讨论，短期内该问题将在 Spring Data REST 和 Spring Cloud 方面解决。 对于 Spring Framework 6.0，我们还将在 Spring Framework 中解决这个问题，不再将具有类型级别的类@RequestMapping作为检测候选，除非也有@Controller.
具体代码如下
public class RequestMappingHandlerMapping { //只将类级别有@Controller的进行扫描,这样改整体看合理但是不向前兼容 protected boolean isHandler(Class&amp;lt;?&amp;gt; beanType) { return AnnotatedElementUtils.hasAnnotation(beanType, Controller.class); } } Spring-cloud-openfeign的解决方案是不支持@FeignClient+@RequestMapping, 具体代码为https://github.com/spring-cloud/spring-cloud-openfeign/commit/d6783a6f1ec8dd08fafe76ecd072913d4e6f66b9
public class SpringMvcContract extends Contract.BaseContract implements ResourceLoaderAware { protected void processAnnotationOnClass(MethodMetadata data, Class&amp;lt;?&amp;gt; clz) { RequestMapping classAnnotation = findMergedAnnotation(clz, RequestMapping.class); if (classAnnotation != null) { LOG.error(&amp;#34;Cannot process class: &amp;#34; + clz.getName() + &amp;#34;. @RequestMapping annotation is not allowed on @FeignClient interfaces.&amp;#34;); throw new IllegalArgumentException( &amp;#34;@RequestMapping annotation not allowed on @FeignClient interfaces&amp;#34;); } }</content></entry><entry><title>Es批量删除api</title><url>https://1162492411.github.io/docs/post/es%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4api/</url><categories><category>实战</category></categories><tags><tag>Java</tag><tag>ES</tag><tag>ES基础Api</tag></tags><content type="html"> ES中通过该API实现数据的批量删除,官方对该API的定义为 : 根据特定的查询条件对ES相关索引中某些特定的文档进行批量删除
API介绍 DSL 如下
POST indexName1/_delete_by_query { &amp;#34;query&amp;#34;: { //这里传递查询条件 &amp;#34;match&amp;#34;: { &amp;#34;message&amp;#34;: &amp;#34;some message&amp;#34; } } } 响应如下
{ &amp;#34;took&amp;#34; : 147,// 从整个操作的开始到结束的毫秒数 &amp;#34;timed_out&amp;#34;: false,// &amp;#34;deleted&amp;#34;: 119,// 成功删除的文档数 &amp;#34;batches&amp;#34;: 1,// 通过查询删除的滚动响应数量 &amp;#34;version_conflicts&amp;#34;: 0,// 根据查询删除时，版本冲突的数量 &amp;#34;noops&amp;#34;: 0,// &amp;#34;retries&amp;#34;: {// 根据查询删除的重试次数是响应于完整队列 &amp;#34;bulk&amp;#34;: 0, &amp;#34;search&amp;#34;: 0 }, &amp;#34;throttled_millis&amp;#34;: 0,// 请求休眠的毫秒数，与`requests_per_second`一致 &amp;#34;requests_per_second&amp;#34;: -1.0,// &amp;#34;throttled_until_millis&amp;#34;: 0,// &amp;#34;total&amp;#34;: 119,// &amp;#34;failures&amp;#34; : [ ]// 失败的索引数组 } 原理 在_delete_by_query执行期间，依次执行多个搜索请求，以便找到要删除的所有匹配文档。每次发现一批文档时，执行相应的批量请求以删除所有这些文档。如果搜索或批量请求被拒绝，_delete_by_query依赖于默认策略来重试拒绝的请求（最多10次，以指数返回）。达到最大重试次数限制会导致_delete_by_query中止，并在响应失败中返回所有故障。已经执行的删除仍然保持。换句话说，进程没有回滚，只会中止。当第一个故障导致中止时，失败批量请求返回的所有故障都会返回到故障元素中;因此，有可能会有不少失败的实体。注意，该API并不是真正将数据删除，而是将其标记为删除状态，并不会真正删除数据及释放磁盘空间
参数 参数 默认值 值域 作用 refresh [true,false,wait_for] 发送refresh将在一旦根据查询删除完成之后， 刷新所有涉及到的分片；如果设置为wait_for,表示使用集群自动刷新机制,具体等待的时间优先取索引级配置,默认1s wait_for_completion [true,false，任意正数] 值为false时es将会创建一个任务来执行删除，可以与TASK API结合使用，ES会增加一条.tasks/task/${taskId}数据用来记录删除进度 wait_for_active_shards 控制在继续请求之前必须有多少个分片必须处于活动状态 timeout 控制每个写入请求等待不可用分片变成可用的时间 requests_per_second -1 任意正数,-1 -1时表示禁用该项，任意正数时，在多个批量批次间将会按该值进行等待 conflicts proceed 计算删除数据时的版本冲突数量,使用该参数后将不会导致存在版本冲突时整个删除进度终止 异步 当我们在删除请求中添加wait_for_completion=false时ES会返回一个taskId来异步进行删除数据。可以通过API查询删除进度
-- 查询任务的进度 GET /_tasks/1 -- 响应 { &amp;#34;completed&amp;#34; : false, &amp;#34;task&amp;#34; : { &amp;#34;node&amp;#34; : &amp;#34;fFDobwtSQvS1ishWYtWQcg&amp;#34;, &amp;#34;id&amp;#34; : 142113449, &amp;#34;type&amp;#34; : &amp;#34;transport&amp;#34;, &amp;#34;action&amp;#34; : &amp;#34;indices:data/write/delete/byquery&amp;#34;, &amp;#34;status&amp;#34; : { &amp;#34;total&amp;#34; : 10918603, &amp;#34;updated&amp;#34; : 0, &amp;#34;created&amp;#34; : 0, &amp;#34;deleted&amp;#34; : 0, &amp;#34;batches&amp;#34; : 243, &amp;#34;version_conflicts&amp;#34; : 242000, &amp;#34;noops&amp;#34; : 0, &amp;#34;retries&amp;#34; : { &amp;#34;bulk&amp;#34; : 0, &amp;#34;search&amp;#34; : 0 }, &amp;#34;throttled_millis&amp;#34; : 0, &amp;#34;requests_per_second&amp;#34; : -1.0, &amp;#34;throttled_until_millis&amp;#34; : 0 }, &amp;#34;description&amp;#34; : &amp;#34;delete-by-query [ads_lading_trade_brief_es_02]&amp;#34;, &amp;#34;start_time_in_millis&amp;#34; : 1604650445876, &amp;#34;running_time_in_nanos&amp;#34; : 159708214419, &amp;#34;cancellable&amp;#34; : true, &amp;#34;headers&amp;#34; : { } } } -- 取消任务 POST _tasks/task_id:1/_cancel 切片 为了在尽量短的时间内删除数据，可以将数据通过scroll划分为不同的切片，然后并行删除多个切片。
当然，该API还支持自动切片。
分段 ​ 在调用该API后，数据仅仅是被标记为删除状态，实际上并未物理删除，需要依赖于Lucene对分段进行合并才能真正完成物理删除数据的效果。
​ ES写入数据的过程实际上被拆分为了以下几步 :
先写到内存中，此时不可搜索
默认经过 1s 之后会通过refresh被写入 lucene 的底层文件 segment 中 ，此时可以搜索到
多个segment通过merge被合并为一个大的segment
多个大的segment通过flush被刷新到磁盘
https://img-blog.csdnimg.cn/20200823133011451.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNzY5MjQ5Mw==,size_16,color_FFFFFF,t_70#pic_center
refresh ​ 在lucene 中，为了实现高索引速度，使用了segment 分段架构存储。一批写入数据保存在一个段中，其中每个段是磁盘中的单个文件，每个段会消耗文件句柄、内存和cpu运行周期。
在 index ，Update , Delete , Bulk 等操作中，可以设置 refresh 的值。取值如下：
refresh=true : 更新数据之后，立刻对相关的分片(包括副本) 刷新，这个刷新操作保证了数据更新的结果可以立刻被搜索到。
refresh=wait_for : 这个参数表示，刷新不会立刻进行，而是等待一段时间才刷新 ( index.refresh_interval)，默认时间是 1 秒。刷新时间间隔可以通过index 的配置动态修改。或者直接手动刷新 POST /twitter/_refresh
refresh=false : refresh 的默认值，更新数据之后不立刻刷新，在返回结果之后的某个时间点会自动刷新，也就是随机的，看es服务器的运行情况。
merge ​ 当lucene中的段的数量过多时，搜索性能会降低，同时搜索时占用的内存也会更多，因此有必要控制段的数量。ES在后台进行段合并，将小段合并为大段，将大段合并为更大的段，同时，在段合并期间，被标记为删除状态的数据不会被合并到新段中。段合并需要消耗IO资源和CPU时间。
​ es对于merge比较保守，可以通过参数限制每次merge的小段的数量(max_num_segments,默认1)，限制每次merge的带宽(max_bytes_per_sec,默认20MB/s)
实战 场景一 : 根据批量的商品码删除ES数据 import org.elasticsearch.ElasticsearchStatusException; import org.elasticsearch.action.ActionListener; import org.elasticsearch.action.admin.indices.create.CreateIndexRequest; import org.elasticsearch.action.bulk.BulkRequest; import org.elasticsearch.action.bulk.BulkResponse; import org.elasticsearch.action.get.GetRequest; import org.elasticsearch.action.get.GetResponse; import org.elasticsearch.action.index.IndexRequest; import org.elasticsearch.action.update.UpdateRequest; import org.elasticsearch.client.RequestOptions; import org.elasticsearch.client.Response; import org.elasticsearch.client.RestHighLevelClient; import org.elasticsearch.common.settings.Settings; import org.elasticsearch.common.unit.TimeValue; import org.elasticsearch.common.xcontent.XContentBuilder; import org.elasticsearch.common.xcontent.XContentFactory; import org.elasticsearch.common.xcontent.XContentType; import org.elasticsearch.index.query.*; import org.elasticsearch.index.reindex.BulkByScrollResponse; import org.elasticsearch.index.reindex.DeleteByQueryRequest; @Autowired private RestHighLevelClient restHighLevelClient; @Override public void deleteBatchForArchive(List&amp;lt;String&amp;gt; goodsCodeList) { DeleteByQueryRequest request = new DeleteByQueryRequest(priceItemIndex); request.setDocTypes(priceItemType); request.setQuery(installSearchRequestForArchive(goodsCodeList)); request.setConflicts(&amp;#34;proceed&amp;#34;); ActionListener&amp;lt;BulkByScrollResponse&amp;gt; listener = new ActionListener&amp;lt;BulkByScrollResponse&amp;gt;() { @Override public void onResponse(BulkByScrollResponse bulkByScrollResponse) { if(Objects.isNull(bulkByScrollResponse)){ log.info(&amp;#34;返回空数据&amp;#34;); return; } if(goodsCodeList.size() != bulkByScrollResponse.getDeleted()){ log.error(&amp;#34;删除了{}条商品码,其中{}条数据存在版本冲突,{}条数据删除失败,详情为:{}&amp;#34;,bulkByScrollResponse.getDeleted(), bulkByScrollResponse.getVersionConflicts(), CollectionUtil.getListSize(bulkByScrollResponse.getBulkFailures()), new Gson().toJson(bulkByScrollResponse.getBulkFailures())); } } @Override public void onFailure(Exception e) { log.error(&amp;#34;归档ES数据发送请求时异常,商品码为:{},&amp;#34;,new Gson().toJson(goodsCodeList),e); } }; // 调用异步方法,当ES侧完成请求时回调上面自定义的listener restHighLevelClient.deleteByQueryAsync(request,RequestOptions.DEFAULT,listener); } /** * 根据码列表+失效状态进行查询 * { * &amp;#34;query&amp;#34;: { * &amp;#34;bool&amp;#34;: { * &amp;#34;must&amp;#34;: [{ * &amp;#34;term&amp;#34;: {&amp;#34;effective&amp;#34;: {&amp;#34;value&amp;#34;: 0,&amp;#34;boost&amp;#34;: 1.0}}}, * {&amp;#34;terms&amp;#34;: {&amp;#34;code&amp;#34;: [&amp;#34;214016384187744256&amp;#34;, &amp;#34;214015610148470784&amp;#34;, &amp;#34;214060765785659392&amp;#34;],&amp;#34;boost&amp;#34;: 1.0} * }], * &amp;#34;adjust_pure_negative&amp;#34;: true, * &amp;#34;boost&amp;#34;: 1.0* } * } * } * @param goodsCodeList * @return */ private QueryBuilder installSearchRequestForArchive(List&amp;lt;String&amp;gt; goodsCodeList){ //组装Query对象 BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery(); TermQueryBuilder statusTermQuery = QueryBuilders.termQuery(&amp;#34;effective&amp;#34;, 0); TermsQueryBuilder goodsCodeTermsQuery = QueryBuilders.termsQuery(&amp;#34;code&amp;#34;, goodsCodeList); queryBuilder.must(statusTermQuery); queryBuilder.must(goodsCodeTermsQuery); return queryBuilder; }</content></entry><entry><title>Github站点443超时问题解决</title><url>https://1162492411.github.io/docs/post/github%E7%AB%99%E7%82%B9443%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/</url><categories><category>软件问题</category></categories><tags><tag>GitHub</tag><tag>软件问题</tag></tags><content type="html"> 问题描述 使用代理可以在浏览器正常访问github,但是本地推送代码时却仍然提示&amp;quot;Failed to connect to github.com port 443: Operation timed out&amp;rdquo;
解决方案 超时原因在于解析DNS时出现问题,那么我们只要在本地配置好该网站的dns记录即可解决
获取真实IP 一共需要获取以下三个网站的DNS
github.com
github.global.ssl.fastly.net
assets-cdn.github.com
在IPAddress可以查找到这些域名对应的真实IP
修改host 找到本地的hosts文件(Mac系统位于/etc/hosts),添加以下内容
Ip1 github.com
Ip2 github.global.ssl.fastly.net
Ip3 assets-cdn.github.com
Ip4 assets-cdn.github.com
ip5 assets-cdn.github.com
&amp;hellip;.
刷新本地DNS 上一步配置完DNS记录后,我们还需要清理掉本地已经缓存的DNS,在控制台执行命令清除本地DNS
Mac : sudo killall -HUP mDNSResponder;say DNS cache has been flushed
Windows : ipconfig /displaydns ipconfig /flushdns
ps:如果最后还是提示 【connect to 127.0.0.01 port 1080: Connection refused】 把代理去掉： git config &amp;ndash;global &amp;ndash;unset http.proxy git config &amp;ndash;global &amp;ndash;unset https.proxy</content></entry><entry><title>MySQL事务隔离级别实验</title><url>https://1162492411.github.io/docs/post/mysql%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%9E%E9%AA%8C/</url><categories/><tags><tag>数据库</tag><tag>隔离级别</tag><tag>MySQL</tag></tags><content type="html"> 实验环境 MySQL v5.7 docker Mac
-- 建表 create table t2 ( id int not null primary key, content varchar(50) null, type int null ); -- 初始化数据 insert into t2 (id, content,type) values (1,&amp;#39;内容1&amp;#39;,1); insert into t2 (id, content,type) values (2,&amp;#39;内容2&amp;#39;,2); insert into t2 (id, content,type) values (4,&amp;#39;内容4&amp;#39;,4); insert into t2 (id, content,type) values (6,&amp;#39;内容6&amp;#39;,6); 理论铺垫 事务定义 事务的并发问题 1）脏读
一个事务读到了另一个未提交事务修改过的数据
2）不可重复读
一个事务只能读到另一个已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，该事务都能查询得到最新值
3）幻读
一个事务先根据某些条件查询出一些记录，之后另一个事务又向表中插入了符合这些条件的记录，原先的事务再次按照该条件查询时，能把另一个事务插入的记录也读出来
事务的隔离级别 事务的创建/提交/回滚 事务的查看 执行命令
-- 需要用户具有process权限 SELECT * FROM information_schema.INNODB_TRX 执行后会出现如下的结果
对于结果中各项属性的作用解释如下
trx_id：唯一事务id号，只读事务和非锁事务是不会创建id的。 TRX_WEIGHT：事务的高度，代表修改的行数（不一定准确）和被事务锁住的行数。为了解决死锁，innodb会选择一个高度最小的事务来当做牺牲品进行回滚。已经被更改的非交易型表的事务权重比其他事务高，即使改变的行和锁住的行比其他事务低。 TRX_STATE：事务的执行状态，值一般分为：RUNNING, LOCK WAIT, ROLLING BACK, and COMMITTING. TRX_STARTED：事务的开始时间 TRX_REQUESTED_LOCK_ID:如果trx_state是lockwait,显示事务当前等待锁的id，不是则为空。想要获取锁的信息，根据该lock_id，以innodb_locks表中lock_id列匹配条件进行查询，获取相关信息。 TRX_WAIT_STARTED：如果trx_state是lockwait,该值代表事务开始等待锁的时间；否则为空。 TRX_MYSQL_THREAD_ID：mysql线程id。想要获取该线程的信息，根据该thread_id，以INFORMATION_SCHEMA.PROCESSLIST表的id列为匹配条件进行查询。 TRX_QUERY：事务正在执行的sql语句。 TRX_OPERATION_STATE：事务当前的操作状态，没有则为空。 TRX_TABLES_IN_USE：事务在处理当前sql语句使用innodb引擎表的数量。 TRX_TABLES_LOCKED：当前sql语句有行锁的innodb表的数量。（因为只是行锁，不是表锁，表仍然可以被多个事务读和写） TRX_LOCK_STRUCTS：事务保留锁的数量。 TRX_LOCK_MEMORY_BYTES：在内存中事务索结构占得空间大小。 TRX_ROWS_LOCKED：事务行锁最准确的数量。这个值可能包括对于事务在物理上存在，实际不可见的删除标记的行。 TRX_ROWS_MODIFIED：事务修改和插入的行数 TRX_CONCURRENCY_TICKETS：该值代表当前事务在被清掉之前可以多少工作，由 innodb_concurrency_tickets系统变量值指定。 TRX_ISOLATION_LEVEL：事务隔离等级。 TRX_UNIQUE_CHECKS：当前事务唯一性检查启用还是禁用。当批量数据导入时，这个参数是关闭的。 TRX_FOREIGN_KEY_CHECKS：当前事务的外键坚持是启用还是禁用。当批量数据导入时，这个参数是关闭的。 TRX_LAST_FOREIGN_KEY_ERROR：最新一个外键错误信息，没有则为空。 TRX_ADAPTIVE_HASH_LATCHED：自适应哈希索引是否被当前事务阻塞。当自适应哈希索引查找系统分区，一个单独的事务不会阻塞全部的自适应hash索引。自适应hash索引分区通过 innodb_adaptive_hash_index_parts参数控制，默认值为8。 TRX_ADAPTIVE_HASH_TIMEOUT：是否为了自适应hash索引立即放弃查询锁，或者通过调用mysql函数保留它。当没有自适应hash索引冲突，该值为0并且语句保持锁直到结束。在冲突过程中，该值被计数为0，每句查询完之后立即释放门闩。当自适应hash索引查询系统被分区（由 innodb_adaptive_hash_index_parts参数控制），值保持为0。 TRX_IS_READ_ONLY：值为1表示事务是read only。 TRX_AUTOCOMMIT_NON_LOCKING：值为1表示事务是一个select语句，该语句没有使用for update或者shared mode锁，并且执行开启了autocommit，因此事务只包含一个语句。当TRX_AUTOCOMMIT_NON_LOCKING和TRX_IS_READ_ONLY同时为1，innodb通过降低事务开销和改变表数据库来优化事务。 也可以直接参考官方链接
快照读和当前读 隔离级别能解决哪些并发问题 隔离级别 脏读 不可重复读 幻影读 RU √ √ √ RC × √ √ RR × × √ SERIALIZABLE × × × 并发问题验证 RU实验 实验一 ： 验证RU下的脏读问题 事务A ：Autocommit = 0，Isolation = RU 事务B ：Autocommit = 0，Isolation = RU 时间 事务A 事务A结果 事务B 事务B结果 备注 T1 begin T2 update t2 set type=2222 where id=1; T3 begin T4 select * from t2 where id=1 ; type的值为2222 分析：
T2时刻，事务B修改了数据但没有提交事务 T4时刻，事务A查询到了事务B未提交的修改，发生了脏读 实验二 ： 验证RU下的不可重复读问题 事务A ：Autocommit = 0，Isolation = RU 事务B ：Autocommit = 0，Isolation = RU 时间 事务A 事务A结果 事务B 事务B结果 备注 T1 begin T2 select * from t2 where id=1 ; type的值为1 T3 begin T4 update t2 set type=2222 where id=1; T5 select * from t2 where id=1 ; type的值为2222 分析：
T1时刻，事务A正常读取到type的值为1 T4时刻，事务B将type的值修改为2222但并没有提交 T5时刻，事务A读取type的值为2222，与T1时刻相比，同一个事务中两次读取的值不同 实验三 ： 验证RU下的幻读问题 事务A ：Autocommit = 0，Isolation = RU 事务B ：Autocommit = 0，Isolation = RU 时间 事务A 事务A结果 事务B 事务B结果 备注 T1 begin T2 select * from t2 where type between 1 and 3; 数据1数据2 T3 begin T4 insert into t2 (id, content, type) value (11,&amp;lsquo;ccc&amp;rsquo;,2); T5 select * from t2 where type between 1 and 3; 数据1数据2数据11 T6 rollback T7 select * from t2 where type between 1 and 3; 数据1数据2 分析：
T4时刻事务B插入了数据11但是并没有提交事务 T5时刻事务A查询到了数据11 T6时刻事务B回滚了事务 T7时刻事务A又查不到数据11，与T5时刻相比，数据11消失不见了 (也可以拿T1～T5时刻进行比较，T5时刻的结果相比T2时刻竟然多了数据11这条记录) RC实验 实验一 ： 验证RC下的脏读问题（不存在） 事务A ：Autocommit = 0，Isolation = RC 事务B ：Autocommit = 0，Isolation = RC 时间 事务A 事务A结果 事务B 事务B结果 备注 T1 begin T2 update t2 set type=2222 where id=1; T3 begin T4 select * from t2 where id=1 ; type的值为1 分析：
T2时刻，事务B修改了数据但没有提交事务 T4时刻，事务A读取数据，type为1，并没有受到事务B修改为2222的影响 实验二 ：验证RC下的不可重复读问题 事务A ：Autocommit = 0，Isolation = RC 事务B ：Autocommit = 0，Isolation = RC 时间 事务A 事务A结果 事务B 事务B结果 备注 T1 begin T2 select * from t2 where id=1 ; type的值为1 T3 begin T4 update t2 set type=2222 where id=1; T5 commit T6 select * from t2 where id=1 ; type的值为2222 分析：
T1时刻，事务A正常读取到type的值为1 T4时刻，事务B将type的值修改为2222并在T5时刻提交事务 T6时刻，事务A读取type的值为2222，与T1时刻相比，同一个事务中两次读取的值不同 实验三 ： 验证RC下的幻读问题 事务A ：Autocommit = 0，Isolation = RC 事务B ：Autocommit = 0，Isolation = RC 时间 事务A 事务A结果 事务B 事务B结果 备注 T1 begin T2 select * from t2 where type between 1 and 3; 数据1数据2 T3 begin T4 insert into t2 (id, content, type) value (11,&amp;lsquo;ccc&amp;rsquo;,2); T5 commit T6 select * from t2 where type between 1 and 3; 数据1数据2数据11 分析 ：
T2时刻事务A可以看到两条数据 T3～T5时刻事务B插入了数据11并提交事务 T6时刻，事务A看到了三条数据，相比T2时刻多了一条数据 RR实验 实验一 ：验证RR下的脏读问题(不存在) 事务A ：Autocommit = 0，Isolation = RR 事务B ：Autocommit = 0，Isolation = RR 时间 事务A 事务A结果 事务B 事务B结果 备注 T1 begin T2 update t2 set type=2222 where id=1; T3 begin T4 select * from t2 where id=1 ; type的值为1 分析：
T2时刻，事务B修改了数据但没有提交事务 T4时刻，事务A读取数据，type为1，并没有受到事务B修改为2222的影响 实验二 ：验证RR下的不可重复读问题(不存在) 事务A ：Autocommit = 0，Isolation = RR 事务B ：Autocommit = 0，Isolation = RR 时间 事务A 事务A结果 事务B 事务B结果 备注 T1 begin T2 select * from t2 where id=1 ; type的值为1 T3 begin T4 update t2 set type=2222 where id=1; T5 commit T6 select * from t2 where id=1 ; type的值为1 分析：
T1时刻，事务A正常读取到type的值为1 T4时刻，事务B将type的值修改为2222并在T5时刻提交事务 T6时刻，事务A读取type的值为1，与T1时刻相比，同一个事务中两次读取的值相同 实验三 ：验证RR下的幻读问题 事务A ：Autocommit = 0，Isolation = RR 事务B ：Autocommit = 0，Isolation = RR 时间 事务A 事务A结果 事务B 事务B结果 备注 T1 begin T2 select * from t2 where type between 1 and 3; 数据1数据2 T3 begin T4 SQL1 : insert into t2 (id, content, type) value (11,&amp;lsquo;ccc&amp;rsquo;,2);SQL2 : update t2 set type=2 where id=4; 执行SQL1/SQL2均可 T5 commit T6 select * from t2 where type between 1 and 3 for update; 数据1数据2数据11/数据4 分析 ：
T2时刻事务A可以看到两条数据 T3～T5时刻事务B插入了数据11/修改了数据4并提交事务 T6时刻，事务A看到了三条数据，相比T2时刻多了一条数据 序列化实验 Todo实验一 ：验证序列化下的脏读问题(不存在) todo实验二 ：验证序列化下的不可重复读问题(不存在) todo实验三 ：验证序列化下的幻读问题(不存在) 实验 实验一 ：验证当前读和快照读 事务A ：Autocommit = 0，Isolation = RR 事务B ：Autocommit = 0，Isolation = RR 时间 事务A 事务A结果 事务B 事务B结果 备注 T1 Begin T2 select * from t2 where type between 5 and 10; 6,文字6,6 T3 begin T4 insert into t2 (id, content,type) values (7,&amp;lsquo;内容7&amp;rsquo;,7); T5 select * from t2 where type between 5 and 10; 6,文字6,6 T6 commit T7 select * from t2 where type between 5 and 10; 6,文字6,6 T8 select * from t2 where type between 5 and 10 for update; 6,文字6,67,内容7,7 T9 commit 分析：
T5时刻执行了查询语句，但是事务A并没有看到数据7，因为事务B此时没有提交事务 T7时刻执行了查询语句，事务B已经提交事务，事务A仍然没有看到数据7，在T8时刻才能看到数据7，因为select是快照读，而select for update是当前读 实验二 ： select for update排它锁的验证 事务A ：Autocommit = 0，Isolation = RR 事务B ：Autocommit = 0，Isolation = RR 时间 事务A 事务A结果 事务B 事务B结果 备注 T1 Begin T2 select * from t2 where type between 5 and 10; 数据6 T3 begin T4 insert into t2 (id, content,type) values (7,&amp;lsquo;内容7&amp;rsquo;,7); T5 SQL1 ： select * from t2 where type between 5 and 10 for update ;SQL2 ：select * from t2 where type = 2 for update ;SQL3 ：select * from t2 where type between 5 and 10;SQL4 ：select * from t2 where type = 2 若执行SQL1会阻塞直到超时；若type字段无索引，执行SQL2会阻塞，若type字段又索引，执行SQL会查询出数据2；若执行SQL3会查询出数据6;若执行SQL4会查询出数据2 SQL5 : select * from t2 where type = 2 for update ;SQL6 : select * from t2 where id = 2 for update ; T6 Commit T7 select * from t2 where type between 5 and 10 ; 数据6 T8 select * from t2 where type between 5 and 10 for update; 数据6数据7 T9 commit 分析：
T5时刻，四条SQL中，若执行SQL1会阻塞，因为此刻事务B正在插入数据，若执行SQL3/SQL4不会阻塞，因为这两条SQL是当前读；若type字段无索引时，执行SQL1/SQL2时会锁表，无法修改表的数据但可以快照读，若type字段有索引并且命中索引时，执行SQL1/SQL2时仅会锁住范围内的数据(关于行锁和表锁以及与索引的实际情况比较复杂) T7时刻，执行SQL无法查询出数据7，因为select是当前读，事务A开始时就看不到数据7，此刻也应该看不到数据7 T8时刻，执行SQL可以查询出数据7，因为此刻没有其他事务在修改/插入数据，所以不会阻塞，因为select for update是当前读，因此可以看到数据7 实验三 ： 验证排它锁的行锁和表锁(todo) 实验四 ：验证两个写事务 事务A ：Autocommit = 0，Isolation = RR 事务B ：Autocommit = 0，Isolation = RR 时间 事务A 事务A结果 事务B 事务B结果 备注 T1 Begin T2 insert into t2 (id, content,type) values (9,&amp;lsquo;内容9&amp;rsquo;,9); T3 begin T4 SQL1 : insert into t2 (id, content,type) values (7,&amp;lsquo;内容7&amp;rsquo;,7);SQL2 : insert into t2 (id, content,type) values (9,&amp;lsquo;内容7&amp;rsquo;,9); T5 commit T6 commit 分析：
T4时刻，执行SQL1，可以正常执行，并且T5、T6可以正常提交 T4时刻，执行SQL2，事务B阻塞等待，T5时刻事务A提交后，事务B会报错主键冲突 实验五 ：select 常量 for update 例如 select &amp;lsquo;xx&amp;rsquo; for update，在这种场景中，即便手动begin事务，仍然不会在mysql的事务列表中观察到事务
需要继续研究 ： 意向锁、意向锁之间的兼容情况、select for update和select in share mode之间的异同点</content></entry><entry><title>版本控制常见面试题</title><url>https://1162492411.github.io/docs/post/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories><category>面试题</category><category>版本控制</category><category>Git</category></categories><tags><tag>面试题</tag><tag>版本控制</tag><tag>Git</tag></tags><content type="html"> 基础篇 merge和rebase区别 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } revert和reset区别 * reset直接将某次提commit之后的commit丢弃掉 * revert是增加了一个针对某次commit的反向commit 举例 ： commitA -- commitB -- commitC 现在想保持最新commit到commitB的代码， * 使用reset之后，提交列表为 : commitA -- commitB * 使用revert之后，提交列表为 : commitA -- commitB -- commitC -- -CommitC .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 实战篇 系统上线后需要回退部分代码如何实现 背景 ：
(1)改完代码匆忙提交,上线发现有问题,怎么办? 赶紧回滚.
(2)改完代码测试也没有问题,但是上线发现你的修改影响了之前运行正常的代码报错,必须回滚.
1.没有push 这种情况发生在你的本地代码仓库,可能你add ,commit 以后发现代码有点问题,准备取消提交,用到下面命令 ```shell git reset [--soft | --mixed | --hard ``` 上面常见三种类型 --mixed 会保留源码,只是将git commit和index 信息回退到了某个版本. ```shell git reset --mixed ##等价于 git reset ``` --soft 保留源码,只回退到commit 信息到某个版本.不涉及index的回退,如果还需要提交,直接commit即可. --hard 源码也会回退到某个版本,commit和index 都回回退到某个版本.(注意,这种方式是改变本地代码仓库源码) 当然有人在push代码以后,也使用 reset --hard 回退代码到某个版本之前,但是这样会有一个问题,你线上的代码没有变,线上commit,index都没有变,当你把本地代码修改完提交的时候你会发现全是冲突.....所以,这种情况你要使用下面的方式 2.已经push 对于已经把代码push到线上仓库,你回退本地代码其实也想同时回退线上代码,回滚到某个指定的版本,线上,线下代码保持一致.你要用到下面的命令`revert` git revert用于反转提交,执行evert命令时要求工作树必须是干净的. git revert用一个新提交来消除一个历史提交所做的任何修改. revert 之后你的本地代码会回滚到指定的历史版本,这时你再 git push 既可以把线上的代码更新.(这里不会像reset造成冲突的问题) revert 使用,需要先找到你想回滚版本唯一的commit标识代码,可以用 git log 或者在adgit搭建的web环境历史提交记录里查看. ``` git revert c011eb3c20ba6fb38cc94fe5a8dda366a3990c61 ``` 通常,前几位即可 ``` git revert c011eb3 ``` git revert是用一次新的commit来回滚之前的commit，git reset是直接删除指定的commit 看似达到的效果是一样的,其实完全不同. * 上面我们说的如果你已经push到线上代码库, reset 删除指定commit以后,你git push可能导致一大堆冲突.但是revert 并不会. * 如果在日后现有分支和历史分支需要合并的时候,reset 恢复部分的代码依然会出现在历史分支里.但是revert 方向提交的commit 并不会出现在历史分支里. * reset 是在正常的commit历史中,删除了指定的commit,这时 HEAD 是向后移动了,而 revert 是在正常的commit历史中再commit一次,只不过是反向提交,他的 HEAD 是一直向前的. .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; }</content></entry><entry><title>消息队列常见面试题</title><url>https://1162492411.github.io/docs/post/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories><category>面试题</category><category>Java</category><category>消息队列</category></categories><tags><tag>面试题</tag><tag>Java</tag><tag>消息队列</tag></tags><content type="html"> .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 概述 什么是消息队列 消息队列就是由生产者将消息放到队列中，由于它先进先出的结构特性，消费者会拿最早进去的消息回来进行消费 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 消息队列有什么优点 * 异步 ： * 解耦 ： * 削峰 ： .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 消息队列有什么缺点 * 复杂度上升 ： 引入新组件 * 系统可用性降低 ： 多个业务系统依赖于消息队列，若消息队列故障则会导致系统出错 * 一致性问题 ：例如系统A通过消息队列发送消息给系统BCD，部分系统消费消息失败 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 消息队列有哪些使用场景 * 用户注册后，需要发注册邮件和注册短信 * 用户下单后，订单系统需要通知库存系统 * 用户下单后，需要将下单商品从购物车清除 * 秒杀活动中防止流量暴增超出服务器处理能力，将用户请求串行化 * kafka处理大量日志或者流数据 * 不同业务系统间的通信 * 分布式事务 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 对比一下常见的消息队列 | 特性 | ActiveMQ | RabbitMQ | RocketMQ | Kafka | | ------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | | 开发语言 | java | erlang | java | scala | | 单机吞吐量 | 万级，比 RocketMQ、Kafka 低一个数量级 | 同 ActiveMQ，万级 | 10 万级，支撑高吞吐 | 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 | | topic 数量对吞吐量的影响 | | | topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic | topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 | | 时效性 | ms 级 | 微秒级，这是 RabbitMQ 的一大特点，延迟最低 | ms 级 | 延迟在 ms 级以内 | | 可用性 | 高，基于主从架构实现高可用（master，slave） | 同 ActiveMQ，高，基于主从架构实现高可用（master，slave） | 非常高，分布式架构 | 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 | | 消息可靠性 | 有较低的概率丢失数据 | 基本不丢 | 经过参数优化配置，可以做到 0 丢失 | 同 RocketMQ | | 功能支持 | 成熟的产品，MQ 领域的功能极其完备，在很多公司得到应用；有较多的文档；各种协议支持较好 | 基于 erlang 开发，并发能力很强，性能极好，延时很低；管理界面较丰富 | MQ 功能较为完善，还是分布式的，扩展性好 | 功能较为简单，主要支持简单的 MQ 功能，像一些消息查询，消息回溯等功能没有提供，毕竟是为大数据准备的，在大数据领域的实时计算以及日志采集被大规模使用 | | 社区活跃度 | 活跃度不高 | 活跃 | 阿里出品，目前捐给Apache，活跃度不高 | 活跃 | .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } RabbitMQ Rabbit MQ架构 Rabbit MQ中有哪些核心概念 * Broker:它提供一种传输服务,它的角色就是维护一条从生产者到消费者的路线，保证数据能按照指定的方式进行传输, * Exchange：消息交换机,它指定消息按什么规则,路由到哪个队列。 * Queue:消息的载体,每个消息都会被投到一个或多个队列。 * Binding:绑定，它的作用就是把exchange和queue按照路由规则绑定起来. * Routing Key:路由关键字,exchange根据这个关键字进行消息投递。 * vhost:虚拟主机,一个broker里可以有多个vhost，用作不同用户的权限分离。 * Producer:消息生产者,就是投递消息的程序. * Consumer:消息消费者,就是接受消息的程序. .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 消息投递有哪几种模式 1）Direct直接模式 ：**将消息发给唯一一个节点(队列**) ![image-20210323210337011](https://gitee.com/1162492411/pic/raw/master/组件-消息队列-RabbitMQ-Direct模式.png) 2）Fanout模式 ：将消息一次发送给多个队列 ![image-20210323210442154](https://gitee.com/1162492411/pic/raw/master/组件-消息队列-RabbitMQ-Fanout模式.png) 3）Topic模式 ：通过exchange将消息转发到关心该消息routingkey 的queue 上 ![image-20210323210510957](https://gitee.com/1162492411/pic/raw/master/组件-消息队列-RabbitMQ-Topic模式.png) .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 消费者如何获取消息 推送/拉取 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 生产侧的消息确认模式有哪些 * 事务模式 ： 生产者调用`channel.txSelect()`开启事务，调用`channel.txCommint()`提交事务，调用`channel.txRollback`回滚事务，只有成功地将消息交付给RabbitMQ，事务才算完成 * confirm模式 ：所有在该信道上面发布的消息都会被指派一个唯一的ID，服务端将消息投递到Queue后将回执发送给生产者 * 单条confirm模式 ：每发送一条消息，调用waitForConfirms()方法，等待MQ服务器端confirm；如果服务器端返回false或者超时未返回，生产者会对这条消息进行重传 * 批量confirm模式 ：生产者程序需要定期（每隔多少秒）或者定量（达到多少条）或者两者结合起来publish消息，然后等待服务器端进行confirm * 异步confirm模式 ：提供一个回调方法，服务器端confirm了一条或者多条消息后client端会回调这个方法。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 消费侧的消息确认模式有哪些 * AcknowledgeMode.NONE：无需确认。在消息发送到消费者时，就进行ack，也就认为消费成功了。这种情况下，只有负责队列的进程出现了异常，才会进行nack，其他情况都是ack * AcknowledgeMode.AUTO：根据情况ack。如果消费过程中没有抛出异常，就认为消费成功了，也就进行ack * AmqpRejectAndDontRequeueException，会认为消息失败，拒绝消息，并且requeue = false（也就是不再重新投递） * ImmediateAcknowledgeAmqpException，会进行ack * 其他的异常均进行nack，同时requeue = true（也就是进行重新投递） * AcknowledgeMode.MANUAL：手动ack，可批量ack 对于消费者这一侧的消息确认，RabbitMQ是没有设置超时时间的，理由是并不能确认消息的消费时长。但是一旦消费者的链接断开，这些没有ack的消息是会被重新投递的 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } ack、nack、reject的含义 1. ack代表消息进行了正常消费 2. nack和reject其实是一样的，都是代表消息并没有被正常消费，其区别在于reject每次只能处理一条消息 3. nack和reject之后，消息的处理策略是依据requeue而定的。如果requeue为true，则消息会被重新投递，再次尝试进行消费（如果存在多个消费者，那么有可能不是之前的消费者拿到这个消息）；如果requeue = false，则会判断当前队列是否设置了Dead Letter Exchange(死信队列)，如果有设置，消息会进入这个exchange，如果没有设置，消息会被直接移除 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 哪些情况下消息会丢失 * 生产者未开启事务模式时，消息进入服务端但未刷盘时服务端宕机 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 消息什么时候落盘 * 生产侧非事务场景下，生产者产生的数据进入服务端时先进入buffer，当buffer满1M或者固定等待25ms后消息落盘 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 负载均衡算法有哪些 * 轮询法 ：将请求按顺序轮流地分配到后端服务器上 * 随机法 ：通过随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问 * ip hash法 ： 对ip地址取模 * 加权轮询法 ： 根据服务器权重比分配，权重高的分配的几率大 * 加权随机法 ： 按照权重随机请求后端服务器 * 最小连接数法 ：根据服务器当前的连接情况，动态地选择其中当前积压连接数最少的一台服务器来处理当前的请求 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 有那些特殊队列 * 排它队列 ：该队列仅对首次声明它的连接可见，并且在断开连接时自动进行删除 * 临时队列 ：队列可以设置一个自动删除属性，如果队列没有任何订阅消费者时，会自动删除的 * 优先级队列 ：RabbitMQ本身并没有实现优先级队列，不过有插件可以进行支持（好像3.5之后RabbitMQ自身已经支持了，没有考证） * 死信队列 ：当消息超时或者消息被nack/reject并且设置requeue=false时，会进入这个队列 * Alternate Exchanges : 生产者生产出来的消息，进入exchange之后，找不到合适的queue时，会落到这个队列中 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Kafka .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Rocket MQ</content></entry><entry><title>微服务常见面试题</title><url>https://1162492411.github.io/docs/post/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories><category>面试题</category><category>Java</category><category>微服务</category><category>分布式</category></categories><tags><tag>面试题</tag><tag>Java</tag><tag>微服务</tag><tag>分布式</tag></tags><content type="html"> .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论篇 什么是CAP定理 * C ：一致性，在分布式系统中数据往往存在多个副本，一致性描述的是这些副本中的数据在内容和组织上的一致 * A ：可用性，在用户能够容忍的时间范围内返回用户期望的结果 * P ：分区容错性，在出现网络分区时系统仍然能够对外提供一致性的可用服务 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 什么是BASE理论 它是对CAP理论的进一步延伸 * BA ：基本可用，分布式系统在出现不可预知故障的时候，允许损失部分可用性 * S ： 软状态，许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时 * E ：最终一致性，系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } BASE理论和CAP定理的关系 在CAP定理中，如果满足其中两项(CP/AP)，在此基础上设计的系统就是BASE架构 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 分布式一致性 分布式一致性的定义 **一致性是指**分布式系统中的多个服务节点，给定一系列的操作，在约定协议的保障下，使它们**对外界呈现的状态是一致的。**换句话说**，也就是**保证集群中所有服务节点中的**数据完全相同**并且能够**对某个提案（Proposal）达成一致** .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 分布式一致性的要求 * 有限性 ： 达成一致的结果在**有限的时间**内完成 * 约同性 ：不同节点最终完成决略的结果是相同 * 合法性 ：决策的结果必须是系统中某个节点提出来的 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 分布式一致性的分类 * **严格一致性** ：对于数据项x的任何读操作将返回最近一次对x进行的写操作的结果所对应的值，效果上等同于一台机器 * **强一致性** ： 包括**顺序一致性**和**线性一致性**，**顺序一致性**是指任何执行结果都是相同的，就好像所有进程对数据存储的读、写操作是按某种序列顺序执行的，并且每个进程的操作按照程序所指定的顺序出现在这个序列中；**线性一致性**假设操作具有一个**全局有效时钟的时间戳**，但是这个时钟仅具有有限的精确度。要求时间戳在前的进程先执行 * **弱一致性** ：指系统并不保证后续进程或线程的访问都会返回最新的更新的值，系统在数据成功吸入之后，不承诺立即可以读到最新写入的值，也不会具体承诺多久读到。但是会尽可能保证在某个时间级别（秒级）之后。可以让数据达到一致性状态 * **最终一致性** ：**最终一致性**是弱一致性的特定形式。系统保证在没有后续更新的前提下，系统最终返回上一次更新操作的值。 **也就是说**，如果经过一段时间后要求能访问到更新后的数据，则是最终一致性 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 分布式共识性 分布式共识性的定义 **共识性**描述了分布式系统中多个节点之间，彼此对某个状态达成一致结果的过程。 在实践中，要保障系统满足不同程度的一致性，核心过程往往需要通过共识算法来达成 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 分布式共识性的常见算法 * Paxos * Raft * Proof-of-Work * Proof-of-Stake * Delegated Proof-of-Stake .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 共识性和一致性的区别 **一致性描述的是结果状态**，**共识则是一种手段**。**达成某种共识并不意味着就保障了一致性（这里的一致性指强一致性）。只能说共识机制，能够实现某种程度上的一致** .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 分布式协调 Zookeeper的使用场景 * 命名服务 ： 提供全局一致性的id * 配置管理 ： 将其作为一个高可用的配置存储器，允许分布式应用的参与者检索和更新配置文件 * 分布式锁 ： 通过 ZooKeeper 的临时节点和 Watcher 机制来实现分布式锁 * 集群管理 ： * 通过创建临时节点来建立心跳检测机制 * 分布式系统的每个服务节点还可以将自己的节点状态写入临时节点从而完成节点的状态报告 * 通过数据的订阅和发布功能，ZooKeeper 还能对分布式系统进行模块的解耦和任务的调度 * 通过监听机制，还能对分布式系统的服务节点进行动态上下线，从而实现服务的动态扩容 * Leader节点选举 * 队列管理 ： 实现同步队列/生产者和消费者模型 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 概述Zookeeper的选举流程 * **自增选举轮次** * **初始化选票** * **发送初始化选票** * **接收外部投票** * **判断选举轮次** * **选票 PK** * **统计选票** * **更新服务器状态** .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 概述Zookeeper的ZAB协议的原子广播 **所有的写请求都会被转发给 Leader，Leader 会以原子广播的方式通知 Follow。当半数以上的 Follow 已经更新状态持久化后，Leader 才会提交这个更新，然后客户端才会收到一个更新成功的响应。**这有些类似数据库中的两阶段提交协议。 在整个消息的广播过程中，Leader 服务器会每个事务请求生成对应的 Proposal，并为其分配一个全局唯一的递增的事务 ID(ZXID)，之后再对其进行广播。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 分布式事务 常见的分布式事务协议有哪些 两阶段提交、三阶段提交 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 概述两阶段提交的流程 一阶段 ：投票 二阶段 ：提交 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 两阶段提交有哪些缺点 * 同步阻塞 ： 执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态 * 协调者故障 ：由于协调者的重要性，一旦协调者发生故障，参与者会一直阻塞下去 * 参与者故障导致的数据不一致 ： 在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象 * 协调者与参与者故障导致的事务状态不确定 ：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 概述三阶段提交的流程 与两阶段提交不同的是，三阶段提交有两个改动点。 1、引入超时机制。同时在协调者和参与者中都引入超时机制。 2、在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。 共分为三个阶段 ：canCommit、preCommit、doCommit .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 常见的分布式事务解决方案有哪些 * 全局事务XA协议 * 需要三种角色：AP(业务系统)、TM(事务管理器)、RM(资源管理器),这只是一种规范，具体的实现包括两阶段提交、三阶段提交等，例如MySQL支持外部的XA接口以及MySQL支持内部的两阶段提交(bin log 与redo log) * 基于消息的分布式事务 * 假定存在业务系统A、业务系统B、消息中间件。系统A成功处理后通过消息中间件发出消息，系统B接收消息并处理 * 最大努力通知事务 * 在基于消息的分布式事务基础上，定时处理发送失败的消息 + 定时校对系统AB * TCC两阶段补偿事务 * 分为三个部分 ：Try(检查待执行的业务方，预留资源)、Confirm(执行业务)、Cancel(若前阶段执行失败则回滚) (模仿数据库本地事务，将其逻辑从数据库层迁移到服务层) .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Dubbo Dubbo的工作原理是什么</content></entry><entry><title>网络常见面试题</title><url>https://1162492411.github.io/docs/post/%E7%BD%91%E7%BB%9C%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories><category>网络</category><category>面试题</category></categories><tags><tag>网络</tag><tag>面试题</tag></tags><content type="html"> .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 网络分层 网络模型分为哪几类 1. OSI七层模型 ： 物理层、链路层、网络层、传输层、会话层、表示层、应用层 2. TCP/IP四层模型 ： 接口层、网际层、传输层、应用层 3. 五层模型 ： 物理层、链路层、网络层、传输层、应用层 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 概述OSI七层模型各层的作用及各层常见协议 自下而上依次为 : 物理层、链路层、网络层、传输层、会话层、表示层、应用层 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 分层 作用 协议 设备 关键字 应用层 负责顶层协议，例如数据封装、分配IP、DNS解析 HTTP、FTP、Telnet、DNS 浏览器、APP 表示层 转换、压缩、加密数据 ASCII、PNG、JPEG 浏览器、APP 会话层 负责两个应用进程之间的逻辑连接 SSL、TLS、、SQL、RPC 浏览器、APP 传输层 传输数据，确保数据包按顺序接收且没有被破坏 TCP、UDP 计算机 流量控制、拥塞控制 网络层 为网络上的不同主机提供通信 IPv4、IPv6、ARP、ICMP 链路层 采用差错检测、差错控制等方法，以帧为单位向网络层提供高质量的数据传输服务 点对点协议PPP、广播协议 交换机、路由器 MAC、MTU、CRC、滑动窗口 物理层 利用传输介质为相邻的计算机节点完成比特流的透明传送 IEEE802.X、Bluetooth、WI-FI协议、USB接口协议 网卡、网线 全双工通信、信道复用 三次握手和四次挥手 这里先放一张TCP状态变迁图
简述HTTP三次握手的流程 为什么需要三次握手 三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。简单来说，基本思想就是**“让我知道你已经知道”**了 第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常 第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常 第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常 所以三次握手就能确认双发收发功能都正常，缺一不可。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 第二次握手为什么回传SYN SYN用于建立并确认从服务端到客户端的通信。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 什么是半连接队列 服务器第一次收到客户端的 SYN 之后，就会处于 SYN_RCVD 状态，此时双方还没有完全建立其连接，服务器会把此种状态下请求连接放在一个**队列**里，我们把这种队列称之为**半连接队列** .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 什么是全连接队列 已经完成三次握手，建立起连接的就会放在全连接队列中。如果队列满了就有可能会出现丢包现象 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 三次握手过程中可以携带数据吗 **第一次、第二次握手不可以携带数据**，但是第三次可以。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 什么是SYN攻击，如何防范 SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server则回复确认包，并等待Client确认，由于源地址不存在，因此Server需要不断重发直至超时，这些伪造的SYN包将长时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络拥塞甚至系统瘫痪 常见的防御 SYN 攻击的方法有如下几种： - 缩短超时（SYN Timeout）时间 - 增加最大半连接数 - 过滤网关防护 - SYN cookies技术 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 简述四次挥手的流程 四次挥手时为什么客户端最后还要等待2MSL 第一，保证客户端发送的最后一个ACK报文能够到达服务器，因为这个ACK报文可能丢失，站在服务器的角度看来，我已经发送了FIN+ACK报文请求断开了，客户端还没有给我回应，应该是我发送的请求断开报文它没有收到，于是服务器又会重新发送一次，而客户端就能在这个2MSL时间段内收到这个重传的报文，接着给出回应报文，并且会重启2MSL计时器。 第二，防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中。客户端发送完最后一个确认报文后，在这个2MSL时间中，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样新的连接中不会出现旧连接的请求报文 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 为什么建立连接是三次握手，关闭连接确是四次挥手呢 简单来说，建立连接时服务器端的ACK/SYN一次性发送给客户端，但关闭连接时ACK/FIN一般分为两次发送给客户端(目的是使得服务端传送完毕数据)。详细来说, * 建立连接的时候， 服务器在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。 * 关闭连接时，服务器收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，而自己也未必全部数据都发送给对方了，所以己方可以立即关闭，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送，从而导致多了一次。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如果已经建立了连接，但是客户端突然出现故障了怎么办 与UDP相比，TCP还设有一个保活计时器，服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。相对应的配置项为net.ipv4.tcp_keepalive_time、net.ipv4.tcp_keepalive_intvl、net.ipv4.tcp_keepalive_probes。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } TCP/UDP TCP和UDP的区别 TCP如何保证可靠传输 * 分块 ：应用数据被分割成 TCP 认为最适合发送的数据块。 * 编号 ：TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 * 校验和 ： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 * 去重 ：TCP 的接收端会丢弃重复的数据。 * 流量控制 ： TCP 连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制） * 拥塞控制 ：当网络拥塞时，减少数据的发送。 * ARQ协议 ：也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 * 超时重传 ：当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } HTTP/HTTPS HTTP1.0与HTTP1.1有什么区别 * 长连接 ： 1.0默认短连接，1.1默认长连接，并且1.1支持流水线，可以多个HTTP连接共用同一个TCP连接 * 缓存机制 ：1.0使用header中的If-Modified-Since,Expires控制缓存，1.1更加丰富，如Entity tag，If-Unmodified-Since, If-Match, If-None-Match * 带宽优化 ：1.1支持通过header中指定range来获取资源的某个部分 * 错误码 ：1.1相比1.0增加了24个错误码 * Host处理 ：1.0并没有host，而1.1强制必须存在host .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } HTTP1.1和HTTP2.0有什么区别 * 本质区别 ：1.1基于文本分割, 2.0基于二进制 * 多路复用 ：2.0支持在同一个TCP中存在多个流，也就是可以发送多个请求，对端可以通过帧中的标识知道属于哪个请求 * header压缩 ：1.1每次都需要发送header，2.0将header压缩，同时通信的两端各自维护了header键名索引 * 服务端推送 ：2.0可以在服务端主动推送数据到客户端 * 流量控制 ：2.0中的通信双方可以在请求时声明需要的数据字节大小 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } QUIC相对HTTP2有什么区别 * 传输层协议 ：QUIC基于UDP，HTTP2基于TCP * 连接建立时间 ：QUIC可以实现0-RTT建立连接，而TCP与SSL/TLS(1.0,1.1,1.2)每次建连需要TCP三次握手+安全握手，需要4~5个RRT * 拥塞控制 ：多个数据在TCP连接上传输时，若一个数据包出现问题，TCP需要等待该包重传后，才能继续传输其它数据包。但在QUIC中，因为其基于UDP协议，UDP数据包在出问题需要重传时，并不会对其他数据包传输产生影响 * 重传机制 ：QUIC协议的每个数据包除了本身的数据以外，会带有其他数据包的部分数据，在少量丢包的情况下，可以使用其他数据包的冗余数据完成数据组装而无需重传 * 重启会话 ：TCP是基于两端的ip和端口和协议来建立连接，而QUIC基于特有的Connection ID 来建立连接 * 头部加密 ：TCP 协议头部未经过加密和认证，但 QUIC 所有的头部信息均经过认证，并且对所有信息进行加密，可有效避免数据传输过程中被中间设备截取并篡改 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } HTTP和HTTPS的区别 * 端口 ：http默认80端口，https默认443端口 * 安全性 ：http明文传输，https加密传输，安全性高 * 连接建立时间与响应时间 ：http无需加密，只需要三次握手，建立连接时间短，https需要先三次握手再ssl握手，建立连接时间长 * 消耗资源 ：https需要额外加解密，相比http更加消耗资源 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } SSL和TLS什么关系 TLS是SSL标准化的产物。早期SSL分为1.0、2.0、3.0版本，SSL3.0 = TLS1.0，TLS后续又产生了1.1、1.2版本等 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 概述ssl握手流程 1.客户端通知加密算法,双方根据加密算法来交换加密所需的证书、随机数、配置参数等信息 2.根据上一步的加密算法及相关参数计算出预主密钥 3.根据预主密钥生成主密钥 4.双方互相发送握手完成信息，后续根据主密钥来加密通信数据 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 给出基于RSA的SSL握手流程和基于DH的SSL握手流程 RSA的SSL握手流程
DH的SSL握手流程
概述给予RSA和DH算法的SSL握手流程的区别 区别就在于密钥交换与身份认证 * RSA非对称算法，利用客户端利用公钥加密预主密钥发送给服务端完成密钥交换，服务端利用私钥解密完成身份认证 * DH对称算法，利用各自发送DH参数完成密钥交换，服务器私钥签名数据，客户端公钥验签完成身份认证 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } SSL握手阶段连接意外中断后的恢复方法有哪些，并比较它们的优缺点 * session id * 流程 ：当 Client 通过一次完整的握手，与 Server 建立了一次完整的 Session，Server 会记录这次 Session 的信息，以备恢复会话的时候使用，其中就包含session id * 优点 ：将握手耗时从2-RTT减少为1-RTT；减少双方的负载，不需要再次消耗CPU计算 * 缺点 ：session id仅存在于单机中 * session ticket * 流程 ：服务器取出它的所有会话数据（状态）并进行加密 (密钥只有服务器知道)，再以票证的方式发回客户端；客户端恢复会话时在 ClientHello 的扩展字段 session_ticket 中携带加密信息将票证提交回服务器，由服务器检查票证的完整性，解密其内容，再使用其中的信息恢复会话 * 优点 ：可用于多台机器 * 缺点 ：存储ticket需要消耗内存 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } TLS1.3 相比TLS1.2有什么区别 * 密钥协商机制 ：引入新的密钥协商机制PSK * 连接建立时间 ：支持 0-RTT 数据传输，在建立连接时节省了往返时间 * 加密算法 ：废弃了 3DES、RC4、AES-CBC 等加密组件，废弃了 SHA1、MD5 等哈希算法 * 安全性 ：ServerHello 之后的所有握手消息采取了加密操作，可见明文大大减少 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 概述QUIC的握手流程</content></entry><entry><title>MyBatis常见面试题</title><url>https://1162492411.github.io/docs/post/mybatis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories><category>面试题</category><category>框架</category><category>MyBatis</category></categories><tags><tag>面试题</tag><tag>框架</tag><tag>MyBatis</tag></tags><content type="html"> 基础篇 # 和$的区别 - `${}`是 Properties 文件中的变量占位符，它可以用于标签属性值和 sql 内部，属于静态文本替换，比如${driver}会被静态替换为`com.mysql.jdbc.Driver`。 - `#{}`是 sql 的参数占位符，MyBatis 会将 sql 中的`#{}`替换为?号，在 sql 执行前会使用 PreparedStatement 的参数设置方法，按序给 sql 的?号占位符设置参数值，比如 ps.setInt(0, parameterValue)，`#{item.name}` 的取值方式为使用反射从参数对象中获取 item 对象的 name 属性值，相当于 `param.getItem().getName()`。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Xml映射文件中，除了常见的select|insert|updae|delete标签之外，还有哪些标签 、、、、 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } MyBatis的动态SQL是什么 Mybatis动态sql可以让我们在Xml映射文件内，以标签的形式编写动态sql，完成逻辑判断和动态拼接sql的功能，Mybatis提供了9种动态sql标签trim|where|set|foreach|if|choose|when|otherwise|bind。 其执行原理为，使用OGNL从sql参数对象中计算表达式的值，根据表达式的值动态拼接sql，以此来完成动态sql的功能 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式 第一种是使用标签，逐一定义列名和对象属性名之间的映射关系。第二种是使用sql列的别名功能，这种方式原理是反射。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里 Hibernate属于全自动ORM映射工具，使用Hibernate查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而Mybatis在查询关联对象或关联集合对象时，需要手动编写sql来完成，所以，称之为半自动ORM映射工具 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } TypeHandler的作用有哪些 * 完成javaType至jdbcType的转换 * 完成javaType至jdbcType的转换 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 核心原理篇 MyBatis有哪些主要组件 * Transaction ：事务接口，所有操作最终由该接口 * TransactionFactory ： 负责Transaction的创建、销毁 * SqlSessionFactory：负责SqlSession的创建、销毁 * SqlSession ： 负责提供给用户可以操作的api，如insert(),insertBatch()等，它的生命周期限定在线程之内 * Executor ： 负责执行对数据库的操作 * StatementHandler ：Executor将工作委托给StatementHandler执行（实际干活的老实人） .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Mybatis的执行流程 简述SQL在Myabtis中的执行流程 简述各Executor子类的作用 * SimpleExecutor ：简单Executor，每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象 * ReuseExecutor ： 重用Executor，执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map内，供下一次使用。在执行commit、rollback等动作前，将会执行flushStatements()方法，将Statement对象逐一关闭 * BatchExecutor ：批量Executor，将所有sql都添加到批处理中（addBatch()），缓存了多个Statement对象，sql添加完成后统一执行（executeBatch()） * CachingExecutor ： 缓存Executor，装饰模式的应用，先从缓存中获取查询结果，存在就返回，不存在，再委托给Executor delegate去数据库取，delegate可以是上面任一的SimpleExecutor、ReuseExecutor、BatchExecutor .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 简述各StatementHandler子类的作用 * SimpleStatementHandler：用于处理**Statement**对象的数据库操作 * PreparedStatementHandler：用于处理**PreparedStatement**对象的数据库操作。 * CallableStatementHandler：用于处理存储过程 * RoutingStatementHandler ： 根据statementType来创建其他三个StatementHandler对象 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 简述KeyGenerator各子类的作用 * NoKeyGenerator : 空实现，不需要处理主键 * Jdbc3KeyGenerator : 用于处理数据库支持自增主键的情况，如MySQL的auto_increment * SelectKeyGenerator : 用于处理数据库不支持自增主键的情况，比如Oracle的sequence序列 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 简述设计模式在MyBatis中的运用 * 工厂模式 ： TransactionFactory负责Transaction的生产、销毁；SqlSessionFactory负责SqlSession的生产、销毁 * 装饰器模式 ： CachingExecutor在SimpleExecutor/ReuseExecutor/BatchExecutor的基础上提供了缓存功能 * 模版模式 ： Executor通过调用StatementHandler的模版方法来完成对数据库的操作 * 适配器模式 ： BaseStatementHandler抽象类分别有三个实现类：SimpleStatementHandler、PreparedStatementHandler、CallableStatementHandler * 策略模式 ： RoutingStatementHandler根据statementType的不同来创建不同的StatementHandler * 责任链模式 ： MyBatis中存在一些插件，它们都会以责任链的方式逐一执行 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 简述一级缓存和二级缓存的原理 * 一级缓存 ： 在CachingExecutor中针对query操作的结果，将其放置在HashMap中，有效范围为同一个SqlSession(默认)/Statement * 二级缓存 ： 在CachingExecutor中实现，有效范围为全局Configuration，在所有SqlSession中均有效 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 一级缓存在哪些情况下失效 * sqlsession变了 缓存失效 * sqlsession不变,查询条件不同，一级缓存失效 * sqlsession不变,中间发生了增删改操作，一级缓存失败 * sqlsession不变,手动清除缓存，一级缓存失败 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 缓存清空策略有哪些 * LRU ：最近最少使用算法，即如果缓存中容量已经满了，会将缓存中最近做少被使用的缓存记录清除掉，然后添加新的记录 * FIFO ：先进先出算法，如果缓存中的容量已经满了，那么会将最先进入缓存中的数据清除掉 * Scheduled ：指定时间间隔清空算法，该算法会以指定的某一个时间间隔将Cache缓存中的数据清空 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Mybatis是否支持延迟加载？如果支持，它的实现原理是什么 Mybatis仅支持association关联对象和collection关联集合对象的延迟加载，association指的就是一对一，collection指的就是一对多查询。在Mybatis配置文件中，可以配置是否启用延迟加载lazyLoadingEnabled=true|false。 它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，拦截器invoke()方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，然后调用a.setB(b)，于是a的对象b属性就有值了，接着完成a.getB().getName()方法的调用。这就是延迟加载的基本原理 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 实战篇 通常一个Xml映射文件都有一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗 * Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为key值，可唯一定位一个MappedStatement，举例：com.mybatis3.mappers.StudentDao.findStudentById，可以唯一找到namespace为com.mybatis3.mappers.StudentDao下面id = findStudentById的MappedStatement * Dao接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Dao接口生成代理proxy对象，代理对象proxy会拦截接口方法，转而执行MappedStatement所代表的sql，然后将sql执行结果返回 * 因此，Dao接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复 不同的Xml映射文件，如果配置了namespace，那么id可以重复；如果没有配置namespace，那么id不能重复；毕竟namespace不是必须的 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Mybatis是如何进行分页的？分页插件的原理是什么 Mybatis使用RowBounds对象进行分页，它是针对ResultSet结果集执行的内存分页，而非物理分页，可以在sql内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。 分页插件的基本原理是使用Mybatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，根据dialect方言，添加对应的物理分页语句和物理分页参数。 举例：select * from student，拦截sql后重写为：select t.* from （select * from student）t limit 0，10 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } MyBatis单条数据插入如何返回主键 * MySQL ： 在中将useGeneratedKeys属性设置为true，并制定keyProperty为实体对象的id * Oracle ： ```xml SELECT SEQ_TEST.NEXTVAL FROM DUAL insert into category (name_zh, parent_id, show_order, delete_status, description ) values xxxx ``` .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } MyBatis批量插入如何返回主键列表 Xml ```xml INSERT INTO (relation_id, summary_id, relation_type) VALUES ( #{shopResource.relationId}, #{shopResource.summaryId}, #{shopResource.relationType} ) ``` Dao ```java public List batchinsertCallId(List shopResourceList) { this.getSqlSession().insert(getStatement(SQL_BATCH_INSERT_CALL_ID), shopResourceList); return shopResourceList;// 重点介绍 } ``` MyBatis需要在3.3.1以上，如果在Dao中使用@Param注解，需要MyBatis3.5以上 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 插件篇 插件存储于哪里 初始化时，会读取插件，保存于Configuration对象的InterceptorChain中 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何编写插件 * 实现org.apache.ibatis.plugin.Interceptor接口 * 配置@Intercepts注解 ：在该注解中配置对哪些Mapper的哪些方法进行拦截 * 重写setProperties()方法：给自定义的拦截器传递xml配置的属性参数 * 重写plugin()方法：决定是否触发intercept()方法 * 重写intercept()方法：执行拦截内容的地方 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 插件可以拦截哪些MyBatis核心对象 Executor、StatementHandler、ParameterHandler、ResultSetHandler .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } RowBounds分页插件的原理 org.apache.ibatis.executor.resultset.DefaultResultSetHandler.handleRowValuesForSimpleResultMap()方法 ： 对取到的结果集在内存中分页 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } PageHelper分页插件的原理 com.github.pagehelper.PageInterceptor ： 1.将分页参数等绑定在ThreadLocal中 2.查询总数 3.改写分页sql，添加limit或者rownum等 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 占 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; }</content></entry><entry><title>Java常见面试题</title><url>https://1162492411.github.io/docs/post/java%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories><category>面试题</category><category>Java</category></categories><tags><tag>面试题</tag><tag>Java</tag><tag>Java基础</tag></tags><content type="html"> 基础篇 Object都有哪些方法，各自作用是什么 对象相等的相关方法：equals()、hashcode(); 对象的基本方法 ： toString()、getClass()、clone()、finalize() 锁相关方法 : wait()、nofity()、notifyAll() .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Java有哪些修饰符/作用域 private、default、protected、public .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 数据结构篇 HashCode为什么采用31作为乘数 1. 31 是一个奇质数，如果选择偶数会导致乘积运算时数据溢出 2. 使用 31、33、37、39 和 41 作为乘积，得到的碰撞几率较小 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } HashMap底层实现 1.7及以前是数组+链表，1.8以后是数组+链表+红黑树 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 为什么HashMap初始化时采用数组+链表而不是红黑树 * 时空平衡 ：数组+链表时间复杂度小,为O(1),插入方便，并且单个节点内存小；红黑树时间复杂度高,为O(n)，插入复杂，它需要通过旋转来平衡，并且单个节点内存大 * 数据分布 : 当hashCode算法足够好时，数据会尽量均匀分布，几乎不会出现单个链表元素数量超过8的情况；当hashCode算法不好时，本身hashmap做了一次hash扰动了，并且足够理想的情况下，随机数据分散程度遵循泊松分布，几率为0.0000006，也几乎不会出现单个链表元素数量超过8的情况 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } HashMap在1.8中为什么相比1.7增加了红黑树 防止恶意的hashcode导致数据分布不均匀，在某些链表中元素过多导致查询效率变低 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } HashMap在什么情况下会出现数组+链表 &amp;lt;&amp;ndash;&amp;gt;红黑树 的相互转换，为什么 * 链表长度为8时转换为红黑树 * 红黑树元素数量为6时转换为数组+链表 * 原因 ： 理想情况下随机hashCode算法下所有bin中节点的分布频率会遵循泊松分布，为6的概率为十万分之一，为7的概率为十万分之一的百分之七，为8的概率为十万分之一的万分之四。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 链表和红黑树是否可能共存在同一个HashMap 是的，数组+链表/数组+红黑树，各个数组间互相不影响 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } HashMap的默认初始大小为什么是16，扩容阈值为什么是0.75 * 初始化大小为16 ： 分配过小容易扩容，分配过大浪费资源 * 扩容阈值为0.75 ： 经验所得 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } HashMap的put如何实现 1.对key的hashCode()做hash，然后再计算index; 2.校验桶数组是否被初始化 ： 如果未被初始化则进行初始化 3.校验某个桶中是否为空 : 如果为空，将本次待插入的数据放入该桶;如果桶非空 3.1 如果目前是红黑树,调用红黑树的插入方法，并将插入后的数据赋值给临时变量e 3.2 如果不是红黑树，并且当前桶的首个数据等于本次待插入的数据，将本次待插入的数据赋值给临时变量e 3.3 其他情况下遍历整个链表,查找待插入数据是否已存在于该Map，如果存在则赋值给临时变量e，如果不存在也将待插入的数据赋值给临时变量e 3.4 上边三步进行完之后，如果临时变量e非空，将Map中指定位置的值替换为本次待插入的数据，同时执行afterNodeAccess扩展方法 4.键值对数量超过阈值时，则进行扩容 5.执行afterNodeInsertion扩展方法 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } HashMap的插入方式在1.7和1.8有什么区别 1.7头插法并且会使得链表反转，1.8尾插法 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } HashMap扩容策略在1.7和1.8有什么区别 * 数据插入与扩容顺序 ： 1.7先扩容再插入数据，1.8先插入数据再扩容 * 插入方法 ： 1.7头插法，1.8尾插法 * 数据移动 ： 1.7重新计算rehash，1.8要么数据保留在原位要么固定向后移动n位(n指扩容前的hashmap大小) .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } HashMap是否是线程安全的，扩容时的锁在什么情况下会出现 * 不安全，导致不安全的情况包括以下几种 * 多线程put导致扩容时会形成环形结构从而导致死循环 * modCount的修改不是原子性的 * 扩容时由于插入数据导致判断的值不一定准确 * 锁 (todo占位) .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何实现线程安全的HashMap * 操作时采用 Synchronized/直接改用HashTable * 使用Collections.synchronizedMap * 改用ConcurrentHashMap .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } ConcurrentHashMap如何实现线程安全 * 使用volatile保证当Node中的值变化时对于其他线程是可见的 * 使用table数组的头结点作为synchronized的锁来保证写操作的安全 * 当头结点为null时，使用CAS操作来保证数据能正确的写入 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } IO篇 UNIX 系统有哪些常见的IO模型 UNIX 系统下， IO 模型一共有 5 种： **同步阻塞 I/O**、**同步非阻塞 I/O**、**I/O 多路复用**、**信号驱动 I/O** 和**异步 I/O** .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Linux中几种IO模型有什么不同 Java中有哪些IO模型 IO流程实际上包括两个阶段： `发起调用`与`实际IO`，发起调用指的是用户态发起系统调用，内核态进行数据的准备(网络 I/O 的情况就是等待远端数据陆续抵达；磁盘I/O的情况就是等待磁盘数据从磁盘上读取到内核态内存中)，实际IO指的是内核态将数据拷贝到用户态。 阻塞/非阻塞指的是发起调用阶段，线程/调用者是否需要等待该阶段的完成，如果需要等待(阻塞)，那么在该阶段完成前一直等待，如果不需要等待(非阻塞)，在该阶段完成前调用者可以继续做其他事情； 同步/非同步指的是实际IO阶段，线程/调用者是否需要自己参与（即线程是否需要询问IO操作完成），如果需要参与(同步)，那么线程需要不断轮询实际IO是否完成，如果不需要参与(非同步)，那么就是内核态完成实际IO后主动通知/回调线程 BIO ： 同步阻塞 NIO ： 同步非阻塞 AIO/NIO2 ： 异步非阻塞 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 线程篇 线程 - 线程和进程的区别 进程是资源分配的最小单位，线程是CPU调度的最小单位 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 线程 - 进程和线程中各自存储什么内容 | 进程 | 线程 | | ------------------ | ---------- | | 地址空间 | 程序计数器 | | 全局变量 | 寄存器 | | 打开文件 | 堆栈 | | 子进程 | 状态 | | 即将发生的报警 | | | 信号与信号处理程序 | | | 账户信号 | | | 同步、互斥信号量 | | .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 线程 - 线程有哪几种状态 线程 - 线程的实现方式有哪些，这些方式之间有什么区别 * 继承Thread类、实现Runnable接口、实现Callback接口 * 实现Runnable/Callable接口相比继承Thread类的优势 * 适合多个线程进行资源共享 * 可以避免java中单继承的限制 * 增加程序的健壮性，代码和数据独立 * 线程池只能放入Runable或Callable接口实现类，不能直接放入继承Thread的类 * Callable和Runnable的区别 * call()方法执行后可以有返回值，run()方法没有返回值 * Callable重写的是call()方法，Runnable重写的方法是run()方法 * call()方法可以抛出异常，run()方法不可以 * 运行Callable任务可以拿到一个Future对象，表示异步计算的结果 。通过Future对象可以了解任务执行情况，可取消任务的执行，还可获取执行结果 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 线程 - Thread类包含start()和run()方法，它们的区别是什么 start() : 它的作用是启动一个新线程，新线程会执行相应的run()方法。start()不能被重复调用。 run() : run()就和普通的成员方法一样，可以被重复调用。单独调用run()的话，会在当前线程中执行run()，而并不会启动新线程 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 线程 - 为什么notify(), wait()等函数定义在Object中，而不是Thread中 notify(), wait()依赖于“同步锁”，而“同步锁”是对象锁持有，并且每个对象有且仅有一个 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 线程 - sleep() 与 wait()的比较 wait()的作用是让当前线程由“运行状态”进入“等待(阻塞)状态”的同时，也会释放同步锁。 而sleep()的作用是也是让当前线程由“运行状态”进入到“休眠(阻塞)状态”。 但是，wait()会释放对象的同步锁，而sleep()则不会释放锁 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 线程 - join()方法的作用和原理 作用是让“主线程”等待“子线程”结束之后才能继续运行，原理就是对应的native方法中先是主线程调用了wait然后在子线程threadA执行完毕之后，JVM会调用lock.notify_all(thread)来唤醒就是主线程 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 线程 - nofity和nofityAll的区别 notify()方法只随机唤醒一个 wait 线程，而notifyAll()方法唤醒所有 wait 线程 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 线程 - 如何实现线程安全，各个实现方法有什么区别 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 怎么唤醒一个阻塞的线程 * wait()、notify() ：在synchronized中调用wait来释放当前线程的锁，然后调用notify来随机唤醒其他线程 * await()、signal() ： 使用Condition对象提供的await()来释放当前线程的锁，然后调用signal()来随机唤醒其他线程 * park()、unpark() : 使用LockSupport提供的park获取许可证(如果许可证的状态是未被获取，那么将获取许可证；否则将会阻塞，该方法不支持重入，多次调用会导致阻塞，许可证默认状态是已被获取)，unpark释放许可证(该方法可多次调用，不会影响许可证的获取) .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 线程池 - JDK自带的有哪几种线程池 * newFixedThreadPool ：创建指定线程数量的线程池，无界阻塞队列LinkedBlockingQueue，适合执行较快的任务, * newCachedThreadPool ：根据需要自动创建线程，无界阻塞队列SynchronousQueue,适合用在**短时间内有大量短任务的场景** * newScheduledThreadPool ： 主要用来延迟执行任务或者定期执行任务，延迟队列DelayedWorkQueue * newWorkStealingPool : 窃取任务，并行stream就是使用的该线程池，建议线程数量为cpu核数 - 1 * newSingleThreadExecutor : 固定一个线程，无界阻塞队列LinkedBlockingQueue，**能保证任务是按顺序执行** .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 线程池 - 线程池的参数有哪些，各自作用是什么 * CorePoolSize：线程池创建时候初始化的线程数,默认1 - MaxPoolSize：线程池最大的线程数，只有在缓冲队列满了之后才会申请超过核心线程数的线程，默认Integer.MAX - QueueCapacity：用来缓冲执行任务的队列的队列大小，默认Integer.MAX - KeepAliveSeconds：线程的空闲时间，单位/s，当超过了核心线程出之外的线程在空闲时间到达之后会被销毁,默认60 - ThreadNamePrefix：线程池中线程名的前缀，继承自父类ExecutorConfigurationSupport，默认是BeanName/方法名 - RejectedExecutionHandler：线程池对拒绝任务的处理策略，自父类ExecutorConfigurationSupport,（策略为JDK ThreadPoolExecutor自带） - AbortPolicy：默认策略，直接抛出异常 RejectedExecutionException - CallerRunsPolicy：直接在 execute 方法的调用线程中运行被拒绝的任务；如果执行程序已关闭，则会丢弃该任务 - DiscardPolicy：该策略直接丢弃 - DiscardOldestPolicy：该策略会先将最早入队列的未执行的任务丢弃掉，然后尝试执行新的任务。如果执行程序已关闭，则会丢弃该任务 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 线程池的execute和submit的区别与联系 * 任务类型 ：execute只能提交Runnable类型的任务，而submit既能提交Runnable类型任务也能提交Callable类型任务 * 异常 ： execute直接抛出异常，submit会吃掉异常，可用future的get捕获 * 顶层接口 ：execute所属顶层接口是Executor,submit所属顶层接口是ExecutorService .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - Java的线程模型 线程又分为用户线程和内核线程。 * 用户线程：语言层面创建的线程，比如 java语言中多线程技术，通过语言提供的线程库来创建、销毁线程。 * 内核线程：内核线程又称为守护线程 Daemon线程，用户线程的运行必须依赖内核线程，通过内核线程调度器来分配到相应的处理器上。 Java的线程模型采用的是一对一，即一个内核线程对应一个用户线程。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 并发篇 理论 - 并发与并行的区别 * 并发 ： 同**一个时间段内**多个任务都在执行 * 并行 ： 在**单位时间内**多个任务都在执行 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 主内存和工作内存各自存储什么 * 主内存 —— 即*main memory*。在java中，实例域、静态域和数组元素是线程之间共享的数据，它们存储在**主内存**中。 * 本地内存 —— 即*local memory*。 局部变量，方法定义参数 和 异常处理器参数是不会在线程之间共享的，它们存储在线程的**本地内存**中。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 并发编程三要素 * 原子性 ：不可分割，一个或多个操作要么全部执行成功要么全部执行失败。它们不会被线程打断 * 有序性 ：程序执行的顺序按照代码的先后顺序执行 * 可见性 ：一个线程对共享变量的修改,另一个线程能够立刻看到 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 为什么会产生原子性问题 对于 64 位的数据，如 long 和 double，允许虚拟机实现选择可以不保证 64 位数据类型的 load、store、read 和 write 这四个操作的原子性，即如果有多个线程共享一个并未声明为 volatile 的 long 或 double 类型的变量，并且同时对它们进行读取和修改操作，那么某些线程可能会读取到一个既非原值，也不是其他线程修改值的代表了“半个变量”的数值 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 为什么会产生有序性问题 “编译器和处理器”为了提高性能，在程序执行时会对程序进行重排序，这打破了有序性 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 为什么会产生可见性问题 * 线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存中的变量。 * 线程之间无法直接访问对方的工作内存中的变量，线程间变量的传递均需要通过主内存来完成。 * 简言之，只要直接采用了多线程的并发模型，并采用共享内存的方式作为数据的通讯方式，就一定有可见性问题 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 什么是上下文切换 * 含义 ：CPU从一个进程或线程切换到另一个进程或线程 * 内容 ：上下文是指某一时间点 CPU 寄存器和程序计数器的内容 * 切换种类 ： * 线程切换 : 同一进程中的两个线程之间的切换 * 进程切换 : 两个进程之间的切换 * 模式切换 : 在给定线程中，用户模式和内核模式的切换 * 地址空间切换 : 将虚拟内存切换到物理内存 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 概述进程切换的步骤 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 概述线程切换的步骤 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 线程切换的原因有哪些 引起线程上下文切换的原因，主要存在三种情况如下： 1. **中断处理**：在中断处理中，其他程序”打断”了当前正在运行的程序。当CPU接收到中断请求时，会在正在运行的程序和发起中断请求的程序之间进行一次上下文切换。**中断分为硬件中断和软件中断**，软件中断包括因为IO阻塞、未抢到资源或者用户代码等原因，线程被挂起。 2. **多任务处理**：在多任务处理中，CPU会在不同程序之间来回切换，每个程序都有相应的处理时间片，CPU在两个时间片的间隔中进行上下文切换。 3. **用户态切换**：对于一些操作系统，当进行用户态切换时也会进行一次上下文切换，虽然这不是必须的。 对于我们经常 **使用的抢占式操作系统** 而言，引起线程上下文切换的原因大概有以下几种： 1. 当前执行任务的时间片用完之后，系统CPU正常调度下一个任务； 2. 当前执行任务碰到IO阻塞，调度器将此任务挂起，继续下一任务； 3. 多个任务抢占锁资源，当前任务没有抢到锁资源，被调度器挂起，继续下一任务； 4. 用户代码挂起当前任务，让出CPU时间； 5. 硬件中断； .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 上下文切换有哪些损耗 1. **直接消耗**：指的是CPU寄存器需要保存和加载, 系统调度器的代码需要执行, TLB实例需要重新加载, CPU 的pipeline需要刷掉； 2. **间接消耗**：指的是多核的cache之间得共享数据, 间接消耗对于程序的影响要看线程工作区操作数据的大小 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何减少线程的上下文切换 * 无锁并发**：多线程竞争时，会引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的ID按照Hash取模分段，不同的线程处理不同段的数据； * CAS算法**：Java的Atomic包使用CAS算法来更新数据，而不需要加锁； * 最少线程**：避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态； * 使用协程**：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换(Java没有协程，线程模型限制所致) .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 什么是happens-before原则 在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这2个操作之间必须要存在happens-before关系。 - 定义: 如果一个操作在另一个操作之前发生(happens-before),那么第一个操作的执行结果将对第二个操作可见, 而且第一个操作的执行顺序排在第二个操作之前。 - 两个操作之间存在happens-before关系，并不意味着一定要按照happens-before原则制定的顺序来执行。如果重排序之后的执行结果与按照happens-before关系来执行的结果一致，那么这种重排序并不非法。 - happens-before规则： 1. 程序次序规则：在一个线程内一段代码的执行结果是有序的。就是还会指令重排，但是随便它怎么排，结果是按照我们代码的顺序生成的不会变！ 2. 锁定规则：一个unLock操作先行发生于后面对同一个锁的lock操作；论是单线程还是多线程，必须要先释放锁，然后其他线程才能进行lock操作 3. volatile变量规则：就是如果一个线程先去写一个volatile变量，然后一个线程去读这个变量，那么这个写操作的结果一定对读的这个线程可见。 4. 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C 5. 线程启动规则：在主线程A执行过程中，启动子线程B，那么线程A在启动子线程B之前对共享变量的修改结果对线程B可见 6. 线程终止规则：在主线程A执行过程中，子线程B终止，那么线程B在终止之前对共享变量的修改结果在线程A中可见。 7. 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程代码检测到中断事件的发生，可以通过Thread.interrupted()检测到是否发生中断 8. 对象终结规则：这个也简单的，就是一个对象的初始化的完成，也就是构造函数执行的结束一定 happens-before它的finalize()方法。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 什么是as-if-serial语义 不管怎么重排序(编译器和处理器为了提高并行度做的优化),(单线程)程序的执行结果不会改变 - 有序性规则表现在以下两种场景: 线程内和线程间 1. 线程内: 指令会按照一种“串行”(as-if-serial)的方式执行，此种方式已经应用于顺序编程语言。 2. 线程间: 一个线程“观察”到其他线程并发地执行非同步的代码时，任何代码都有可能交叉执行。唯一起作用的约束是：对于同步方法，同步块以及volatile字段的操作仍维持相对有序。 - As-if-serial只是保障单线程不会出问题，所以有序性保障，可以理解为把As-if-serial扩展到多线程，那么在多线程中也不会出现问题 - 从底层的角度来看，是借助于处理器提供的相关指令内存屏障来实现的 - 对于Java语言本身来说，Java已经帮我们与底层打交道，我们不会直接接触内存屏障指令，java提供的关键字synchronized和volatile，可以达到这个效果，保障有序性（借助于显式锁Lock也是一样的，Lock逻辑与synchronized一致） .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - JMM有哪八个原子操作指令 * **read** 读取：作用于主内存，将共享变量从主内存传动到线程的工作内存中，供后面的 load 动作使用。 * **load** 载入：作用于工作内存，把 read 读取的值放到工作内存中的副本变量中。 * **store** 存储：作用于工作内存，把工作内存中的变量传送到主内存中，为随后的 write 操作使用。 * **write** 写入：作用于主内存，把 store 传送值写到主内存的变量中。 * **use** 使用：作用于工作内存，把工作内存的值传递给执行引擎，当虚拟机遇到一个需要使用这个变量的指令，就会执行这个动作。 * **assign** 赋值：作用于工作内存，把执行引擎获取到的值赋值给工作内存中的变量，当虚拟机栈遇到给变量赋值的指令，执行该操作。比如 `int i = 1;` * **lock（锁定）** 作用于主内存，把变量标记为线程独占状态。 * **unlock（解锁）** 作用于主内存，它将释放独占状态 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 如何实现可见性 - 通过**volatile关键字**标记内存屏障保证可见性。 - 通过**synchronized关键字**定义同步代码块或者同步方法保障可见性。 - 通过**Lock接口**保障可见性。 - 通过**Atomic类型**保障可见性。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 为了实现可见性，volatile和synchronized所使用的方法有何不同 volatile通过内存屏障来实现，而synchronized通过系统内核互斥实现，相当于JMM中的lock、unlock，退出代码块时刷新变量到主内存 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - volatile、synchronized、Lock、Atomic对原子性、一致性、有序性的保障情况 | 特性 | volatile关键字 | synchronized关键字 | Lock接口 | Atomic变量 | | ------ | -------------- | ------------------ | -------- | ---------- | | 原子性 | 无法保障 | 可以保障 | 可以保障 | 可以保障 | | 可见性 | 可以保障 | 可以保障 | 可以保障 | 可以保障 | | 有序性 | 一定程度保障 | 可以保障 | 可以保障 | 无法保障 | .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 实现可见性的方法有哪些？ 常用的并发工具类有哪些？ CyclicBarrier 和 CountDownLatch 的区别 synchronized 的作用？ volatile 关键字的作用 sleep 方法和 wait 方法有什么区别? 什么是 CAS CAS 的问题 什么是 Future？ 什么是 AQS AQS 支持两种同步方式 ReadWriteLock 是什么 FutureTask 是什么 synchronized 和 ReentrantLock 的区别 线程 B 怎么知道线程 A 修改了变量 synchronized、volatile、CAS 比较 为什么 wait()方法和 notify()/notifyAll()方法要在同步块中被调用 多线程同步有哪几种方法？ 线程的调度策略 ConcurrentHashMap 的并发度是什么？ 如果你提交任务时，线程池队列已满，这时会发生什么？ Java 中用到的线程调度算法是什么？ 什么是线程调度器(Thread Scheduler)和时间分片(TimeSlicing)？ 什么是自旋？ Java Concurrency API 中的 Lock 接口(Lock interface)是什么？对比同步它有什么优势？
理论 - volatile关键字的作用 1.保证线程间的可见性 2.禁止CPU进行指令重排 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 简述volatile的内存语义 * volatile****写**：当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存。 * volatile****读**：当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - JMM如何实现volatile的禁止指令重排 首先要讲一下内存屏障，内存屏障可以分为以下几类： - LoadLoad 屏障：对于这样的语句Load1，LoadLoad，Load2。在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。 - StoreStore屏障：对于这样的语句Store1， StoreStore， Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。 - LoadStore 屏障：对于这样的语句Load1， LoadStore，Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。 - StoreLoad 屏障：对于这样的语句Store1， StoreLoad，Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。 在每个volatile读操作后插入LoadLoad屏障，在读操作后插入LoadStore屏障 在每个volatile写操作的前面插入一个StoreStore屏障，后面插入一个SotreLoad屏障。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 什么是MESI ​ 在多核CPU中某核发生修改，可能产生数据不一致，一致性协议正是为了保证多个CPU cache之间的缓存共享数据的一致性。其中MESI对应modify(修改)、exclusive(独占)、shared(共享)、invalid（失效）。 | 状态 | 描述 | | ------------ | ------------------------------------------------------------ | | M(modify) | 该缓存行中的内容被修改了，并且该缓存行只缓存在该CPU中,而且和主存数据不一致 | | E(exclusive) | 只有当前CPU中有数据，其他CPU中没有该数据，当前CPU和主存的数据一致 | | S(shared) | 当前CPU和其他CPU中都有共同的数据，并且和主存中的数据一致 | | I(invalid) | 当前CPU中的数据失效，数据应该从主存中获取 | - 目前CPU的写，主要是2种策略 - 1、write back：即CPU向内存写数据时，先把数据写入store buffer中，后续某个时间点会将store buffer中的数据刷新到内存 - 2、write through：即CPU向内存写数据，同步完成写store buffer与内存 - CPU大多采用write back策略 ```markdown 1、CPU异步完成写内存产生的延时是可以接受的，而且延迟很短，只有在多线程环境下需要严格保证内存可见等极少数特殊情况下才需要保证CPU的写在外界看来是同步完成的。 2、编译器和CPU可以保证输出结果一样的情况下对指令重排序。插入内存屏障，相当于告诉CPU和编译器先于这个命令的先执行，后于的命令必须后执行。 3、使用Lock前缀指令，会使多核心CPU互斥使用这个内存地址。当指令执行完，这个锁定动作也消失。 ``` .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 理论 - 有了MESI为什么还要有volatile CPU的MESI能够保证缓存一致性，但是不能保证一个线程对变量修改后其他线程立即可见。 想象下：一个CPU0中的变量所在的cache line已经是invalid，但是在CPU1中缓存的该变量最新值还没有刷新到内存中。那么CPU0需要使用该变量，会从主存中读取到旧的值。 使用volatile可以保证可见性，该CPU该volatile修饰的变量的写操作立即同步到主存。而且volatile内存屏障的作用，也会将之前的发生的数据更新刷新到内存中。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 简述final的特性 对于**基本类型**的final域，编译器和处理器要遵守两个重排序规则： (01) final写：“构造函数内对一个final域的写入”，与“随后把这个被构造对象的引用赋值给一个引用变量”，这两个操作之间不能重排序。 (02) final读：“初次读一个包含final域的对象的引用”，与“随后初次读对象的final域”，这两个操作之间不能重排序。 对于**引用类型**的final域，除上面两条之外，还有一条规则： (03) final写：在“构造函数内对一个final引用的对象的成员域的写入”，与“随后在构造函数外把这个被构造对象的引用赋值给一个引用变量”，这两个操作之间不能重排序。 注意： 写final域的重排序规则可以确保：在引用变量为任意线程可见之前，该引用变量指向的对象的final域已经在构造函数中被正确初始化过了。其实要得到这个效果，还需要一个保证：在构造函数内部，不能让这个被构造对象的引用为其他线程可见，也就是对象引用不能在构造函数中“逸出”。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 实践 - JMM如何实现final的特性 通过“内存屏障”实现。 在final域的写之后，构造函数return之前，插入一个StoreStore障屏。在读final域的操作前面插入一个LoadLoad屏障。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 锁篇 什么是可重入锁 允许一个线程多次请求自己持有对象锁的临界资源 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } synchronized的可重入锁原理 synchronized 锁对象有个计数器，会随着线程获 取锁后 +1 计数，当线程执行完毕后 -1，直到清零释放锁 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } ReentrantLock的可重入锁原理 基于AQS的同步状态：state。 其原理大致为：当某一线程获取锁后，将state值+1，并记录下当前持有锁的线程，再有线程来获取锁时，判断这个线程与持有锁的线程是否是同一个线程，如果是，将state值再+1，如果不是，阻塞线程。 当线程释放锁时，将state值-1，当state值减为0时，表示当前线程彻底释放了锁，然后将记录当前持有锁的线程的那个字段设置为null，并唤醒其他线程，使其重新竞争锁 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 公平锁和非公平锁的区别，为什么公平锁效率低于非公平锁 同步队列器AQS思想，以及基于AQS实现的lock 偏向锁、轻量级锁、重量级锁三者各自的应用场景 偏向锁：只有一个线程进入临界区； 轻量级锁：多个线程交替进入临界区； 重量级锁：多个线程同时进入临界区 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Java中对锁有哪些优化 1. 减少锁持有时间 - 不需要同步执行的代码，能不放在同步快里面执行就不要放在同步快内，可以让锁尽快释放； 2. 减少锁的粒度 - 它的思想是将物理上的一个锁，拆成逻辑上的多个锁，增加并行度，从而降低锁竞争。它的思想也是用空间来换时间； - java中很多数据结构都是采用这种方法提高并发操作的效率： - ConcurrentHashMap: 使用Segment数组,Segment继承自ReenTrantLock，所以每个Segment就是个可重入锁，每个Segment 有一个HashEntry数组用来存放数据，put操作时，先确定往哪个Segment放数据，只需要锁定这个Segment，执行put，其它的Segment不会被锁定；所以数组中有多少个Segment就允许同一时刻多少个线程存放数据，这样增加了并发能力。 - LongAdder:实现思路也类似ConcurrentHashMap，LongAdder有一个根据当前并发状况动态改变的Cell数组，Cell对象里面有一个long类型的value用来存储值;开始没有并发争用的时候或者是cells数组正在初始化的时候，会使用cas来将值累加到成员变量的base上，在并发争用的情况下，LongAdder会初始化cells数组，在Cell数组中选定一个Cell加锁，数组有多少个cell，就允许同时有多少线程进行修改，最后将数组中每个Cell中的value相加，在加上base的值，就是最终的值；cell数组还能根据当前线程争用情况进行扩容，初始长度为2，每次扩容会增长一倍，直到扩容到大于等于cpu数量就不再扩容，这也就是为什么LongAdder比cas和AtomicInteger效率要高的原因，后面两者都是volatile+cas实现的，他们的竞争维度是1，LongAdder的竞争维度为“Cell个数+1”为什么要+1？因为它还有一个base，如果竞争不到锁还会尝试将数值加到base上； - 拆锁的粒度不能无限拆，最多可以将一个锁拆为当前CPU数量即可； 3. 锁粗化 - 大部分情况下我们是要让锁的粒度最小化，锁的粗化则是要增大锁的粒度(如:循环内的操作); 4. 锁分离 - 使用读写锁: ReentrantReadWriteLock 是一个读写锁，读操作加读锁，可以并发读，写操作使用写锁，只能单线程写； - 读写分离: CopyOnWriteArrayList 、CopyOnWriteArraySet - CopyOnWrite容器即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以CopyOnWrite容器也是一种读写分离的思想，读和写不同的容器 - CopyOnWrite并发容器用于读多写少的并发场景，因为，读的时候没有锁，但是对其进行更改的时候是会加锁的，否则会导致多个线程同时复制出多个副本，各自修改各自的； - LinkedBlockingQueue: LinkedBlockingQueue也体现了这样的思想，在队列头入队，在队列尾出队，入队和出队使用不同的锁，相对于LinkedBlockingArray只有一个锁效率要高； 5. 锁消除 - 在即时编译时,如果发现不可能被共享的对象,则可以消除对象的锁操作 6. 无锁 (如CAS) - 如果需要同步的操作执行速度非常快，并且线程竞争并不激烈，这时候使用CAS效率会更高，因为加锁会导致线程的上下文切换，如果上下文切换的耗时比同步操作本身更耗时，且线程对资源的竞争不激烈，使用volatiled+CAS操作会是非常高效的选择； 7. 消除缓存行的伪共享 - 除了我们在代码中使用的同步锁和jvm自己内置的同步锁外，还有一种隐藏的锁就是缓存行，它也被称为性能杀手。在多核cpu的处理器中，每个cpu都有自己独占的一级缓存、二级缓存，甚至还有一个共享的三级缓存，为了提高性能，cpu读写数据是以缓存行为最小单元读写的；32位的cpu缓存行为32字节，64位cup的缓存行为64字节，这就导致了一些问题。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 死锁的条件有哪些 * 互斥条件(Mutual exclusion) ：资源不能被共享，只能由一个进程使用。 * 请求与保持条件(Hold and wait)：进程已获得了一些资源，但因请求其它资源被阻塞时，对已获得的资源保持不放。 * 不可抢占条件(No pre-emption) ：有些系统资源是不可抢占的，当某个进程已获得这种资源后，系统不能强行收回，只能由进程使用完时自己释放。 * 循环等待条件(Circular wait) ：若干个进程形成环形链，每个都占用对方申请的下一个资源 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 死锁产生的原因 * 系统资源的竞争 * 进程推进的顺序不当 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 处理死锁的策略有哪些 (1) 死锁预防：破坏导致死锁必要条件中的任意一个就可以预防死锁。例如，要求用户申请资源时一次性申请所需要的全部资源，这就破坏了保持和等待条件；将资源分层，得到上一层资源后，才能够申请下一层资源，它破坏了环路等待条件。预防通常会降低系统的效率。 (2) 死锁避免：避免是指进程在每次申请资源时判断这些操作是否安全，例如，使用银行家算法（如果申请资源会导致死锁，那么拒绝申请；如果申请不会导致死锁，那么允许申请）。死锁避免算法的执行会增加系统的开销。 (3) 死锁检测：死锁预防和避免都是事前措施，而死锁的检测则是判断系统是否处于死锁状态，如果是，则执行死锁解除策略。 (4) 死锁解除：这是与死锁检测结合使用的，它使用的方式就是剥夺。即将某进程所拥有的资源强行收回，分配给其他的进程。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } synchoronized的的实现原理 根据线程的竞争情况，锁状态的会逐渐升级 ：无锁-- 偏向锁 -- 轻量锁 -- 重量锁。线程运行完之后降级 * 偏向锁 * 适用场景 ：单个线程间断性获取锁 * 加锁过程：持有偏向锁的线程以后每次进入这个锁相关的同步块时，只需比对一下 mark word 的线程 id 是否为本线程，如果是则获取锁成功，将线程id写入mark word中 * 撤销过程 ：等待全局安全点后将其恢复到轻量锁或者无锁 * 轻量锁 * 适用场景 ：多个线程交替获取锁 * 加锁过程 ：JVM 在当前线程的栈帧中创建 Lock Reocrd，并将对象头中的 Mark Word 复制到 Lock Reocrd 中；线程尝试使用 CAS 将对象头中的 Mark Word 替换为指向 Lock Reocrd 的指针，获取成功则加锁成功，否则膨胀为重量锁 * 撤销过程 ：使用CAS将LockRecord还原，还原成功则释放，否则膨胀为重量锁 * 重量锁 * todo：流程有些复杂 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Synchronized和ReentrantLock的区别 * Synchronized是JVM层次的锁实现，ReentrantLock是JDK层次的锁实现； * Synchronized的锁状态是无法在代码中直接判断的，但是ReentrantLock可以通过`ReentrantLock#isLocked`判断； * Synchronized是非公平锁，ReentrantLock是可以是公平也可以是非公平的； * Synchronized是不可以被中断的，而`ReentrantLock#lockInterruptibly`方法是可以被中断的； * 在发生异常时Synchronized会自动释放锁（由javac编译时自动实现），而ReentrantLock需要开发者在finally块中显示释放锁； * ReentrantLock获取锁的形式有多种：如立即返回是否成功的tryLock(),以及等待指定时长的获取，更加灵活； * Synchronized在特定的情况下**对于已经在等待的线程**是后来的线程先获得锁（上文有说），而ReentrantLock对于**已经在等待的线程**一定是先来的线程先获得锁 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 代理反射篇 JDK动态代理和CGLiB代理区别 * JDK 动态代理是基于接口的，所以**要求代理类一定是有定义接口的**。 * CGLIB 基于ASM字节码生成工具，它是通过继承的方式来实现代理类，所以**要注意 final 方法**。 * 执行效率方面 ：JDK高于CGLIB，调用次数为百万级别时JDK速度比CGLIB高30%，千万级别时JDK速度比CGLIB高100% .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } JDK动态代理的实现原理 1. 首先通过实现 InvocationHandler 接口得到一个切面类(干活的人)。 2. 然后利用 Proxy 根据目标类的类加载器、接口和切面类得到一个代理类(协调管理的人)。 3. 代理类的逻辑就是把所有接口方法的调用转发到切面类的 invoke() 方法上，然后根据反射调用目标类的方法。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } CGLIB代理的实现原理 核心思路与JDK动态代理类似，但是使用的原理是ASM字节码技术 ```java //1.new Enhancer en = new Enhancer(); //2.设置父类，也就是代理目标类，上面提到了它是通过生成子类的方式 en.setSuperclass(target.getClass()); //3.设置回调函数，这个this其实就是代理逻辑实现类，也就是切面，可以理解为JDK 动态代理的handler en.setCallback(this); //4.创建代理对象，也就是目标类的子类了。 return en.create(); ``` .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } JVM篇 JVM的内存区域分为哪几块，其中哪几块是线程共享的，每一块存储什么 * 堆 ： 存放实际对象，堆中一般分代，线程共享 * 方法区 ： 存储虚拟机加载的类信息、常量、静态变量以及即时编译器编译后的代码等数据，永久代就是方法区的具体实现，线程共享 * 常量池 ： 字符串常量池存放字面量，类常量池存放各种class，class被加载解析后的内容存放在运行时常量池 * 程序计数器 ：主要存储字节码的行号指示器，控制程序的执行，线程不共享 * 虚拟机栈 ：以栈帧为单位，每个栈帧对应一个被调用的Java方法，存放基本数据类型、对象引用、方法参数、操作数栈、方法出口等，线程不共享 * 本地方法栈 ：与虚拟机栈类似，区别在于它对应的是native本地方法，线程不共享 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 内存溢出和内存泄露的区别 * 内存溢出 out of memory，是指**程序在申请内存时，没有足够的内存空间供其使用**，出现out of memory； * 内存泄露 memory leak，是指**程序在申请内存后，无法释放已申请的内存空间**，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存，迟早会被占光。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何判断哪些对象需要被GC * 引用计数法 ： 引用计数为0的需要被回收 * 可达性分析 ： 从GC Roots出发的不可达对象需要被回收 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 什么是GC Roots * 方法区中的类静态属性引用的对象； * 方法区中常量引用的对象； * 虚拟机栈（栈帧中的本地变量表）中引用的对象； * 本地方法栈中JNI（即一般说的Native方法）中引用的对象 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 对象的访问定位方式 * 使用句柄 * 直接指针 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 强引用、软引用、弱引用、虚引用是什么 * 强引用 ： 正常的对象引用 * 软引用 ： 维护一些可有可无的对象。只有在内存不足时，系统则会回收软引用对象，如果回收了软引用对象之后仍然没有足够的内存，才会抛出内存溢出异常 * 弱引用 ：比较软引用，要更加无用一些，它拥有更短的生命周期。当JVM进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象 * 虚引用 ：形同虚设的引用，主要用来跟踪对象被垃圾回收的活动 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } GC的方法 * 标记-清除 * 标记-整理 * 复制 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 对象在哪些情况下会从新生代进入老年代 * 达到回收年龄，默认15 * 动态对象年龄判断 * 分配担保时新生代不够用会直接分配到老年代 * 对象超出一定大小时直接分配到老年代 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 优化时调整过JVM的哪些参数 * **Xms** ： 初始堆内存大小 * **Xmx** ：最大堆内存大小 * **Xmn** ：年轻代大小 * **Xss** ：每个线程的内存大小 * **XX:NewRatio** ：设置新生代和老年代的比值 * **XX:SurvivorRatio** ： 新生代中Eden区与两个Survivor区的比值 * **XX:+UseG1GC** : 使用G1垃圾收集器 * **XX:+PrintGC** ：统计垃圾回收信息 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 内存溢出是什么，有哪些原因，如何解决 * 定义 ： 分配的内存空间超过系统内存 * 原因 ： * 使用了大量的递归或无限递归 * 在循环内大量创建对象 * 使用了大量的static修饰变量，或者用static修饰了过大的数据 * 有数组，List，Map中存放的是大量对象的引用而不是对象 * 没有重新equals()和hashCode()方法，从而导致向数组/List/Map中添加时导致重复添加 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 内存泄漏是什么，有哪些原因，如何解决 * 定义 ：系统分配的内存没有被回收 * 原因 ： * 连接未关闭 * finalize方法没有被执行从而导致jvm认为对象还无法被回收 * ThreadLocal的错误使用，使用完之后未及时remove .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } MinorGC，MajorGC、FullGC都什么时候发生 MinorGC在年轻代空间不足的时候发生，MajorGC指的是老年代的GC，出现MajorGC一般经常伴有MinorGC。 FullGC有三种情况。 1. 当老年代无法再分配内存的时候 2. 元空间不足的时候 3. 显示调用System.gc的时候。另外，像CMS一类的垃圾回收器，在MinorGC出现promotion failure的时候也会发生FullGC .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 类加载的过程 * 加载 ：将二进制流搞到内存中来，生成一个 Class 类 * 连接 * 验证 ：验证加载进来的二进制流是否符合一定格式，是否规范，是否符合当前 JVM 版本 * 准备 ：为静态变量(类变量)赋初始值(例如int默认赋值0)，也即为它们在方法区划分内存空间 * 解析 ：将常量池的符号引用转化成直接引用 * 初始化 ：执行一些静态代码块，为静态变量赋值(这里的赋值才是代码里面的赋值) .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 双亲委派 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } MinGC与FullGC各自指什么
.spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } HotSpot的GC算法以及7种垃圾回收期
.spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 设计模式篇 简要介绍各设计模式中的关键点 单例模式：某个类只能有一个实例，提供一个全局的访问点。 简单工厂：一个工厂类根据传入的参量决定创建出那一种产品类的实例。 工厂方法：定义一个创建对象的接口，让子类决定实例化那个类。 抽象工厂：创建相关或依赖对象的家族，而无需明确指定具体类。 建造者模式：封装一个复杂对象的构建过程，并可以按步骤构造。 原型模式：通过复制现有的实例来创建新的实例。 适配器模式：将一个类的方法接口转换成客户希望的另外一个接口。 组合模式：将对象组合成树形结构以表示“”部分-整体“”的层次结构。 装饰模式：动态的给对象添加新的功能。 代理模式：为其他对象提供一个代理以便控制这个对象的访问。 亨元（蝇量）模式：通过共享技术来有效的支持大量细粒度的对象。 外观模式：对外提供一个统一的方法，来访问子系统中的一群接口。 桥接模式：将抽象部分和它的实现部分分离，使它们都可以独立的变化。 模板模式：定义一个算法结构，而将一些步骤延迟到子类实现。 解释器模式：给定一个语言，定义它的文法的一种表示，并定义一个解释器。 策略模式：定义一系列算法，把他们封装起来，并且使它们可以相互替换。 状态模式：允许一个对象在其对象内部状态改变时改变它的行为。 观察者模式：对象间的一对多的依赖关系。 备忘录模式：在不破坏封装的前提下，保持对象的内部状态。 中介者模式：用一个中介对象来封装一系列的对象交互。 命令模式：将命令请求封装为一个对象，使得可以用不同的请求来进行参数化。 访问者模式：在不改变数据结构的前提下，增加作用于一组对象元素的新功能。 责任链模式：将请求的发送者和接收者解耦，使的多个对象都有处理这个请求的机会。 迭代器模式：一种遍历访问聚合对象中各个元素的方法，不暴露该对象的内部结构。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 实战排查 如何查找哪个线程使用 CPU 最长 （1）获取项目的pid，jps或者ps -ef | grep java （2）top -H -p pid，顺序不能改变 这样可以打印出当前的项目进程，每条线程占用CPU时间的百分比 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何排查CPU问题过高 * top -H查看CPU使用最高的进程，将其转换为十六进制，例如pid为67136，转换为16进制的1065b * `jstack`命令获取应用的栈信息，搜索这个16进制 ，例如`jstack -l 67136|grep 1065b`,-l作用是查询锁信息 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何排查堆外内存 进程占用的内存，可以使用top命令，看RES段占用的值。如果这个值大大超出我们设定的最大堆内存，则证明堆外内存占用了很大的区域。 使用gdb可以将物理内存dump下来，通常能看到里面的内容。更加复杂的分析可以使用perf工具，或者谷歌开源的gperftools。那些申请内存最多的native函数，很容易就可以找到。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何排查线上出现的JVM问题</content></entry><entry><title>Spring常见面试题</title><url>https://1162492411.github.io/docs/post/spring%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories><category>面试题</category><category>Spring</category></categories><tags><tag>面试题</tag><tag>Spring</tag></tags><content type="html"> IOC 创建IOC容器的过程 以最原始的XmlBeanFactory为例讲解, 1.创建Ioc配置文件的抽象资源，这个抽象资源包含了BeanDefinition的定义信息 ； 2.创建一个BeanFactory，这里使用了DefaultListableBeanFactory ； 3.创建一个载入BeanDefinition的读取器，这里使用XmlBeanDefinitionReader来载入XML文件形式的BeanDefinition ； 4.然后将上面定位好的Resource，通过一个回调配置给BeanFactory ； 5.从定位好的资源位置读入配置信息，具体的解析过程由XmlBeanDefinitionReader完成 ； 6.完成整个载入和注册Bean定义之后，需要的Ioc容器就初步建立起来了 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 配置Bean的方法 - 基于XML文件进行配置 - 基于注解进行配置 - 基于Java程序进行配置 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Spring bean的初始化顺序 * Constructor * BeanPostProsser的before * @PostConstruct(其实也是一个BeanPostProsser的before) * InitializingBean * init-method * BeanPostProsser的after .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Spring Bean的销毁顺序 * destory * destroy-method .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Spring中Bean的生命周期 Bean的作用域有哪些 * single ：单例，适用于无状态的bean，默认 * prototype : 每次获取都返回一个新的实例，适用于有状态的bean，默认启动时不加载prototype的bean * request ：每次http请求会创建一个新的实例 * session ：每次session共享一个新的实例 * globalSession ：全局session共享一个新的实例 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Spring如何解决setter方式的循环依赖 singletonFactories ： 单例对象工厂的cache； earlySingletonObjects ：提前暴光的单例对象的工厂Cache； singletonObjects：单例对象的cache； 1. 实例化a，先把beanName放到singletonsCurrentlyInCreation中，然后调用无参构造方法实例化bean,然后构造一个singletonFactory对象放到singletonFactories中，暴露给其它可能的依赖; 2. 最后装配属性时，发现需要注入b,那么就开始构造b,构造b的流程和上一步一致 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Spring无法解决哪些方式的循环依赖 * 构造器 ： a构造器中依赖b * setter的prototype ： prototype的bean启动时不加载，不在三级缓存中 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 有哪些常见的Resource - UrlResource：访问网络资源的实现类。 - ClassPathResource：访问类加载路径里资源的实现类。 - FileSystemResource：访问文件系统里资源的实现类。 - ServletContextResource：访问相对于 ServletContext 路径里的资源的实现类： - InputStreamResource：访问输入流资源的实现类。 - ByteArrayResource：访问字节数组资源的实现类。 这些 Resource 实现类，针对不同的的底层资源，提供了相应的资源访问逻辑，并提供便捷的包装，以利于客户端程序的资源访问 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 有哪些常见的Context - FileSystemXmlApplicationContext：该容器从 XML 文件中加载已被定义的 bean。在这里，你需要提供给构造器 XML 文件的完整路径 - ClassPathXmlApplicationContext：该容器从 XML 文件中加载已被定义的 bean。在这里，你不需要提供 XML 文件的完整路径，只需正确配置 CLASSPATH 环境变量即可，因为，容器会从 CLASSPATH 中搜索 bean 配置文件。 - WebXmlApplicationContext：该容器会在一个 web 应用程序的范围内加载在 XML 文件中已被定义的 bean .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 有哪些内置的BeanPostProcessor * ApplicationContextAwareProcessor * 作用 ：向组件中注入IOC容器，提供对xxxAware类接口的实现 * 注册 ：Spring容器的refresh方法内部调用prepareBeanFactory方法，prepareBeanFactory方法会添加ApplicationContextAwareProcessor到BeanFactory中 * CommonAnnotationBeanPostProcessor * 作用 ：提供对@PostConstruct、@PreDestroy、@Resource、WebServiceRef注解的支持 * 注册 ：在AnnotationConfigUtils类的registerAnnotationConfigProcessors方法中被封装成RootBeanDefinition并注册到Spring容器中 * AutowiredAnnotationBeanPostProcessor * 作用 ：提供对@Autowired、@Value、@Lookup和@Inject注解的实现 * 注册 ：在AnnotationConfigUtils类的registerAnnotationConfigProcessors方法被注册到Spring容器中 * RequiredAnnotationBeanPostProcessor * 作用 ：提供对@Required注解的实现 * 注册 ：在AnnotationConfigUtils类的registerAnnotationConfigProcessors方法被注册到Spring容器中 * BeanValidationPostProcessor * 作用 ：提供对JSR-303验证的支持 * AbstractAutoProxyCreator * 作用 ：提供对AOP的支持 * 注册 ：默认不注册。在SpringBoot中加入aop-starter之后，会触发AopAutoConfiguration自动化配置，然后将AnnotationAwareAspectJAutoProxyCreator注册到Spring容器中 * MethodValidationPostProcessor * 作用 ：支持方法级别的JSR-303规范。需要在类上加上@Validated注解，以及在方法的参数中加上验证注解，比如@Max，@Min，@NotEmpty * 注册 ： 默认不注册 * ScheduledAnnotationBeanPostProcessor * 作用 ：Spring Scheduling功能对bean中使用了@Scheduled注解的方法进行调度处理 * 注册 ： 默认不注册，添加@EnableScheduling会被注册 * AsyncAnnotationBeanPostProcessor * 作用 ：提供对@Async注解的实现，通过AOP实现 * 注册 ： 默认不注册，添加@EnableAsync会被注册 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } MVC MVC的启动流程 注解 如何开启注解功能 * XML方式 ：在Spring配置文件中配置 context:annotation-config/元素 * 注解方式 ： 使用**@ComponentScan** .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } @Autowired和@Resources注解的异同 1.autowired默认按类型查找对象，resources默认按照名称查找对象 2.autowired是spirng提供的注解，resouces是j2ee提供的注解，但是二者都是jsr标准下的注解 3.两个注解都可以用在字段上 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 同样的接口存在多个实现时如何指定某一个实现 1.@Qualifer 2.@Primary .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } AOP aop的底层实现
动态代理有哪些实现方式，有什么区别
事务 Spring事务底层如何实现
传播级别和隔离级别各自定义 * 传播级别定义的是事务的控制范围 * 隔离级别定义的是事务在数据库读写方面的控制范围 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 事务的五个隔离级别 **TransactionDefinition 接口中定义了五个表示隔离级别的常量：** - **TransactionDefinition.ISOLATION_DEFAULT:** 使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别. - **TransactionDefinition.ISOLATION_READ_UNCOMMITTED:** 最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读** - **TransactionDefinition.ISOLATION_READ_COMMITTED:** 允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生** - **TransactionDefinition.ISOLATION_REPEATABLE_READ:** 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生。** - **TransactionDefinition.ISOLATION_SERIALIZABLE:** 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。但是这将严重影响程序的性能。通常情况下也不会用到该级别 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Spring事务的七个传播级别 - PROPAGATION_REQUIRED: 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择。 - PROPAGATION_SUPPORTS: 支持当前事务，如果当前没有事务，就以非事务方式执行。 - PROPAGATION_MANDATORY: 支持当前事务，如果当前没有事务，就抛出异常。 - PROPAGATION_REQUIRES_NEW: 新建事务，如果当前存在事务，把当前事务挂起。 - PROPAGATION_NOT_SUPPORTED: 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 - PROPAGATION_NEVER: 以非事务方式执行，如果当前存在事务，则抛出异常。 - PROPAGATION_NESTED:如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 嵌套事务和挂起事务的区别是什么 * 嵌套事务 ：本质上还是同一个事务的不同保存点，如果涉及到外层事务回滚，则内层的也将会被回滚 * 挂起事务 ：对应的是一个新的事务，拿到的是新的资源，所以外层事务回滚时，不影响内层事务 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 哪些情况下事务注解无法生效 * 方法自调用 ： * 举例 ：例如类AService中方法a中直接调用方法b,这样因为本质是调用了this.B(),而this指的是A而不是A的代理对象， * 解决方法 ：配置aop时将expose-proxy设置为true，这样会将当前代理对象放入ThreadLocal中，然后使用AopContext.currentProxy()获取当前代理，将this.b()改为((AService)AopContext.currentProxy()).b() * 方法为private/protect * 举例 ：例如类AService中方法a中直接调用private/protect修饰的方法b * 解决方法 ：将b修改为public，原因是 SpringAOP对于最外层的函数只拦截public方法，不拦截protected和private方法 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } JDK代理和CGLIB代理的区别 JDK动态代理只能对实现了接口的类生成代理,而不能针对类. CGLIB是针对类实现代理,主要是对指定的类生成一个子类,覆盖其中的方法,因为是继承,所以该类或方法最好不要声明成fina .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 拷贝 什么是浅拷贝和深拷贝，有什么区别 深拷贝有哪些实现方式 * 手动赋值 * 实现Serializable接口，实现clonable接口的clone方法 * 使用反射，先序列化再反序列化为新对象，例如fastjson .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 常用的实体拷贝有哪几种方式，各自是如何实现的 Spring的BeanUtils拷贝存在哪些细节问题 设计模式 Spring中用到了哪些设计模式 （1）工厂模式：BeanFactory就是简单工厂模式的体现，用来创建对象的实例； （2）单例模式：Bean默认为单例模式。 （3）代理模式：Spring的AOP功能用到了JDK的动态代理和CGLIB字节码生成技术； （4）模板方法：用来解决代码重复的问题。比如. RestTemplate, JmsTemplate, JpaTemplate。 （5）观察者模式：定义对象键一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都会得到通知被动更新，如Spring中listener的实现–ApplicationListener。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } SpringBoot 简述配置文件加载顺序 由高到低依次为： * 命令行参数。所有的配置都可以在命令行上进行指定； * 来自java:comp/env的JNDI属性； * Java系统属性（System.getProperties()）； * 操作系统环境变量 ； * jar包外部的application-{profile}.properties或application.yml(带spring.profile)配置文件 * jar包内部的application-{profile}.properties或application.yml(带spring.profile)配置文件 再来加载不带profile * jar包外部的application.properties或application.yml(不带spring.profile)配置文件 * jar包内部的application.properties或application.yml(不带spring.profile)配置文 * @Configuration注解类上的@PropertySource .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 概述SpringBoot的启动流程 1. 从`spring.factories`配置文件中**加载`EventPublishingRunListener`对象** 2. **准备环境变量**，包括系统变量，环境变量，命令行参数，默认变量，`servlet`相关配置变量，随机值以及配置文件等 3. 控制台**打印SpringBoot的`bannner`标志** 4. **根据不同类型环境创建不同类型的`applicationcontext`容器** 5. 从`spring.factories`配置文件中**加载`FailureAnalyzers`对象**,用来报告SpringBoot启动过程中的异常 6. **为刚创建的容器对象做一些初始化工作**，准备一些容器属性值等，对`ApplicationContext`应用一些相关的后置处理和调用各个`ApplicationContextInitializer`的初始化方法来执行一些初始化逻辑等； 7. **刷新容器**，这一步至关重要。比如调用`bean factory`的后置处理器，注册`BeanPostProcessor`后置处理器，初始化事件广播器且广播事件，初始化剩下的单例`bean`和SpringBoot创建内嵌的`Tomcat`服务器等等重要且复杂的逻辑都在这里实现，主要步骤可见代码的注释，关于这里的逻辑会在以后的spring源码分析专题详细分析； 8. **执行刷新容器后的后置处理逻辑**，注意这里为空方法； 9. **调用`ApplicationRunner`和`CommandLineRunner`的run方法**，我们实现这两个接口可以在spring容器启动后需要的一些东西比如加载一些业务数据等; 10. **报告启动异常**，即若启动过程中抛出异常，此时用`FailureAnalyzers`来报告异常; 11. 最终**返回容器对象**，这里调用方法没有声明对象来接收 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 概述SpringBoot的自动装配原理 * 判断自动装配开关是否打开。默认`spring.boot.enableautoconfiguration=true`，可在 `application.properties` 或 `application.yml` 中设置 * 获取`EnableAutoConfiguration`注解中的 `exclude` 和 `excludeName` * 获取需要自动装配的所有配置类，读取`META-INF/spring.factories` * 通过@Conditionalxxx的结果判断需要加载哪些配置类 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; }</content></entry><entry><title>Linux常见面试题</title><url>https://1162492411.github.io/docs/post/linux%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories><category>Linux</category><category>面试题</category></categories><tags><tag>Linux</tag><tag>面试题</tag></tags><content type="html"> .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 内存 如何查看系统内存
free .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何清理系统内存
```shell # 仅清除页面缓存（PageCache） echo 1 /proc/sys/vm/drop_caches # 清除目录项和inode echo 2 /proc/sys/vm/drop_caches # 清除页面缓存，目录项和inode echo 3 /proc/sys/vm/drop_caches ``` .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 内核 用户态如何切换到内核态 1. 系统调用，例如fork() 2. 异常 3. 外围设备的中断 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; }
系统函数 select和epoll的区别
CPU cpu的load值不同时，cpu有哪几种模式(空闲，轻度负载，高负载)
文件系统 Linux，查找磁盘上最大的文件的命令
常用命令 如何查看系统负载 使用top命令 ，执行后效果如下 ``` top - 01:06:48 up 1:22, 1 user, load average: 0.06, 0.60, 0.48 Tasks: 29 total, 1 running, 28 sleeping, 0 stopped, 0 zombie Cpu(s): 0.3% us, 1.0% sy, 0.0% ni, 98.7% id, 0.0% wa, 0.0% hi, 0.0% si Mem: 191272k total, 173656k used, 17616k free, 22052k buffers Swap: 192772k total, 0k used, 192772k free, 123988k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1379 root 16 0 7976 2456 1980 S 0.7 1.3 0:11.03 sshd 14704 root 16 0 2128 980 796 R 0.7 0.5 0:02.72 top 1 root 16 0 1992 632 544 S 0.0 0.3 0:00.90 init 2 root 34 19 0 0 0 S 0.0 0.0 0:00.00 ksoftirqd/0 3 root RT 0 0 0 0 S 0.0 0.0 0:00.00 watchdog/0 ``` 第一行是任务队列信息 ：分别表示系统当前时间、系统运行时间、当前登陆用户数量、系统负载(即任务队列的平均长度,三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值) 第二、三行是进程和CPU的信息 ： ``` total 进程总数 running 正在运行的进程数 sleeping 睡眠的进程数 stopped 停止的进程数 zombie 僵尸进程数 Cpu(s): 0.3% us 用户空间占用CPU百分比 1.0% sy 内核空间占用CPU百分比 0.0% ni 用户进程空间内改变过优先级的进程占用CPU百分比 98.7% id 空闲CPU百分比 0.0% wa 等待输入输出的CPU时间百分比 0.0%hi：硬件CPU中断占用百分比 0.0%si：软中断占用百分比 0.0%st：虚拟机占用百分比 ``` 第四、五行是内存信息 ``` Mem: 191272k total 物理内存总量 173656k used 使用的物理内存总量 17616k free 空闲内存总量 22052k buffers 用作内核缓存的内存量 Swap: 192772k total 交换区总量 0k used 使用的交换区总量 192772k free 空闲交换区总量 123988k cached 缓冲的交换区总量 ``` 剩下的是进程信息区统计信息区域，它显示了各个进程的详细信息，各列的含义如下 ``` 序号 列名 含义 a PID 进程id b PPID 父进程id c RUSER Real user name d UID 进程所有者的用户id e USER 进程所有者的用户名 f GROUP 进程所有者的组名 g TTY 启动进程的终端名。不是从终端启动的进程则显示为 ? h PR 优先级 i NI nice值。负值表示高优先级，正值表示低优先级 j P 最后使用的CPU，仅在多CPU环境下有意义 k %CPU 上次更新到现在的CPU时间占用百分比 l TIME 进程使用的CPU时间总计，单位秒 m TIME+ 进程使用的CPU时间总计，单位1/100秒 n %MEM 进程使用的物理内存百分比 o VIRT 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES p SWAP 进程使用的虚拟内存中，被换出的大小，单位kb。 q RES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA r CODE 可执行代码占用的物理内存大小，单位kb s DATA 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb t SHR 共享内存大小，单位kb u nFLT 页面错误次数 v nDRT 最后一次写入到现在，被修改过的页面数。 w S 进程状态(D=不可中断的睡眠状态,R=运行,S=睡眠,T=跟踪/停止,Z=僵尸进程) x COMMAND 命令名/命令行 y WCHAN 若该进程在睡眠，则显示睡眠中的系统函数名 z Flags 任务标志，参考 sched.h ``` .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } vmstat命令各字段含义 - r: 运行队列中进程数量（当数量大于CPU核数表示有阻塞的线程） - b: 等待IO的进程数量 - swpd: 使用虚拟内存大小 - free: 空闲物理内存大小 - buff: 用作缓冲的内存大小(内存和硬盘的缓冲区) - cache: 用作缓存的内存大小（CPU和内存之间的缓冲区） - si: 每秒从交换区写到内存的大小，由磁盘调入内存 - so: 每秒写入交换区的内存大小，由内存调入磁盘 - bi: 每秒读取的块数 - bo: 每秒写入的块数 - in: 每秒中断数，包括时钟中断。 - cs: 每秒上下文切换数。 - us: 用户进程执行时间百分比(user time) - sy: 内核系统进程执行时间百分比(system time) - wa: IO等待时间百分比 - id: 空闲时间百分比 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; }</content></entry><entry><title>数据库常见面试题</title><url>https://1162492411.github.io/docs/post/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories><category>数据库</category><category>面试题</category></categories><tags><tag>MySQL</tag><tag>面试题</tag></tags><content type="html"> 基础篇 1NF、2NF、3NF是什么 * 1NF ： 表中每一列的属性都不可再分 * 2NF ： 表中每一列的属性都不可再分，且非主属性完全依赖于主属性 * 3NF ：在2NF的基础上，每个非主属性之间都不传递函数依赖于其他非主属性 * BC NF ： 在3NF基础上，任何非主属性不能对主键子集依赖（在3NF基础上消除对主码子集的依赖），BCNF是3NF的一个子集 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } InndoDB相比MyISAM有什么优点 * 支持行锁 * 支持事务 * 支持外键 * 支持崩溃后的数据恢复 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } delete、truncate、drop区别 * delete属于DML语言；每次删除一行，都在事务日志中为所删除的每行记录一项；删除数据，不释放空间，不删除表结构 * truncate属于DDL语言；通过释放数据页来删除数据，并且只在事务日志中记录页的释放；删除数据，释放空间，不删除表结构 * drop数据数据DDL语言；删除表的结构，以及被依赖的约束、触发器、索引；删除数据，删除空间，删除表结构 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 事务篇 事务有哪些特性 1. **原子性（Atomicity）：** 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 2. **一致性（Consistency）：** 执行事务后，数据库从一个正确的状态变化到另一个正确的状态； 3. **隔离性（Isolation）：** 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 4. **持久性（Durability）：** 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 并发事务会带来哪些问题 - **脏读（Dirty read）:** 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 - **丢失修改（Lost to modify）:** 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。 - **不可重复读（Unrepeatableread）:** 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 - **幻读（Phantom read）:** 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 事务的隔离级别有哪些 - **READ-UNCOMMITTED(读取未提交)：** 最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读**。 - **READ-COMMITTED(读取已提交)：** 允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生**。 - **REPEATABLE-READ(可重复读)：** 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生**。 - **SERIALIZABLE(可串行化)：** 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 事务的隔离级别都能解决哪些并发事务问题 | 隔离级别 | 脏读 | 不可重复读 | 幻影读 | | ---------------- | ---- | ---------- | ------ | | READ-UNCOMMITTED | √ | √ | √ | | READ-COMMITTED | × | √ | √ | | REPEATABLE-READ | × | × | √ | | SERIALIZABLE | × | × | × | .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } MySQL默认的隔离级别是哪个 REPEATABLE-READ，但是因为MySQL的REPEATABLE-READ采用的是next-key lock,因此事实上实现了SERIALIZABLE的效果(todo后半句存在疑问，需要实践一下) .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何修改隔离级别 * 全局修改 ： 修改mysql.ini,添加以下配置项 * ``` #可选参数有：READ-UNCOMMITTED, READ-COMMITTED, REPEATABLE-READ, SERIALIZABLE. [mysqld] transaction-isolation = REPEATABLE-READ ``` * 修改会话级别 ： set session transaction isolation level read uncommitted; * 在代码中修改 ： 如Spring事务可以在@Transactional注解中指定Isolation为READ-UNCOMMITTED/READ-COMMITTED/REPEATABLE-READ/SERIALIZABLE，这样可以实现会话级别的修改 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 连接篇 内连接和外连接什么区别 内连接（inner join）：取出两张表中匹配到的数据，匹配不到的不保留 外连接（outer join）：取出连接表中匹配到的数据，匹配不到的也会保留，其值为NULL .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 左连接和右连接什么区别 左连接 ： 以左边的表为主表 右连接 ： 以右边的表为主表 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } MySQL有哪些Join算法 1.**Simple Nested-Loop Join** (MySQL并没有实现这种)： 简单的嵌套循环连接，流程如下 ![数据库-MySQL-Join-nlj](https://gitee.com/1162492411/pic/raw/master/数据库-MySQL-Join-nlj.png) 2.**Block Nested-Loop Join** `Block Nested-Loop(BNL)`算法缓冲在外循环中读取的行，来减少读取内循环表的次数。例如，如果将10行数据读取到缓冲区中，然后将缓冲区传入到内循环，内循环中读取的行可以一次与缓冲区中的10行数据进行对比，这可以减少读取内循环中的表的次数。 该算法的流程是： 1. 把表A的数据读入线程内存join_buffer中 2. 扫描表B，把表B的每一行取出来，跟join_buffer中的数据进行对比，满足join条件则作为结果集的一部分返回 ![block join](https://gitee.com/1162492411/pic/raw/master/数据库-MySQL-Join-BlockNestedLoopJoin.jpg) 3.**Index Nested-Loop Join** Index Nested-Loop Join是基于索引进行连接，驱动表通过被驱动表上的索引进行匹配，避免与被驱动表的每条记录都进行对比，减少对比次数，提升Join性能.执行过程如下： 1. 从表t1中读取一行记录A1 2. 从数据行A1中取出字段a到t2里去查找 3. 取出t2中满足条件的行，与A1组成一行作为结果集 4. 重复上述3个步骤，直到t1遍历完成 ![index join](https://gitee.com/1162492411/pic/raw/master/数据库-MySQL-Join-IndexNestedLoopJoin.jpg) 4. **Hash Join** Hash join 不需要索引的支持。大多数情况下，hash join 比之前的 BNL 算法在没有索引时的等值连接更加高效。这种算法自MySQL8.0开始支持 具体步骤： - 1）把驱动表相关字段存入Join Buffer，这一步和BNL套路相同。 - 2）（build）把Join Buffer中对应的字段值生成一个散列表，保存在内存中。 - 3）（probe）扫描被驱动表，对被驱动表中的相关字段进行散列并比较。 ![2019-11-yamin-hash-join.jpg](https://gitee.com/1162492411/pic/raw/master/数据库-MySQL-Join-HashJoin.jpg) 5. **Batched Key Access Join** MySQL 5.6推出了 `Batched Key Access Join`，该算法通过常见的空间换时间，随机I/O转顺序I/O，以此来极大的提升Join的性能。 ![BKA Join](https://gitee.com/1162492411/pic/raw/master/数据库-MySQL-Join-BKA.png) .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 索引篇 MySQL支持的索引类型 * B+树 * 全文索引 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 索引的优缺点 优势：可以快速检索，减少I/O次数，加快检索速度；根据索引分组和排序，可以加快分组和排序； 劣势：索引本身也是表，因此会占用存储空间，一般来说，索引表占用的空间的数据表的1.5倍；索引表的维护和创建需要时间成本，这个成本随着数据量增大而增大；构建索引会降低数据表的修改操作（删除，添加，修改）的效率，因为在修改数据表的同时还需要修改索引表 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 索引的分类 * 主键索引：即主索引，根据主键pk_clolum（length）建立索引，**不允许重复，不允许空值** * 唯一索引：用来建立索引的列的值必须是**唯一的，允许空值** * 普通索引：用表中的普通列构建的索引，没有任何限制 * 组合索引：用多个列组合构建的索引，这多个列中的值不允许有空值 * 全文索引：用大文本对象的列构建的索引 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 什么是聚簇索引？什么是非聚簇索引 * 聚簇索引 ：索引与数据存储在一起，如主键索引 * 非聚簇索引 ：索引与数据分离 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 什么时候适合使用索引 * 经常作为查询条件在WHERE或者ORDER BY 语句中出现的列要建立索引； * 作为排序的列要建立索引； * 查询中与其他表关联的字段，外键关系建立索引 * 高并发条件下倾向组合索引； * 用于聚合函数的列可以建立索引，例如使用了max(column_1)或者count(column_1)时的column_1就需要建立索引 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 什么时候不适合使用索引 * 经常增删改的列不要建立索引； * 有大量重复的列不建立索引； * 表记录太少不要建立索引 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 索引什么时候失效 * 在索引的列上使用表达式或者函数会使索引失效 * 在查询条件中使用IS NULL或者IS NOT NULL会导致索引失效 * LIKE操作中，'%aaa%'不会使用索引，'%aaa'不会使用索引 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 三星索引是什么 * 第一颗星： * 定义 ： 如果与一个查询相关的索引行是相邻的(where匹配出的结果之间物理距离近)，或者至少相距足够靠近的话，那这个索引就可以标记上一颗星 * 收益 ： 它最小化了必须扫描的索引片的宽度 * 实现 ： 把 WHERE 后的等值条件列作为索引最开头的列，如此，必须扫描的索引片宽度就会缩至最短 * 第二颗星： * 定义：如果索引行的顺序与查询语句的需求一致，则索引可以标记上第二颗星。 * 收益：它排除了排序操作 * 实现：将 ORDER BY 列加入到索引中，保持列的顺序 * 第三颗星： * 定义：如果索引行中包含查询语句中的所有列，那么这个索引就可以标记上第三颗星。 * 收益：这避免了访问表的操作（避免了回表操作），只访问索引就可以满足了。 * 实现：将查询语句中剩余的列都加入到索引中/仅查询包含索引的列 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 锁篇 MySQL的InnoDB引擎中有哪些锁 1）按照锁的粒度划分 * 表级锁 * 行级锁 * Record lock：单个行记录上的锁 * Gap lock：间隙锁，锁定一个范围，不包括记录本身 * Next-key lock：record+gap 锁定一个范围，包含记录本身 2）按照是否可写划分 * 共享锁(S锁) ： 读锁，其他用户可以并发读取数据，但任何事务都不能获取数据上的排他锁，直到已释放所有共享锁 * 排它锁(X锁) ：写锁，若事务T对数据对象A加上X锁，则只允许T读取和修改A，其它任何事务都不能再对A加任何类型的锁，直到T释放A上的锁 3） 意向锁 * 意向共享锁(IS) ：表示事务准备给数据行记入共享锁，事务在一个数据行加共享锁前必须先取得该表的IS锁 * 意向排它锁(IX) ：表示事务准备给数据行加入排他锁，事务在一个数据行加排他锁前必须先取得该表的IX锁 意向锁是表级锁，**仅仅表示事务正在读或写某一行记录，在真正加行锁时才会判断是否冲突**，意向锁由数据库自动加载，无需用户干预。意向锁不会和行级别的X锁、S锁发生冲突，会和表级别的X锁、S锁发生冲突 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 概述MyQSQL的InnoDB中锁的兼容情况 ps:个人存在疑问 | | 共享锁 | 排它锁 | 意向共享锁 | 意向排它锁 | | ---------- | ------ | ------ | ---------- | ---------- | | 共享锁 | 👌 | ❌ | 👌 | ❌ | | 排它锁 | ❌ | ❌ | ❌ | ❌ | | 意向共享锁 | 👌 | ❌ | 👌 | 👌 | | 意向排它锁 | ❌ | ❌ | 👌 | 👌 | .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 日志篇 优化篇 什么是Multi-Range Read，解决的什么问题？ 它的原理是，将多个需要回表的二级索引根据主键进行排序，然后一起回表，将原来的回表时进行的随机IO，转变成顺序IO MRR 仅仅针对 **二级索引 的范围扫描** 和 **使用二级索引进行 join** 的情况。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 实战篇 一条sql的执行流程 一条sql的解析顺序 * from * join * on * where * group by(开始使用select中的别名，后面的语句中都可以使用) * avg,sum.... * having * select * distinct * order by * limit .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 一条SQL语句执行慢的原因有哪些 * 偶尔很慢的情况 * 数据库在刷新脏页(flush) ，flush有四种场景：redo log写满了；内存不够用了；MySQL认为服务器空闲了；MySQL正常关闭时刻 * 数据可能被其他连接加锁了 * 一直很慢的情况 * 服务器压力大 * 磁盘转速过低或者IO被其他服务大量使用 * SQL未正确使用索引导致全表扫描或者大范围锁表 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何查看SQL执行计划 在执行的语句前加上`explain`关键字 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何查看被优化器优化后的SQL ```mysql EXPLAIN ; SHOW WARNINGS; ``` .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 执行计划中的select type有哪些 * simple：表示不需要union操作或者不包含子查询的简单查询。 * primary：表示最外层查询。 * union：union操作中第二个及之后的查询。 * dependent union：union操作中第二个及之后的查询，并且该查询依赖于外部查 * subquery：子查询中的第一个查询。 * dependent subquery：子查询中的第一个查询，并且该查询依赖于外部查询。 * derived：派生表查询，既from字句中的子查询。 * materialized：物化查询。 * uncacheable subquery：无法被缓存的子查询，对外部查询的每一行都需要重新进行查询 * uncacheable union：union操作中第二个及之后的查询，并且该查询属于uncacheable subquery。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 执行计划中的type有哪些 按照性能从高到低依次为： * NULL：无需访问表或者索引，比如获取一个索引列的最大值或最小值。 * system/const：当查询最多匹配一行时，常出现于where条件是＝的情况。system是const的一种特殊情况，既表本身只有一行数据的情况。 * eq_ref：多表关联查询时，根据唯一非空索引进行查询的情况。 * ref：多表查询时，根据非唯一非空索引进行查询的情况。 * range：在一个索引上进行范围查找。 * index：遍历索引树查询，通常发生在查询结果只包含索引字段时。 * ALL：全表扫描，没有任何索引可以使用时。这是最差的情况，应该避免 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 执行计划中的extra有哪些 * Using filesort mysql ：对数据使用了一个外部的索引排序，而不是按照表内的索引顺序进行读取。Mysql中无法利用索引完成的排序操作称为“文件排序”。 * Using temporary 使用临时表保存中间结果，mysql在对查询结果排序时使用临时表。常见于排序order by和分组查询group by * Using index 表示响应的select操作中使用了索引覆盖，避免访问了表的数据行，效率不错。如果同时出现using where，表明索引被用来执行索引键值的查找；如果没有同时出现using where，表明索引用来读取数据而非执行查找动作 * Using where 表明使用了where过滤 * using join buffer 使用了连接缓存 * impossible where where子句的值总是false，不能用来获取任何元组 * select tables optimized away 在没有group by子句的情况下，基于索引优化Min、max操作或者对于MyISAM存储引擎优化count（*），不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化； * distinct 优化distinct操作，在找到第一匹配的元组后即停止找同样值的动作 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 什么是索引下推 简单讲就是将索引中的部分过滤工作下推到存储引擎层去执行 如果存在某些被索引的列的判断条件时，MySQL服务器将这一部分判断条件传递给存储引擎，然后由存储引擎通过判断索引是否符合MySQL服务器传递的条件，只有当索引符合条件时才会将数据检索出来返回给MySQL服务器 。 **索引条件下推优化可以减少存储引擎查询基础表的次数，也可以减少MySQL服务器从存储引擎接收数据的次数。 例如我们在name和age字段建立联合索引，sql为SELECT * from user where name like '陈%' and age=20, * 不使用索引下推时，直接通过name进行查询，在(name,age)这课树上查找到了两个结果，id分别为2,1，然后拿着取到的id值一次次的回表查询，因此这个过程需要**回表两次** * 在索引内部就判断了age是否等于20，对于不等于20的记录直接跳过，因此在(name,age)这棵索引树中只匹配到了一个记录，此时拿着这个id去主键索引树中回表查询全部数据，**这个过程只需要回表一次** .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } select(*)、select(1)、select(列名)有什么区别 Count(列名)表示的是该列的值不为空的总行数 count(*) = count(1),它们会统计所有行数(即使列的值为空)，具体的执行效率由优化器根据成本优化，默认选择最小成本的辅助索引(不选择主键索引是因为mysql的主键是聚簇的，主键与该行数据存储在一起，这时候辅助索引反而文件更小效率更高成本更低) .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 高可用篇 MySQL的主从复制有哪几种模式 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; }</content></entry><entry><title>Redis常见面试题</title><url>https://1162492411.github.io/docs/post/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories><category>Redis</category><category>NoSQL</category><category>数据库</category></categories><tags><tag>Redis</tag><tag>中间件</tag><tag>NoSQL</tag><tag>数据库</tag></tags><content type="html"> 原理篇 如何理解Redis的通讯协议resp协议 为了服务端和客户端轻量化通信而设计的简单协议 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 为什么早期版本Redis是单线程的 Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis为什么速度快 1）绝大部分操作为基于内存的操作 2）数据结构和对数据的操作简单 3）采用单线程减少上下文切换和竞争，不需要考虑锁的问题 4）使用多路I/O复用模型，非阻塞IO，多个网络连接使用同一个线程 5）通过队列将访问串形化，减少传统关系型数据的串行控制开销 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis如何划分内存 1）used_memory ：Redis分配器分配的内存总量； 2）used_memory_rss ：进程占据操作系统的内存； 3）mem_fragmentation_ratio ： 内存碎片率，used_memory_rss / used_memory； 4）mem_allocator ： 使用的内存分配器 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis事务的CAS .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } redis的文件事件处理器都包含哪些部分 1）多个 socket用来完成请求的接收与响应信息的发送 2）IO 多路复用程序 3）文件事件分派器用来协调调度 4）事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）用来真正干活 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 基础篇 Redis都有哪些基础的数据结构，他们各自的底层是如何实现的，对应的使用场景是什么 1）String ： 类似Java的动态数组，在内部预先分配一定空间，场景为存储键值对； 2）Hash ： 类似Java的HashMap，数据+链表结构，发生 hash 碰撞时将会把元素追加到链表上，场景为存储购物车信息或者对象； 3）List ： 类似Java的LinkedList，插入与删除数据的复杂度为O(1),数据量少时为一块内存连续的ziplist，数据量多时采用有前后指针的quicklist，redis3.2以后是ziplist+quicklist，场景为点赞列表、评论列表； 4）Set ： 类似Java的HashSet，键无序且唯一，value为null，场景为好友、关注、粉丝、感兴趣的人集合； 5）SortedSet ：有序集合，内部实现为ziplist或者skiplist，场景为排行榜 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis有哪些高级的数据结构，对应的使用场景是什么 1)BitMaps : 位图,面向bit进行操作,每个bit位为一个值,极度节省空间,经典使用场景是用户的每日签到记录 2)HyperLogLog : 基数统计,基数是数据集去重后元素个数,经典使用场景是统计用户UV 3)GEO : 处理地理位置 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis如何做到数据持久化，这些方式各自有什么优缺点 RDB保存快照，AOF保存执行命令的记录并合并命令； 1) RDB有两种方式：同步save模式和异步bgsave模式，同步save模式可以保证数据一致性； save会导致redis阻塞，bgsave在大数据量时fork会引起抖动，导致短暂时间内redis响应变慢，且fork需要一定的内存开销； rdb文件默认每次rdb时进行替换并压缩； rdb优点：文件紧凑，体积小，适合全量备份与复制，且加载rdb文件的速度比加载aof文件的速度快 rdb缺点：无法秒级别持久化，老版本redis无法兼容新版本的rdb 2) aof是目前主流的持久数据的方式，aof每次都会将写命令保存到缓冲区然后追加输出到aof文件中， .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis慢查询如何开启 设置slowlog-log-slower-than属性来配置慢查询时间的阈值，设置slowlog-max-len属性来配置存储多少条慢查询命令 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis的默认内存为多大 32位机器默认3个G，64位机器默认不限制 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis的淘汰策略有哪些 1）noeviction ： 不删除 2）allkeys-lru ： 从所有key中删除最近最少使用的key 3）volatile-lru ： 从设置了过期时间的key中删除最近最少使用的key 4）allkeys-random ： 从所有key中随机删除 5）volatile-random ： 从设置了过期时间的key中随机删除 6）volatile-ttl ： 从设置了过期时间的key中删除剩余时间最短的 7）allkeys-lfu ：淘汰访问频率最低的key 8）volatile-lfu ：只淘汰访问频率最低的过期key .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis的删除策略有哪些，这些删除策略各自有什么优缺点 1）定时删除 ： 在设置键的过期时间的同时，创建一个定时任务，当键达到过期时间时，立即执行对键的删除操作，优点是对内存友好可以即时释放，缺点是对cpu不友好可能大量key同时删除； 2）定期删除 ： 每隔一定时间删除过期的键，优点是对cpu友好，缺点是对内存不友好； 3）惰性删除 ： 放任键过期不管，但在每次获取键时，判断是否过期，若过期再删除，优点是对cpu友好，缺点是对内存不友好 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis的Pipeline如何理解 将多个命令一次性发送并执行，节省网络消耗，虽然命令执行时可能被其他命令穿插 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis如何设置过期时间 1）expire key milliseconds在指定毫秒后过期； 2）expire key seconds在指定秒后过期； 3）expire at key timestamp 在指定的时间戳（秒级别）后过期； 4）expire at key millisecondsTimestamp 在指定的时间戳（毫秒）后过期 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis支持哪些集群模式 1）主从复制模式；2）Sentinel哨兵模式；3）cluster模式 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis的事务是否支持回滚 Redis的事务有哪些相关命令 Redis有哪些常用的缓存更新策略 扩容篇 String如何扩容 增加空间时 ：小于1m时每次加倍扩容+1，大于1m时每次增加1m+1，最大为512m 释放空间时：惰性释放，当底层buf数组的实际使用小于数组容量时候不马上缩容。 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Hash如何扩容 1）申请旧hash两倍的内存空间，使得原有的字典同时持有旧hash表和新hash表 2）维护一个标志变量rehashindex用于记录进度 3）访问字典时将旧hash表中位于rehashindex这个桶中的key全部转移到新hash表中 4）全部转移完时修改记录进度rehashindex .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } List如何扩容 1）老版本的redis中list有两种数据结构 ： ziplist和linkedlist。首先以ziplist进行存储，在不满足ziplist的存储要求后转换为linkedlist列表 ziplist是为了节省内存而开辟的一连串连续空间，linkedlist则是以节点为单位的无环双向链表 当不满足以下条件时会将ziplist转换为linkedlist - 列表对象保存的所有字符串元素的长度小于64字节 - 列表对象保存的元素数量小于512个 2）后来redis3.2进一步引入了quicklist，以ziplist为单位，将多个ziplist作为链表的节点串起来 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 实战篇 Redis有哪些常用场景 1）缓存 2）Session共享 3）简单的消息队列 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何理解缓存穿透、缓存击穿、缓存雪崩，如何解决 1）缓存穿透 ： 查询一个redis中不可能存在的值，比如空值或者特殊的值，解决方案： * 将一些不存在的值也放入redis中； * 采用布隆过滤器； 2）缓存击穿 ：说单个key非常热点，当这个key在失效的瞬间，大量的请求直接请求数据库，解决方案： * 热点数据设置为永远不过期，缺点是内存消耗大，并且不能保持数据最新 * java层面的互斥 ：**通过synchronized+双重检查机制：某个key只让一个线程查询，阻塞其它线**程，在同步块中，继续判断检查，保证不存在，才去查DB。缺点是会阻塞其他线程 * redis层面的互斥 ：在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用set NX去set一个mutex key(最好加上失效时间避免待会儿删除mutex key失败)，当操作返回成功时，再进行load db的操作并回设缓存并删除mutex key；否则，就重试整个get缓存的方法。缺点是代码更复杂，存在死锁风险，删除mutex key失败时短期内这些key查不到数据 3）缓存雪崩 ： 大批量的缓存集中在某个时刻失效，解决方案： * 设置过期时间不一致 * 加锁排队 * 建立备份缓存 * 事前：redis 高可用，主从+哨兵，redis cluster，避免全盘崩溃。 - 事中：本地 ehcache 缓存 + hystrix 限流&amp;降级，避免 MySQL 被打死。 - 事后：redis持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据； .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis内存使用满会出现什么现象 无法写入只能读取 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis如何实现定时队列 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis如何实现消息队列 1）基于List的 LPUSH+BRPOP 的实现，使用rpush和lpush操作入队列，lpop和rpop操作出队列，引入阻塞读blpop和brpop，阻塞读在队列没有数据的时候进入休眠状态，一旦数据到来则立刻醒过来，消息延迟几乎为零，这种方案当一直没有消息时会导致连接空闲从而被释放，下次使用连接时报错，而且也没有消费者ACK机制，也不能重复消费，也不能进行广播； 2）PUB/SUB，订阅/发布模式，广播模式，消息可以即时发送，但是若消息发布时消费者不在线会丢失小消息，消息积压时也不好处理； 3）基于Sorted Set，消息id自己实现有序递增，缺点是不能存在重复的消息id； 4）基于stream，redis5.0开始支持，借鉴kafka，采用消息链表，消息持久化，可以记录消费者的消费进度，可以确保消息至少被消费一次，但是消息过多时旧消息会丢失，消费者消费消息但不ack会导致pel列表增大而消耗内存 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis的并发竞争如何解决 1）多个实例更新一个key时通过加锁排队让命令串形化 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis和数据库如何实现双写一致性 1）Cache Aside Pattern ： * 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 - 更新的时候，先更新数据库，然后再删除缓存，这种方案实际上在高并发的时候可以继续进行优化 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 有哪些基于Redis实现的分布式锁 1）setnx + expire； 2）setIfAbsent； 3）基于zookeeper的有序节点实现分布式锁； 4）redssion ：采用看门狗，定期续期 ； 5）redlock ： 将加锁命令发送到多个节点参与，如果大多数都加锁成功就成功，如果失败就逐个恢复锁； .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis中的key过期了是否立即释放内存，为什么 不是，为了快 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何保证Redis的高可用和高并发 1）基于一主多从，主节点进行写入，每秒w级别的qps，从节点进行读取，每秒10w级别的qps 2）加上哨兵，当节点出现故障时进行主备切换 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 一致性hash算法是什么 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis变慢如何排查 1）查看慢命令，分析是否存在复杂命令 2）查看是否存在大key 3）查看是否缓存雪崩 4）查看淘汰策略，查看内存是否打满 5）查看fork进程频率是否合理 6）查看内存分配是否合理 7）查看aof追加策略 8）如果是单机部署了多个redis，定位是否存在aof竞争问题 9）查看是否使用swap 10）查看网卡负载是否正常 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何为Redis一次增加大批量数据 1）管道2）手动拼接发送resp命令 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 主从模式 如何配置主从模式 * 临时性 ： 直接在客户端执行 slaveof ip port 的方式 * 永久性 ： 通过在配置文件 redis.conf 中设置 slaveof 方式 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 主从结构下，数据一致性问题如何解决 1）分析业务场景，若允许不一致，则无需处理 2）选择性读主：核心思路是将变动的情况写入在一个cache中，从节点从这个cache中查看是否包含本次的数据。 * 记录变化情况 ：将哪个库，哪个表，哪个主键三个信息拼装一个key设置到cache里，这条记录的超时时间，设置为“主从同步时延” * 查询时若cache有这个key ： 说明1s内刚发生过写请求，数据库主从同步可能还没有完成，此时就应该去主库查询 * 查询时若cache没有这个key ：说明最近没有发生过写请求，此时就可以去从库查询 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis的主从复制原理是什么 1）从节点连接主节点，向主节点发起同步请求 2）主节点开始生成rdb文件(如果是首次则全量生成，如果非首次，根据从节点传递的偏移量生成)一边发送rdb文件到从节点，一边将开始生成rdb文件之后到命令放入缓冲区 3）主节点将rdb文件发送到从节点之后，将缓冲区内的命令发送给从节点 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 主从模式下每次新增从节点时主节点均全量复制，如何减轻主节点的压力 在旧的从节点中建立级联从节点，这样虽然可能导致部分数据延迟复制到新的从节点，但是可以大幅度减小主节点压力 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 集群模式 集群模式下redis节点会开启哪些端口 * 6xxx端口 ： 提供正常的读写功能 * 6xxxx + 10000 端口 ： 通过集群总线进行节点间通信，进行故障检测，配置更新，故障转移授权 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } Redis集群模式下，redis的key如何寻址，分布式寻址都有哪些算法 寻址算法 ： 1）hash 算法（大量缓存重建）：计算hash后取模，访问不同的节点 2）一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡） ：将整个 hash 值空间组织成一个虚拟的圆环 3）redis cluster 的 hash slot 算法 ：对每个 key 计算 CRC16 值，然后对 16384 取模，放入16384个slot中的一个，每个redis节点持有部分slot。增加节点时各节点迁移部分slot给新节点，减少节点时将该节点的slot分发给其他节点 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 如何理解Redis的cluster bus的gossip协议 1）用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间； 2）gossip 协议包含多种消息，包含 ping、pong、meet、fail 等等； 3）ping ： 每个节点都会频繁给其它节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据； 4）meet ： 某个节点发送 meet 给新加入的节点，让新节点加入集群中； 5）pong ： 返回meet和ping； 6）fail ： 节点停止后发送fail告知其他节点； .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 一致性hash 一致性hash为了解决什么问题 一致性hash算法主要应用于分布式存储系统中，可以有效地解决分布式存储结构下普通余数Hash算法带来的伸缩性差的问题，可以保证在动态增加和删除节点的情况下尽量有多的请求命中原来的机器节点 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 一致性hash如何解决伸缩性差的问题 将节点ip根据 2的31次方取模，防止在hash环中，这样当节点增加/减少/发生故障时，仅会影响一两个redis节点的数据 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 一致性hash如何解决数据倾斜问题 引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在主机名的后面增加编号来实现。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node 1#1”、“Node 1#2”、“Node 1#3”、“Node 2#1”、“Node 2#2”、“Node 2#3”的哈希值，于是形成六个虚拟节点 .spoiler { color: black; background-color:black; white-space:pre-line; } .spoiler:hover{ color: white; } 优化篇 开发层次有哪些常用的优化建议 key的长度尽量要短，在数据量非常大时，过长的key名会占用更多的内存；
一定避免存储过大的数据（大value），过大的数据在分配内存和释放内存时耗时严重，会阻塞主线程；
Redis 4.0以上建议开启lazy-free机制，释放大value时异步操作，不阻塞主线程；
建议设置过期时间，把Redis当做缓存使用，尤其在数量很大的时，不设置过期时间会导致内存的无限增长；
不使用复杂度过高的命令，例如SORT、SINTER、SINTERSTORE、ZUNIONSTORE、ZINTERSTORE，使用这些命令耗时较久，会阻塞主线程；
查询数据时，一次尽量获取较少的数据，在不确定容器元素个数的情况下，避免使用LRANGE key 0 -1，ZRANGE key 0 -1这类操作，应该设置具体查询的元素个数，推荐一次查询100个以下元素；
写入数据时，一次尽量写入较少的数据，例如HSET key value1 value2 value3…，控制一次写入元素的数量，推荐在100以下，大数据量分多个批次写入；
批量操作数据时，用MGET/MSET替换GET/SET、HMGET/MHSET替换HGET/HSET，减少请求来回的网络IO次数，降低延迟，对于没有批量操作的命令，推荐使用pipeline，一次性发送多个命令到服务端；
禁止使用KEYS命令，需要扫描实例时，建议使用SCAN，线上操作一定要控制扫描的频率，避免对Redis产生性能抖动
避免某个时间点集中过期大量的key，集中过期时推荐增加一个随机时间，把过期时间打散，降低集中过期key时Redis的压力，避免阻塞主线程；
根据业务场景，选择合适的淘汰策略，通常随机过期要比LRU过期淘汰数据更快；
使用连接池访问Redis，并配置合理的连接池参数，避免短连接，TCP三次握手和四次挥手的耗时也很高；
只使用db0，不推荐使用多个db，使用多个db会增加Redis的负担，每次访问不同的db都需要执行SELECT命令，如果业务线不同，建议拆分多个实例，还能提高单个实例的性能；
读的请求量很大时，推荐使用读写分离，前提是可以容忍从节数据更新不及时的问题；
写请求量很大时，推荐使用集群，部署多个实例分摊写压力
运维层次有哪些常用的优化建议 不同业务线部署不同的实例，各自独立，避免混用，推荐不同业务线使用不同的机器，根据业务重要程度划分不同的分组来部署，避免某一个业务线出现问题影响其他业务线；
保证机器有足够的CPU、内存、带宽、磁盘资源，防止负载过高影响Redis性能；
以master-slave集群方式部署实例，并分布在不同机器上，避免单点，slave必须设置为readonly； master和slave节点所在机器，各自独立，不要交叉部署实例，通常备份工作会在slave上做，做备份时会消耗机器资源，交叉部署会影响到master的性能；
推荐部署哨兵节点增加可用性，节点数量至少3个，并分布在不同机器上，实现故障自动故障转移；
提前做好容量规划，一台机器部署实例的内存上限，最好是机器内存的一半，主从全量同步时会占用最多额外一倍的内存空间，防止网络大面积故障引发所有master-slave的全量同步导致机器内存被吃光；
做好机器的CPU、内存、带宽、磁盘监控，在资源不足时及时报警处理，Redis使用Swap后性能急剧下降，网络带宽负载过高访问延迟明显增大，磁盘IO过高时开启AOF会拖慢Redis的性能；
设置最大连接数上限，防止过多的客户端连接导致服务负载过高；
单个实例的使用内存建议控制在10G以下，过大的实例会导致备份时间久、资源消耗多，主从全量同步数据时间阻塞时间更长；
设置合理的slowlog阈值，推荐10毫秒，并对其进行监控，产生过多的慢日志需要及时报警；
设置合理的复制缓冲区repl-backlog大小，适当调大repl-backlog可以降低主从全量复制的概率；
设置合理的slave节点client-output-buffer-limit大小，对于写入量很大的实例，适当调大可以避免主从复制中断问题；
备份时推荐在slave节点上做，不影响master性能；
不开启AOF或开启AOF配置为每秒刷盘，避免磁盘IO消耗降低Redis性能；
当实例设置了内存上限，需要调大内存上限时，先调整slave再调整master，否则会导致主从节点数据不一致；
对Redis增加监控，监控采集info信息时，使用长连接，频繁的短连接也会影响Redis性能；
线上扫描整个实例数时，记得设置休眠时间，避免扫描时QPS突增对Redis产生性能抖动；
做好Redis的运行时监控，尤其是expired_keys、evicted_keys、latest_fork_usec指标，短时间内这些指标值突增可能会阻塞整个实例，引发性能问题</content></entry><entry><title>MySQL排他锁实战</title><url>https://1162492411.github.io/docs/post/mysql%E6%8E%92%E4%BB%96%E9%94%81%E5%AE%9E%E6%88%98/</url><categories><category>MySQL</category><category>锁</category></categories><tags><tag>MySQL</tag><tag>分布式锁</tag></tags><content type="html"> 1. 需求背景 ​ 基于MySQL/Oracle数据库实现分布式锁，保证一个项目中的定时任务代码在多台机器中同时执行时最多有一个任务可以成功获取锁，其他任务获取锁失败
2. 排他锁介绍 2.1 概念 ​ 如果事务T对数据A加上排他锁(exclusive lock，即X锁)后，则其他事务不能再对A加任任何类型的锁，将会等待事务T结束。获准排他锁的事务既能读数据，又能修改数据.
​ 在MySQL中，X锁仅适用于InnoDB引擎，而且必须在事务中才能生效，根据where条件是否通过索引命中数据，MySQL中的X锁分为行锁与表锁 ：命中数据时采用行锁，本质是对索引加锁；其他情况下均为表锁（例如没有where条件对应的数据，where后的字段没有索引）；特殊地，如果表数据过少，InnoDB引擎也可能将SQL优化为表锁，这种情况下可以通过force index来强制使用索引。
2.2 用法示例 2.2.1 基本用法 select … for update;
例如：select * from goods where id = 1 for update;
2.2.2 进阶用法 # nowait --&amp;gt; 不再等待事务而是立即返回结果，如果发现where条件的结果集被其他事务锁定则立即返回失败，该语法自MySQL的8.0版本开始支持，Oracle支持 select ... for update no wait; # wait --&amp;gt; 最多等待指定的时间x秒之后返回结果，该语法在Orale中支持 select ... for update wait x; # skip locked --&amp;gt; 如果数据锁定时跳过锁定的数据,该语法自MySQL的8.0版本开始支持，Oracle支持 select ... for update skip locked; 3.准备数据 3.1 准备表 create table t_gdts_sync_flag ( n_id bigint auto_increment comment '流水id' primary key, c_company_id varchar(20) null comment '集团id,对应t_gdts_company_rel的n_id', n_type tinyint(2) null comment '同步标识类型,1集团,2部门,3人员', c_status varchar(10) null comment '同步状态,sync/idle' ) comment '同步标识表'; 3.2 准备数据 INSERT INTO gropt.t_gdts_sync_flag (n_id, c_company_id, n_type, c_status) VALUES (75, '1326009432085127169', 1, 'idle'); INSERT INTO gropt.t_gdts_sync_flag (n_id, c_company_id, n_type, c_status) VALUES (76, '1326009432085127169', 2, 'idle'); INSERT INTO gropt.t_gdts_sync_flag (n_id, c_company_id, n_type, c_status) VALUES (77, '1326009432085127169', 3, 'idle'); 4. 实战 4.1 定义用于获取锁的线程池 4.2 获取锁的SQL语句 4.3 获取锁的Service代码 4.4 定时任务代码</content></entry><entry><title>Hystrix熔断器</title><url>https://1162492411.github.io/docs/post/hystrix%E7%86%94%E6%96%AD%E5%99%A8/</url><categories><category>熔断</category></categories><tags><tag>服务高可用</tag><tag>熔断</tag></tags><content type="html"> 熔断器 状态及转换 Hystrix提供的熔断器具有自我反馈，自我恢复的功能，Hystrix会根据调用接口的情况，让熔断器在closed,open,half-open三种状态之间自动切换,三种状态简要说明如下:
closed : 代表关闭熔断,默认状态,在此期间执行远程调用方法 open : 代表打开熔断,在此期间执行本地降级策略,不执行远程调用 half-open : 代表中间状态,在此期间,先执行远程调用,如果成功,下次继续执行远程调用,如果失败,转换为open状态 1)正常状态下为closed状态,若访问接口超过设置阈值且错误请求数比例达到设置值时,转换为open状态,时间段从0开始(打开熔断)
2)保持open状态一个时间段;
3)下个时间段后,状态自动转换为half-open,在此期间,
​ 3.1)如果第一次请求时接口失败,则转换为open状态,时间段从0开始,
​ 3.2)如果请求数量达到设置阈值且错误请求书比例未达到设置值时,转换状态为closed,时间段从0开始(恢复正常),
​ 3.3)如果请求数量没有达到阈值, 一直保持half-open状态
核心流程 将远程服务调用逻辑封装进一个HystrixCommand。 对于每次服务调用可以使用同步或异步机制，对应执行execute()或queue()。 判断熔断器(circuit-breaker)是否打开或者半打开状态，如果打开跳到步骤8，进行回退策略，如果关闭进入步骤4。 判断线程池/队列/信号量（使用了舱壁隔离模式）是否跑满，如果跑满进入回退步骤8，否则继续后续步骤5。 run方法中执行了实际的服务调用。 a. 服务调用发生超时时，进入步骤8。 判断run方法中的代码是否执行成功。 a. 执行成功返回结果。 b. 执行中出现错误则进入步骤8。 所有的运行状态(成功，失败，拒绝，超时)上报给熔断器，用于统计从而影响熔断器状态。 进入getFallback()回退逻辑。 a. 没有实现getFallback()回退逻辑的调用将直接抛出异常。 b. 回退逻辑调用成功直接返回。 c. 回退逻辑调用失败抛出异常。 返回执行成功结果。</content></entry><entry><title>服务压测调优</title><url>https://1162492411.github.io/docs/post/%E6%9C%8D%E5%8A%A1%E5%8E%8B%E6%B5%8B%E8%B0%83%E4%BC%98/</url><categories><category>调优</category></categories><tags><tag>服务优化</tag><tag>调优</tag><tag>Linux</tag></tags><content type="html"> 服务压测问题修复 Linux服务器环境优化 调整linux最大线程数 ​ /etc/sysctl.conf 配置文件中，加入 sys.kernel.threads-max = 40960
调整linux全局最大pid ​ /etc/sysctl.conf 配置文件中，加入 sys.kernel.pid_max = 40960
调整linux TCP进程参数 ​ /etc/sysctl.conf 配置文件中，加入 以下内容,执行：sysctl -p ，使设置立即生效：
# 进程可以同时打开的最大文件句柄数，这个参数直接限制最大并发连接数 fs.file-max=999999 ############## TCP数据窗口相关参数 ############## # 默认的TCP数据接收窗口大小/字节,默认229376 net.core.rmem_default = 256960 # 最大的TCP数据接收窗口大小/字节,默认131071 net.core.rmem_max = 513920 # 默认的TCP数据发送窗口大小/字节,默认229376 net.core.wmem_default = 256960 # 最大的TCP数据发送窗口/字节,默认131071 net.core.wmem_max = 513920 ################# TCP队列相关参数 ############### # 当网卡接收数据包的速度大于内核处理数据的时候，会有一个队列保存这些数据包,即接收队列长度。这个参数表示这个队列的最大值,默认1000 net.core.netdev_max_backlog = 2000 # TCP三次握手建立阶段服务器接收SYN请求队列的最大长度,即SYN半连接队列长度,对于超出该队列的请求直接丢弃 net.ipv4.tcp_max_syn_backlog = 262144 # 全局的每一个端口最大的监听队列的长度,默认128 net.core.somaxconn = 2048 ############## TCP缓冲区相关参数 ########### # 全局的所有TCP的SocketBuffer配置,该SocketBuffer用于发送方发送数据/接收方接受数据时存储这些数据,有三个值，单位为内存页(通常为4K):当TCP使用了低于第一个值的内存页面数时，TCP不会考虑释放内存;当TCP使用了超过第二个值的内存页面数量时,TCP试图稳定其内存使用，进入pressure模式;当内存占用超过第三个值，系统拒绝分配socket,报错TCP: too many of orphaned sockets.默认94011 125351 188022 net.ipv4.tcp_mem = 131072 262144 524288 # TCP读缓冲区/字节,三个值分别表示TCP接收缓存（用于TCP接收滑动窗口）的最小值、默认值、最大值,默认4096 87380 4011232 net.ipv4.tcp_rmem = 8760 256960 4088000 # TCP写缓冲区/字节,三个值分别表示TCP发送缓存（用于TCP接收滑动窗口）的最小值、默认值、最大值,默认4096 16384 4011232 net.ipv4.tcp_wmem = 8760 256960 4088000 # 每个套接字所允许的最大缓冲区的大小/字节,默认20480 net.core.optmem_max = 81920 ############### keepalive相关参数 ##################### # CLOSE_WAIT 状态维持的秒数 = tcp_keepalive_time + tcp_keepalive_intvl * tcp_keepalive_probes # 当keepalive启用时，TCP发送keealive消息的频度,默认7200/秒 net.ipv4.tcp_keepalive_time = 1800 # 以该参数指定的秒数为时间间隔/s，向客户端发起对它的探测 net.ipv4.tcp_keepalive_intvl = 30 # 内核发起对客户端探测的次数，如果都没有得到响应，那么就断定客户端不可达或者已关闭，内核就关闭该TCP连接 net.ipv4.tcp_keepalive_probes = 3 ############### Time_wait相关参数 ##################### # 是否开启timstamp校验,该配置项会影响net.ipv4.tcp_tw_reuse、net.ipv4.tcp_tw_recycle,只有发起方和接收方都开启该项才会使得net.ipv4.tcp_tw_reuse、net.ipv4.tcp_tw_recycle生效,该配置项提供以下两个功能:a.更加准确的RTT测量数据，尤其是有丢包时 – RTTM b. 保证了在极端情况下，TCP的可靠性 – PAWS net.ipv4.tcp_timestamps = 1 # 是否允许将TIME—WAIT状态的socket重新用于新的TCP连接,默认0,1开启,开启后将会在Time_wait状态的1s后复用socket net.ipv4.tcp_tw_reuse = 1 # 设置是否对TIME_WAIT状态的TCP进行快速回收,默认0,1开启 net.ipv4.tcp_tw_recycle = 1 # 当服务器主动关闭连接的时候，主动关闭方的socket保持在FIN-WAIT-2状态的最大时间/秒,默认60 net.ipv4.tcp_fin_timeout = 30 ############### 其他参数 ##################### # 是否启用有选择的应答,开启此项后,可以让发送方只发送丢失部分的数据,即支持乱序接收 net.ipv4.tcp_sack = 1 # 是否打开FACK拥塞避免和快速重传功能 net.ipv4.tcp_fack = 1 # 是否支持更大的TCP窗口,如果TCP窗口最大超过65535(64K), 必须设置该数值为1 net.ipv4.tcp_window_scaling = 1 # 是否打开SYN Cookie功能，该功能可以防止部分SYN flood攻击 net.ipv4.tcp_syncookies = 1 # 在UDP和TCP连接本地端口的取值范围 net.ipv4.ip_local_port_range = 1024 65000 调整linux最大文件数量 /etc/security/limits.conf文件尾部添加如下代码：
* soft nofile 65535 * hard nofile 65535 Tmcat参数优化 设置内存参数 tomcat安装目录/bin/catalina.sh 106 行添加以下内容
JAVA_OPTS=&amp;quot;-Xmx8192M -Xms8192M -XX:MaxPermSize=512M -XX:PermSize=512M -XX:+UseG1GC -XX:MaxGCPauseMillis=100 -XX:+ParallelRefProcEnabled -XX:+HeapDumpOnOutOfMemoryError -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC&amp;quot; 【注意】不要换行是一行 参数 参考地址为：https://console.perfma.com/ 线程数计算公式： Max memory （除去系统占用和其他应用程序占用后的操作系统总内存大小） = [-Xmx] + [-XX:MaxMetaSpaceSize] + number_of_threads * [-Xss]
修改日志打印 访问日志原始值：
&amp;lt;Valve className=&amp;quot;org.apache.catalina.valves.AccessLogValve&amp;quot; directory=&amp;quot;logs&amp;quot; prefix=&amp;quot;localhost_access_log.&amp;quot; suffix=&amp;quot;.txt&amp;quot; pattern=&amp;quot;%h %l %u %t %s %b&amp;quot; /&amp;gt; 调整后的值:
&amp;lt;Valve className=&amp;quot;org.apache.catalina.valves.AccessLogValve&amp;quot; directory=&amp;quot;logs&amp;quot; prefix=&amp;quot;localhost_access_log.&amp;quot; suffix=&amp;quot;.txt&amp;quot; pattern=&amp;quot;%h %l %u %t %q %s %b %F %D %I &amp;quot; /&amp;gt; pattern属性的值由文字文本字符串和以“％”字符为前缀的模式标识符组合而成，以替换为来自当前请求和响应的相应变量值。支持以下模式代码：(原始地址:http://tomcat.apache.org/tomcat-7.0-doc/config/valve.html)
％ a-远程IP地址 **％A-**本地IP地址 **％b-**发送的字节，不包括HTTP标头，如果为零则为&amp;rsquo;-&amp;rsquo; **％B-**发送的字节，不包括HTTP标头 **％h-**远程主机名（如果enableLookups连接器为false，则为IP地址 ） **％H-**请求协议 ％l -identd的远程逻辑用户名（总是返回“-”） **％m-**请求方法（GET，POST等） **％p-**接收此请求的本地端口。另请参见%{xxx}p下文。 **％q-**查询字符串（如果存在，则以“？”开头） **％r-**请求的第一行（方法和请求URI） **％s-**响应的HTTP状态代码 **％S-**用户会话ID **％t-**日期和时间，采用通用日志格式 **％u-**已验证（如果有）的远程用户，否则为&amp;rsquo;-&amp;rsquo; **％U-**请求的URL路径 **％v-**本地服务器名称 **％D-**以毫秒为单位处理请求所花费的时间。注意：在httpd中，％D是微秒。从Tomcat 10开始，行为将与httpd对齐。 **％T-**处理请求所花费的时间，以秒为单位。注意：此值具有毫秒分辨率，而在httpd中具有第二分辨率。行为将与Tomcat 10及更高版本中的httpd保持一致。 **％F-**提交响应所花费的时间（以毫秒为单位） **％I-**当前请求线程名称（以后可以与堆栈跟踪进行比较） 调整线程数 tomcat安装目录/conf/server.xml 71行
调整前原始值：
&amp;lt;Connector port=&amp;quot;8080&amp;quot; protocol=&amp;quot;HTTP/1.1&amp;quot; connectionTimeout=&amp;quot;20000&amp;quot; redirectPort=&amp;quot;8443&amp;quot; /&amp;gt; 调整后的值：
&amp;lt;Connector port=&amp;quot;8080&amp;quot; protocol=&amp;quot;HTTP/1.1&amp;quot; maxKeepAliveRequests=&amp;quot;200&amp;quot; socketBuffer=&amp;quot;9000&amp;quot; enableLookups=&amp;quot;false&amp;quot; tcpNoDelay=&amp;quot;true&amp;quot; minSpareThreads=&amp;quot;100&amp;quot; maxSpareThreads=&amp;quot;100&amp;quot; maxThreads=&amp;quot;2000&amp;quot; connectionTimeout=&amp;quot;5000&amp;quot; maxHttpHeaderSize=&amp;quot;32768&amp;quot; URIEncoding=&amp;quot;UTF-8&amp;quot; acceptCount=&amp;quot;200&amp;quot; redirectPort=&amp;quot;8443&amp;quot; /&amp;gt; 配置项解释
参数 含义 示例 port 绑定的端口,如果设置为0，tomcat则随机获取一个空闲端口 默认 port=&amp;quot;8080&amp;rdquo; protocol 传输协议和版本 默认 protocol = &amp;ldquo;HTTP/1.1&amp;rdquo; connectionTimeout 连接超时时间，单位毫秒 默认 connectionTimeout=&amp;quot;20000&amp;rdquo; redirectPort 接收到的ssl请求后重定向的端口 默认 redirectPort=&amp;quot;8443&amp;rdquo; maxThreads tomcat能创建来处理请求的最大线程数，也为最大并发数 超过则放入请求队列中进行排队，默认值为200；需要根据业务和系统性能进行调整 maxThreads=&amp;quot;1000&amp;rdquo; URIEncoding url的字符编码，在tomcat8.5版本中，该值默认为UTF-8,除非在org.apache.catalina.STRICT_SERVLET_COMPLIANCE 将system property 设置为true才会使用ISO-8859-1 URIEncoding=&amp;quot;UTF-8&amp;rdquo; minProcessors 启动时创建的线程数（最小线程数） minProcessors=&amp;quot;50&amp;rdquo; acceptCount 指定当所有可以使用的处理请求的线程数都被使用时，可以放到队列中的请求数，就是被排队的请求数，超过这个数的请求将拒绝连接 默认值为100 acceptcount=&amp;quot;500&amp;rdquo; acceptorThreadCount 可以用于接受连接的进程数，默认为1，但是在一些多核的的服务器上，我们会将它的值设置为2或者更大的数，来应对一些不活跃的连接。 minSpareThreads 最小空闲线程数，任何情况都会存活的线程数，即便超过了最大空闲时间，也不会被回收，默认值10; minSpareThreads=&amp;quot;25&amp;rdquo; maxSpareThreads 最大空闲线程数，在最大空闲时间（maxIdleTime）内活跃过，此时空闲，当空闲时间大于maxIdleTime则被回收，小则继续存活，等待被调度，默认值50； enableLookups 调用request、getRemoteHost()执行DNS查询，以返回远程主机的主机名，如果设置为false，则直接返回IP地址 默认是禁用的，在请求过滤中的根据远程主机名过滤，需要将该参数设置为true enableLookups=&amp;quot;false&amp;rdquo; maxIdleTime 最大空闲时间，超过这个空闲时间，且线程数大于minSpareThreads的，都会被回收，默认值1分钟（60000ms) maxPostSize address 对于一些具有多个ip的服务器，我们可以通过该参数指定绑定的ip，默认情况下监听所有的地址 address=&amp;quot;192.168.1.110&amp;rdquo; compressibleMimeType 该值用来指定哪些文件类型的文件可以进行压缩，默认值为：text/html,text/xml,text/plain,text/css,text/javascript,application/javascript compression 开启gzip 压缩，可以接受的值是 &amp;ldquo;off&amp;rdquo;(禁用压缩),&amp;ldquo;on&amp;rdquo;(开启压缩),&amp;ldquo;force(强制压缩)&amp;quot;，&amp;ldquo;1-9&amp;rdquo;(等效于开启压缩，并且设定压缩等级),开启了压缩，也就意味着要占用更多的cpu资源 compression compressionMinSize 在compression 参数指定为on后，该参数用来指定压缩的阈值，只有大于该阈值才会被压缩，默认为 2048 keepAliveTimeout 指connector两个HTTP请求直接的等待时间，超过该时间没有接收到第二个HTTP请求就关闭连接，默认是使用connectionTimeout 的值，单位为毫秒 maxConnections 在一定时间内可以接受和处理的最大连接数，达到限制后，服务器接受但不处理该链接，但可以存放到acceptCount，该默认值因连接器类型而异。对于NIO和NIO2，默认值为10000。对于APR / native，默认为8192。 maxCookieCount 请求允许的最大cookie 数，值小于0表示无限制，默认值为 200 disableUploadTimeout 默认是true ，禁用数据上传超时 connectionUploadTimeout 设定数据上传的超时时间，只有在disableUploadTimeout设置为false才生效，单位毫秒 connectionUploadTimeout=&amp;quot;50000&amp;rdquo; processorCache 进程缓冲器，默认值是maxThreads的值,使用好该值可以提升并发请求。</content></entry><entry><title>Docker 常用命令</title><url>https://1162492411.github.io/docs/post/docker-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url><categories><category>Docker</category></categories><tags><tag>容器化</tag><tag>部署运维</tag><tag>Docker</tag><tag>命令</tag></tags><content type="html"> Docker是一个开源的应用容器引擎，让开发者可以打包应用及依赖包到一个可移植的镜像中，然后发布到任何流行的Linux或Windows机器上。使用Docker可以更方便地打包、测试以及部署应用程序。
Docker环境安装 安装yum-utils； yum install -y yum-utils device-mapper-persistent-data lvm2 复制代码 为yum源添加docker仓库位置； yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 复制代码 安装docker服务； yum install docker-ce 复制代码 启动docker服务。 systemctl start docker 复制代码 Docker镜像常用命令 搜索镜像 docker search java 复制代码 下载镜像 docker pull java:8 复制代码 查看镜像版本 由于docker search命令只能查找出是否有该镜像，不能找到该镜像支持的版本，所以我们需要通过Docker Hub来搜索支持的版本。
进入Docker Hub的官网，地址：https://hub.docker.com
然后搜索需要的镜像：
查看镜像支持的版本：
进行镜像的下载操作：
docker pull nginx:1.17.0 列出镜像 docker images 删除镜像 指定名称删除镜像： docker rmi java:8 指定名称删除镜像（强制）： docker rmi -f java:8 删除所有没有引用的镜像： docker rmi `docker images | grep none | awk &amp;#39;{print $3}&amp;#39;` 强制删除所有镜像： docker rmi -f $(docker images) 打包镜像 # -t 表示指定镜像仓库名称/镜像名称:镜像标签 .表示使用当前目录下的Dockerfile文件 docker build -t mall/mall-admin:1.0-SNAPSHOT . Docker容器常用命令 新建并启动容器 docker run -p 80:80 --name nginx \ -e TZ=&amp;#34;Asia/Shanghai&amp;#34; \ -v /mydata/nginx/html:/usr/share/nginx/html \ -d nginx:1.17.0 -p：将宿主机和容器端口进行映射，格式为：宿主机端口:容器端口； &amp;ndash;name：指定容器名称，之后可以通过容器名称来操作容器； -e：设置容器的环境变量，这里设置的是时区； -v：将宿主机上的文件挂载到宿主机上，格式为：宿主机文件目录:容器文件目录； -d：表示容器以后台方式运行。 列出容器 列出运行中的容器： docker ps 列出所有容器： docker ps -a 停止容器 注意：$ContainerName表示容器名称，$ContainerId表示容器ID，可以使用容器名称的命令，基本也支持使用容器ID，比如下面的停止容器命令。
docker stop $ContainerName(or $ContainerId) 例如：
docker stop nginx #或者 docker stop c5f5d5125587 强制停止容器 docker kill $ContainerName 启动容器 docker start $ContainerName 进入容器 先查询出容器的pid： docker inspect --format &amp;#34;{{.State.Pid}}&amp;#34; $ContainerName 根据容器的pid进入容器： nsenter --target &amp;#34;$pid&amp;#34; --mount --uts --ipc --net --pid 删除容器 删除指定容器： docker rm $ContainerName 按名称通配符删除容器，比如删除以名称mall-开头的容器： docker rm `docker ps -a | grep mall-* | awk &amp;#39;{print $1}&amp;#39;` 强制删除所有容器； docker rm -f $(docker ps -a -q) 查看容器的日志 查看容器产生的全部日志： docker logs $ContainerName 动态查看容器产生的日志： docker logs -f $ContainerName 查看容器的IP地址 docker inspect --format &amp;#39;{{ .NetworkSettings.IPAddress }}&amp;#39; $ContainerName 修改容器的启动方式 # 将容器启动方式改为always docker container update --restart=always $ContainerName 同步宿主机时间到容器 docker cp /etc/localtime $ContainerName:/etc/ 指定容器时区 docker run -p 80:80 --name nginx \ -e TZ=&amp;#34;Asia/Shanghai&amp;#34; \ -d nginx:1.17.0 查看容器资源占用状况 查看指定容器资源占用状况，比如cpu、内存、网络、io状态： docker stats $ContainerName 查看所有容器资源占用情况： docker stats -a 查看容器磁盘使用情况 docker system df 执行容器内部命令 docker exec -it $ContainerName /bin/bash 指定账号进入容器内部 # 使用root账号进入容器内部 docker exec -it --user root $ContainerName /bin/bash 查看所有网络 docker network ls ## 结果示例 [root@local-linux ~]# docker network ls NETWORK ID NAME DRIVER SCOPE 59b309a5c12f bridge bridge local ef34fe69992b host host local a65be030c632 none 创建外部网络 docker network create -d bridge my-bridge-network 指定容器网络 docker run -p 80:80 --name nginx \ --network my-bridge-network \ -d nginx:1.17.0 修改镜像的存放位置 查看Docker镜像的存放位置： docker info | grep &amp;#34;Docker Root Dir&amp;#34; 关闭Docker服务： systemctl stop docker 先将原镜像目录移动到目标目录： mv /var/lib/docker /mydata/docker 建立软连接： ln -s /mydata/docker /var/lib/docker 再次查看可以发现镜像存放位置已经更改。 本文 GitHub github.com/macrozheng/… 已经收录，欢迎大家Star！
作者：MacroZheng 链接：https://juejin.cn/post/6895888125886332941 来源：掘金 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</content></entry><entry><title>Spring中Async的使用与源码解析</title><url>https://1162492411.github.io/docs/post/spring%E4%B8%ADasync%E7%9A%84%E4%BD%BF%E7%94%A8%E4%B8%8E%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/</url><categories><category>SpringBoot</category><category>线程池</category></categories><tags><tag>线程池</tag><tag>异步</tag><tag>SpringBoot</tag></tags><content type="html"> 背景介绍 对于异步方法调用，从Spring3开始提供了@Async注解，该注解可以被标注在方法上，以便异步地调用该方法。调用者将在调用时立即返回，方法的实际执行将提交给Spring TaskExecutor的任务中，由指定的线程池中的线程执行。
常见的场景 系统日志记录 耗时任务的执行 使用方法 1.启动类增加@EnableAsync注解(since Spring 3.1)
@EnableAsync @SpringBootApplication public class SpringBootDemoAsyncApplication { public static void main(String[] args) { SpringApplication.run(SpringBootDemoAsyncApplication.class, args); } } 2.如有需要，可以自定义线程池
@Configuration public class ExecutorConfiguration { /** * 配置应用访问日志专用线程池 * @return */ @Bean(name = &amp;#34;sodAppLogAsyncExecutor&amp;#34;) public ThreadPoolTaskExecutor asyncExecutor() { ThreadPoolTaskExecutor threadPool = new ThreadPoolTaskExecutor(); threadPool.setThreadNamePrefix(&amp;#34;drs-sodAppLog-&amp;#34;); threadPool.setCorePoolSize(3); threadPool.setMaxPoolSize(4); threadPool.setKeepAliveSeconds(60); threadPool.setQueueCapacity(11); threadPool.setRejectedExecutionHandler(new ThreadPoolExecutor.DiscardPolicy()); //优雅关闭 threadPool.setWaitForTasksToCompleteOnShutdown(true); threadPool.setAwaitTerminationSeconds(60 * 15); return threadPool; } } 3.在需要使用异步的方法上添加@Async注解，可以通过value属性指定线程池,返回值支持void、Future、ListenableFuture、CompletableFuture，如果不指定value，那么采用默认线程池
/** * 模拟5秒的异步任务 */ @Async public Future&amp;lt;Boolean&amp;gt; asyncTask1() throws InterruptedException { doTask(&amp;#34;asyncTask1&amp;#34;, 5); return new AsyncResult&amp;lt;&amp;gt;(Boolean.TRUE); } /** * 模拟业务代码 * @param taskName * @param time */ private void doTask(String taskName, Integer time) { log.info(&amp;#34;{}模拟执行【{}】,线程内存地址:{}&amp;#34;, taskName, Thread.currentThread().getName(), UnsafeUtil.addressOf(Thread.currentThread())); } Spring实现的线程池 SimpleAsyncTaskExecutor：默认线程池，每次调用都启动一个新线程(并不会复用线程池已有线程),支持对并发总数设限（ConcurrencyLimit，默认-1不限制，0不允许），当超过线程并发总数限制时，阻塞新的调用 ThreadPoolTaskExecutor:对JDK的ThreadPoolExecutor的封装，SpringBoot通过TaskExecutionAutoConfiguration自动装配了一个名为applicationTaskExecutor的ThreadPoolTaskExecutor @ConditionalOnClass(ThreadPoolTaskExecutor.class) @Configuration @EnableConfigurationProperties(TaskExecutionProperties.class) public class TaskExecutionAutoConfiguration { @Bean @ConditionalOnMissingBean public TaskExecutorBuilder taskExecutorBuilder() { TaskExecutionProperties.Pool pool = this.properties.getPool(); TaskExecutorBuilder builder = new TaskExecutorBuilder(); builder = builder.queueCapacity(pool.getQueueCapacity()); builder = builder.corePoolSize(pool.getCoreSize()); builder = builder.maxPoolSize(pool.getMaxSize()); builder = builder.allowCoreThreadTimeOut(pool.isAllowCoreThreadTimeout()); builder = builder.keepAlive(pool.getKeepAlive()); builder = builder.threadNamePrefix(this.properties.getThreadNamePrefix()); builder = builder.customizers(this.taskExecutorCustomizers); builder = builder.taskDecorator(this.taskDecorator.getIfUnique()); return builder; } @Lazy @Bean(name = APPLICATION_TASK_EXECUTOR_BEAN_NAME) @ConditionalOnMissingBean(Executor.class) public ThreadPoolTaskExecutor applicationTaskExecutor(TaskExecutorBuilder builder) { return builder.build(); } } SimpleAsyncTaskExecutor 属性列表
Daemon:是否为守护线程，默认false ThreadPriority:线程优先级,默认5 ThreadNamePrefix:线程名前缀，默认&amp;quot;SimpleAsyncTaskExecutor&amp;rdquo; ConcurrencyLimit:并发上限,默认-1不限制，0表示不允许并发？？？？ ThreadPoolTaskExecutor 属性列表
CorePoolSize：线程池创建时候初始化的线程数,默认1 MaxPoolSize：线程池最大的线程数，只有在缓冲队列满了之后才会申请超过核心线程数的线程，默认Integer.MAX QueueCapacity：用来缓冲执行任务的队列的队列大小，默认Integer.MAX KeepAliveSeconds：线程的空闲时间，单位/s，当超过了核心线程出之外的线程在空闲时间到达之后会被销毁,默认60 ThreadNamePrefix：线程池中线程名的前缀，继承自父类ExecutorConfigurationSupport，默认是BeanName/方法名 RejectedExecutionHandler：线程池对拒绝任务的处理策略，自父类ExecutorConfigurationSupport,（策略为JDK ThreadPoolExecutor自带） AbortPolicy：默认策略，直接抛出异常 RejectedExecutionException CallerRunsPolicy：直接在 execute 方法的调用线程中运行被拒绝的任务；如果执行程序已关闭，则会丢弃该任务 DiscardPolicy：该策略直接丢弃 DiscardOldestPolicy：该策略会先将最早入队列的未执行的任务丢弃掉，然后尝试执行新的任务。如果执行程序已关闭，则会丢弃该任务 waitForTasksToCompleteOnShutdown：关闭程序时是否等待任务执行完毕，继承自父类ExecutorConfigurationSupport，默认false表示中断正在执行的任务，清空队列 awaitTerminationSeconds：关闭程序时的等待时间，需配合waitForTasksToCompleteOnShutdown使用，继承自父类ExecutorConfigurationSupport，默认0 线程处理流程 /** * Executes the given task sometime in the future. The task * may execute in a new thread or in an existing pooled thread. * * If the task cannot be submitted for execution, either because this * executor has been shutdown or because its capacity has been reached, * the task is handled by the current {@code RejectedExecutionHandler}. * * @param command the task to execute * @throws RejectedExecutionException at discretion of * {@code RejectedExecutionHandler}, if the task * cannot be accepted for execution * @throws NullPointerException if {@code command} is null */ public void execute(Runnable command) { if (command == null) throw new NullPointerException(); /* * Proceed in 3 steps: * * 1. If fewer than corePoolSize threads are running, try to * start a new thread with the given command as its first * task. The call to addWorker atomically checks runState and * workerCount, and so prevents false alarms that would add * threads when it shouldn&amp;#39;t, by returning false. * * 2. If a task can be successfully queued, then we still need * to double-check whether we should have added a thread * (because existing ones died since last checking) or that * the pool shut down since entry into this method. So we * recheck state and if necessary roll back the enqueuing if * stopped, or start a new thread if there are none. * * 3. If we cannot queue task, then we try to add a new * thread. If it fails, we know we are shut down or saturated * and so reject the task. */ int c = ctl.get(); if (workerCountOf(c) &amp;lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } if (isRunning(c) &amp;amp;&amp;amp; workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) &amp;amp;&amp;amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } else if (!addWorker(command, false)) reject(command); } 如果此时线程池中的数量小于corePoolSize，即使线程池中的线程都处于空闲状态，也要创建新的线程来处理被添加的任务。 如果此时线程池中的数量等于 corePoolSize，但是缓冲队列 workQueue未满，那么任务被放入缓冲队列。 如果此时线程池中的数量大于corePoolSize，缓冲队列workQueue满，并且线程池中的数量小于maxPoolSize，那么建新的线程来处理被添加的任务。 如果此时线程池中的数量大于corePoolSize，缓冲队列workQueue满，并且线程池中的数量等于maxPoolSize，那么通过handler所指定的策略来处理此任务。（也就是：处理任务的优先级为：核心线程corePoolSize、任务队列workQueue、最大线程 maxPoolSize，如果三者都满了，使用handler处理被拒绝的任务） 当线程池中的线程数量大于corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终止。这样，线程池可以动态的调整池中的线程数 自定义线程池 自定义线程池有如下模式：
配置由自定义的TaskExecutor 重新实现接口AsyncConfigurer 继承AsyncConfigurerSupport 方式一：自定义TaskExecutor @Configuration public class ExecutorConfiguration { /** * 配置应用访问日志专用线程池 * @return */ @Bean(name = &amp;#34;sodAppLogAsyncExecutor&amp;#34;) public ThreadPoolTaskExecutor asyncExecutor() { ThreadPoolTaskExecutor threadPool = new ThreadPoolTaskExecutor(); threadPool.setThreadNamePrefix(&amp;#34;drs-sodAppLog-&amp;#34;); threadPool.setCorePoolSize(3); threadPool.setMaxPoolSize(4); threadPool.setKeepAliveSeconds(60); threadPool.setQueueCapacity(11); threadPool.setRejectedExecutionHandler(new ThreadPoolExecutor.DiscardPolicy()); //优雅关闭 threadPool.setWaitForTasksToCompleteOnShutdown(true); threadPool.setAwaitTerminationSeconds(60 * 15); return threadPool; } } 方式二：实现AsyncConfigurer /** * 自定义线程池方法二：自定义类，配置默认Executor与默认异步异常处理器 * @author zyg */ @Configuration public class CusAsyncConfigure implements AsyncConfigurer { /** * 配置默认Executor */ @Override public Executor getAsyncExecutor() { ThreadPoolTaskExecutor threadPool = new ThreadPoolTaskExecutor(); threadPool.setThreadNamePrefix(&amp;#34;cus-async-configure-&amp;#34;); threadPool.setCorePoolSize(2); threadPool.setMaxPoolSize(3); threadPool.setKeepAliveSeconds(60); threadPool.setQueueCapacity(5); threadPool.setRejectedExecutionHandler(new ThreadPoolExecutor.DiscardPolicy()); return threadPool; } /** * 配置默认异步异常处理器 */ @Override public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() { return new CusAsyncUncaughtExceptionHandler(); } } （原理是ProxyAsyncConfiguration的父类AbstractAsyncConfiguration的setConfigurers(Collection)中执行了AsyncConfigurer的方法来配置Executor与AsyncUncaughtExceptionHandler）
方式三：继承AsyncConfigurerSupport /** * 自定义线程池方法三:继承AsyncConfigurerSupport,重写getAsyncExecutor与getAsyncUncaughtExceptionHandler * @author zyg */ @Configuration public class CusAsyncConfigurerSupport extends AsyncConfigurerSupport { /** * 配置默认Executor */ @Override public Executor getAsyncExecutor() { ThreadPoolTaskExecutor threadPool = new ThreadPoolTaskExecutor(); threadPool.setThreadNamePrefix(&amp;#34;cus-async-configure-support-&amp;#34;); threadPool.setCorePoolSize(2); threadPool.setMaxPoolSize(3); threadPool.setKeepAliveSeconds(60); threadPool.setQueueCapacity(5); threadPool.setRejectedExecutionHandler(new ThreadPoolExecutor.DiscardPolicy()); return threadPool; } /** * 配置默认异步异常处理器 */ @Override public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() { return new CusAsyncUncaughtExceptionHandler(); } } （原理是AsyncConfigurerSupport的父类是AsyncConfigurer）
异常处理 如果任务的返回类型是Future，那么将直接抛出异常，否则异常由AsyncUncaughtExceptionHandler的handleUncaughtException()进行处理，Spring自4.1默认提供了SimpleAsyncUncaughtExceptionHandler，该类处理异常的逻辑是通过日志打印错误，如有需要可以自定义类继承AsyncUncaughtExceptionHandler，复写其handleUncaughtException()方法。
/** * 自定义线程池方法二：自定义默认异步异常处理器 * @author zyg */ @Component public class CusAsyncUncaughtExceptionHandler implements AsyncUncaughtExceptionHandler { private Logger logger = LoggerFactory.getLogger(this.getClass()); @Override public void handleUncaughtException(Throwable ex, Method method, Object... params) { logger.error(&amp;#34;自定义异步异常处理器捕捉到异常，&amp;#34;,ex); } } @EnableAsync加载流程 前置知识点 @Import注解的作用 BeanPostProcessor在Spring中的作用 Aware类接口在Spring中的作用 切面与通知的概念与作用 代码分析： @EnableAsync中Import了AsyncConfigurationSelector； AsyncConfigurationSelector的作用是通过配置确定是调用ProxyAsyncConfiguration还是AspectJ的AspectJAsyncConfiguration； 在ProxyAsyncConfiguration的asyncAdvisor()方法可以看到，其中定义了后置处理器AsyncAnnotationBeanPostProcessor AsyncAnnotationBeanPostProcessor直接或间接实现了BeanFactoryAware、BeanPostProcessor两个接口，既然AsyncAnnotationBeanPostProcessor实现了BeanFactoryAware，那么就会执行setBeanFactory(BeanFactory)方法,该方法中设置了切面AsyncAnnotationAdvisor 切面中定义了切点：类上标注@Async、@Asynchronous注解的切点与在方法上标注@Async、@Asynchronous注解的切点 切面中定义了通知：通知Executor与SimpleAsyncUncaughtExceptionHandler， 通知具体的实现类为AnnotationAsyncExecutionInterceptor，它的父类AsyncExecutionInterceptor进行了实际的通知处理操作 配置默认Executor 在生成切面AsyncAnnotationAdvisor对象时，生成了AnnotationAsyncExecutionInterceptor对象，调用了AnnotationAsyncExecutionInterceptor的configure(Supplier,Supplier)方法,在该方法中，调用了getDefaultExecutor(BeanFactory)来寻找默认Executor，查找Executor的优先级如下：
从BeanFactory中查找类型为TaskExecutor的对象 从BeanFactory中查找类型为Executor、Bean名称为taskExecutor的对象 如果上述步骤中找不到，那么子类AsyncExecutionInterceptor中生成SimpleAsyncTaskExecutor对象 通知的处理 通过determineAsyncExecutor(Method)方法查找AsyncExecutor 包装一下任务，当任务出现异常时调用AsyncUncaughtExceptionHandler的handleUncaughtException()处理异常 调用AsyncExecutor的submit()/submitListenable()/CompletableFuture.supplyAsync()等方法提交任务 查找AsyncExecutor AsyncExecutionAspectSupport的determineAsyncExecutor(Method)中查找了AsyncEexcutor，逻辑如下
首先尝试从成员变量Map&amp;lt;Method, AsyncTaskExecutor&amp;gt; executors查找是否存在，如果存在则返回 然后从AsyncExecutionAspectSupport.getExecutorQualifier()获取专属于该Method的AsyncExecutor的Bean名称，如果存在，则向BeanFactory获取类型为Executor、Bean名称为该名称的Executor并返回 从成员变量SingletonSupplier获取，如果存在则返回 如果经过上述几步查找仍然无法找到那么就返回空 如果经过上述几步找到了Executor，判断Executor的类型 如果是AsyncListenableTaskExecutor，将其强制转换为AsyncListenableTaskExecutor后放入到成员变量executors中 如果不是AsyncListenableTaskExecutor，通过TaskExecutorAdapter包装一个concurrentExecutor然后放入到成员变量executors中 异步事务 在@Async标注的方法，同时也适用了@Transactional进行了标注；在其调用数据库操作时，将无法产生事务管理的控制，原因就在于其是基于异步处理的操作。 那该如何给这些操作添加事务管理呢？可以将需要事务管理操作的方法放置到异步方法内部，在内部被调用的方法上添加@Transactional. 例如： 方法A，同时使用了@Async/@Transactional来标注，但是无法产生事务控制的目的。 方法B，使用了@Async来标注， B中调用了方法C、D，方法C、D分别使用@Transactional做了标注，则可实现事务控制的目的。</content></entry><entry><title>Netty架构简介</title><url>https://1162492411.github.io/docs/post/netty%E6%9E%B6%E6%9E%84%E7%AE%80%E4%BB%8B/</url><categories><category>Netty</category></categories><tags><tag>高性能组件</tag><tag>代码研究</tag></tags><content type="html"> Netty功能特性如下
1）传输服务：支持 BIO 和 NIO；
2）容器集成：支持 OSGI、JBossMC、Spring、Guice 容器；
3）协议支持：HTTP、Protobuf、二进制、文本、WebSocket 等一系列常见协议都支持。还支持通过实行编码解码逻辑来实现自定义协议；
4）Core 核心：可扩展事件模型、通用通信 API、支持零拷贝的 ByteBuf 缓冲对象。
高性能设计 Netty 作为异步事件驱动的网络，高性能之处主要来自于其 I/O 模型和线程处理模型，前者决定如何收发数据，后者决定如何处理数据
Netty采用的I/O模型为NIO,如下图
Netty采用的线程处理模型为Reactor模型.Reactor 模型中有 2 个关键组成：
1）Reactor：Reactor 在一个单独的线程中运行，负责监听和分发事件，分发给适当的处理程序来对 IO 事件做出反应。它就像公司的电话接线员，它接听来自客户的电话并将线路转移到适当的联系人；
2）Handlers：处理程序执行 I/O 事件要完成的实际事件，类似于客户想要与之交谈的公司中的实际官员。Reactor 通过调度适当的处理程序来响应 I/O 事件，处理程序执行非阻塞操作。
Reactor模型共有3个变种:单 Reactor 单线程、单 Reactor 多线程、主从 Reactor 多线程.
Netty的线程模型是基于主从 Reactors 多线程模型进行修改.
核心组件 Boostrap:客户端程序的启动引导类,主要作用是配置整个 Netty 程序，串联各个组件
ServerBootstrap:服务端启动引导类
ChannelEvent : 因为Netty是基于事件驱动的，ChannelEvent就相当于某一个事件，比如说连接成功时打印一句话
Channel:网络通信的组件，能够用于执行网络 I/O 操作,下面是一些常用的 Channel 类型：
NioSocketChannel，异步的客户端 TCP Socket 连接。 NioServerSocketChannel，异步的服务器端 TCP Socket 连接。 NioDatagramChannel，异步的 UDP 连接。 NioSctpChannel，异步的客户端 Sctp 连接。 NioSctpServerChannel，异步的 Sctp 服务器端连接，这些通道涵盖了 UDP 和 TCP 网络 IO 以及文件 IO。
Selector:通过 Selector 一个线程可以监听多个连接的 Channel 事件。当向一个 Selector 中注册 Channel 后，Selector 内部的机制就可以自动不断地查询Selector中注册的 Channel 是否有已就绪的 I/O 事件（例如可读，可写，网络连接完成等），这样程序就可以很简单地使用一个线程高效地管理多个 Channel
NioEventLoop:NioEventLoop 中维护了一个线程和任务队列，支持异步提交执行任务，线程启动时会调用 NioEventLoop 的 run 方法，执行 I/O 任务和非 I/O 任务
NioEventLoopGroup : 主要管理 eventLoop 的生命周期，可以理解为一个线程池，内部维护了一组线程，每个线程(NioEventLoop)负责处理多个 Channel 上的事件
ChannelHandler : 一个接口，处理 I/O 事件或拦截 I/O 操作，并将其转发到其 ChannelPipeline(业务处理链)中的下一个处理程序。
ChannelHandler 本身并没有提供很多方法，因为这个接口有许多的方法需要实现，方便使用期间，可以继承它的子类：
ChannelInboundHandler 用于处理入站 I/O 事件。 ChannelOutboundHandler 用于处理出站 I/O 操作。
或者使用以下适配器类：
ChannelInboundHandlerAdapter 用于处理入站 I/O 事件。 ChannelOutboundHandlerAdapter 用于处理出站 I/O 操作。 ChannelDuplexHandler 用于处理入站和出站事件。
ChannelPipline : 保存 ChannelHandler 的 List，用于处理或拦截 Channel 的入站事件和出站操作,可以理解为一种高级形式的拦截过滤器模式
ChannelHandlerContext : 保存 Channel 相关的所有上下文信息
组件间关系 当客户端和服务端连接的时候会建立一个 Channel,这个 Channel 我们可以理解为 Socket 连接，它负责基本的 IO 操作，例如：bind（），connect（），read（），write（） 等等,简单的说，Channel 就是代表连接，实体之间的连接，程序之间的连接，文件之间的连接，设备之间的连接。同时它也是数据入站和出站的载体。
EventLoopGroup、EventLoop、Channel关系如下
在 Netty 中每个 Channel 都有且仅有一个 ChannelPipeline 与之对应，它们的组成关系如下：
一个 Channel 包含了一个 ChannelPipeline，而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表，并且每个 ChannelHandlerContext 中又关联着一个 ChannelHandler。
入站事件和出站事件在一个双向链表中，入站事件会从链表 head 往后传递到最后一个入站的 handler，出站事件会从链表 tail 往前传递到最前一个出站的 handler，两种类型的 handler 互不干扰。
这些核心组件的整体关系如下
核心工作流程 典型的初始化并启动 Netty 服务端的过程代码如下：
public final class EchoServer { static final boolean SSL = System.getProperty(&amp;#34;ssl&amp;#34;) != null; static final int PORT = Integer.parseInt(System.getProperty(&amp;#34;port&amp;#34;, &amp;#34;8007&amp;#34;)); public static void main(String[] args) throws Exception { // 配置SSL final SslContext sslCtx; if (SSL) { SelfSignedCertificate ssc = new SelfSignedCertificate(); sslCtx = SslContextBuilder.forServer(ssc.certificate(), ssc.privateKey()).build(); } else { sslCtx = null; } // 配置服务端 EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); final EchoServerHandler serverHandler = new EchoServerHandler(); try { ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 100) .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer&amp;lt;SocketChannel&amp;gt;() { @Override public void initChannel(SocketChannel ch) throws Exception { ChannelPipeline p = ch.pipeline(); if (sslCtx != null) { p.addLast(sslCtx.newHandler(ch.alloc())); } //p.addLast(new LoggingHandler(LogLevel.INFO)); p.addLast(serverHandler); } }); // Start the server. ChannelFuture f = b.bind(PORT).sync(); // Wait until the server socket is closed. f.channel().closeFuture().sync(); } finally { // Shut down all event loops to terminate all threads. bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } } 基本过程描述如下：
1）初始化创建 2 个 NioEventLoopGroup：其中 boosGroup 用于 Accetpt 连接建立事件并分发请求，workerGroup 用于处理 I/O 读写事件和业务逻辑。
2）基于 ServerBootstrap(服务端启动引导类)：配置 EventLoopGroup、Channel 类型，连接参数、配置入站、出站事件 handler。
3）绑定端口：开始工作。
Netty启动流程图如下
结合上面介绍的 Netty Reactor 模型，介绍服务端 Netty 的工作架构图：
ps:上图中NioEventGroup有误,应为NioEventLoop</content></entry><entry><title>TopN问题解决</title><url>https://1162492411.github.io/docs/post/topn%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/</url><categories><category>数据库</category></categories><tags><tag>MySQL</tag><tag>TopN</tag></tags><content type="html"> 需求 将数据分组,每组内取前n条.最常见的需求是取每组内第一条,例如以imei分组,组内取time最新的一条
表结构 create table com( n_id int auto_increment primary key, c_imei varchar(10) null, c_time bigint null, c_name varchar(10) null ); create index com_c_imei_index on com (c_imei); 表数据 INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (1, 'a', 8, '010101'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (2, 'e', 2, '020202'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (3, 'c', 9, '030303'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (4, 'b', 4, '040404'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (5, 'd', 5, '050505'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (6, 'a', 6, '060606'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (7, 'e', 4, '070707'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (8, 'c', 3, '0808080'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (9, 'b', 5, '090909'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (10, 'd', 8, '101010'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (11, 'a', 5, '111111'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (12, 'e', 7, '121212'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (13, 'c', 2, '131313'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (14, 'b', 6, '141414'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (15, 'd', 9, '151515'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (16, 'a', 2, '161616'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (17, 'e', 1, '171717'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (18, 'c', 5, '181818'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (19, 'b', 8, '191919'); INSERT INTO com (n_id, c_imei, c_time, c_name) VALUES (20, 'd', 7, '202020'); SQL #方法一，自连接 SELECT a.c_imei, a.n_id, a.c_time FROM com a LEFT JOIN com b ON a.c_imei = b.c_imei AND a.c_time &amp;lt; b.c_time WHERE b.c_time IS NULL ORDER BY a.c_imei; #方法一的另一种形式,如果要取每组内前n条，那么将1改成n即可 SELECT n_id, c_imei, c_time, c_name FROM com a WHERE (SELECT count(*) FROM com b WHERE a.c_imei = b.c_imei AND a.c_time &amp;lt; b.c_time) &amp;lt; 1 order by c_imei; #方法二，派生表排序后分组，注意limit必须加不然没用 select n_id, c_imei, c_time, c_name from (select n_id, c_imei, c_time, c_name from com order by c_time desc limit 999999) a group by a.c_imei; #方法三,相关子查询，注意GROUP_CONCAT结果的长度受限于group_concat_max_len，默认1024 SELECT n_id, c_imei, c_time, c_name FROM com WHERE n_id IN (SELECT SUBSTRING_INDEX(GROUP_CONCAT(n_id ORDER BY c_time DESC), ',', 1) FROM com GROUP BY c_imei) ORDER BY c_imei; #方法四,派生表关联查询 select distinct com.n_id, com.c_imei, com.c_time, com.c_name from com join (select c_imei, max(c_time) as ct from com group by c_imei) tmp on com.c_imei = tmp.c_imei and com.c_time = tmp.ct order by com.c_imei; ##方法四优化 select distinct com.n_id, com.c_imei, com.c_time, com.c_name from com right join (select c_imei, max(c_time) as ct from com group by c_imei) tmp on com.c_imei = tmp.c_imei and com.c_time = tmp.ct order by com.c_imei; 其他方法 MySQL8及以上的row_number、rank、dense_rank、over函数</content></entry><entry><title>参数校验</title><url>https://1162492411.github.io/docs/post/%E5%8F%82%E6%95%B0%E6%A0%A1%E9%AA%8C/</url><categories><category>校验</category></categories><tags><tag>校验</tag><tag>Spring</tag><tag>HibernateValidate</tag></tags><content type="html"> 作为后台开发人员,为保证数据的有效性与完整性,避免处理前台传递的无效或不完整的信息,会进行后台的数据校验,常用的是Spring中的SpringValidation, 事实上它是对Hibernate Validator的封装,而Hibernate Validator又是对Bean Validation规范的实现,下面我们来较为全面的了解一下关于校验的那点事儿.
1.Bean Validation规范 Bean Validation规范主要用于对 Bean 中的字段的值进行约束定义、描述和验证,截止目前一共有三个版本 : Bean Validation 1.0/1.1/2.0,分别对应 JSR 303/349/380,有兴趣的同学可以到https://www.jcp.org/en/jsr/overview , 根据JSR编号查找相应的规范提案,关于Bean Validation各版本区别可前往https://beanvalidation.org查看。
2.Bean Validation实现 规范离不开相应的实现,Bean Validation的实现有两个 : Hibernate Validator与Apache BVal,Spring Validation就是基于Hibernate Validator封装的,它们都离不开javax Validation,公司也封装了framework-validation供大家使用。
这里介绍一下规范和实现之间的版本关系 :
Bean Validation 1.0 &amp;ndash;&amp;gt; Hibernate Validator 4.3.1与Apache BVal 0.5 Bean Validation 1.1 &amp;ndash;&amp;gt; Hibernate Validator 5.1.1与Apache BVal 1.1.2 Bean Validation 2.0 &amp;ndash;&amp;gt; Hibernate Validator 6.0.1 Bean Validation 主要提供了以下验证规则(javax.validation.constraints): 1)AssertFalse : 验证 Boolean 对象是否为 true 2)AssertTrue : 验证 Boolean 对象是否为 false 3)DecimalMax : 被标注的值必须不大于约束中指定的最大值(含精度) 4)DecimalMin : 被标注的值必须不小于约束中指定的最小值(含精度) 5)Digits : 验证 Number 和 String 的构成是否合法 6)Future : 验证 Date 和 Calendar 对象是否在当前时间之后 7)Max : 验证 Number 和 String 对象是否小等于指定的值 8)Min : 验证 Number 和 String 对象是否大等于指定的值 9)NotNull : 验证对象是否不为null 10)Null : 验证对象是否为null 11)Past : 验证 Date 和 Calendar 对象是否在当前时间之前 12)Pattern : 验证 Date 和 Calendar 对象是否在当前时间之后 13)Size : 验证CharSequence/Collection/Map/Array对象长度是否在给定的范围之内
Hibernate Validator在javax.validation的基础上增加了以下验证规则(org.hibernate.validator.constraints): 1)CreditCardNumber : 信用卡验证 2)EAN : 验证是否为EAN-13的商品用条码 3)Email : 邮箱地址验证 4)Length : 验证字符串长度 5)LuhnCheck : 验证是否符合模10算法的规则,例如大多数银行卡号编码规则采用了模10算法,前往https://en.wikipedia.org/wiki/Luhn_algorithm#cite_note-0 参考该算法 6)Mod10Check : 验证是否符合Mod10算法 7)Mod11Check : 验证是否符合Mod11算法 8)NotBlank : 检查约束字符串是不是Null还有被Trim的长度是否大于0,只对字符串,且会去掉前后空格 9)NotEmpty : 检查约束元素是否为NULL或者是EMPTY 10)ParameterScriptAssert : 使用脚本进行验证 11)Range : 校验数字或表示数字的字符串的取值范围 12)SafeHtml : 校验是否包含恶意脚本 13)ScriptAssert : 调用静态方法验证 14)URL : 校验是否是合法的URL
Spring Validation没有增加额外的验证规则,而是着重于通过BeanPostProcesser、Interceptor等在接收HTTP请求处理参数时进行参数校验,并封装了验证结果如BindingResult、Errors等,方便开发者使用。
3.Bean Validation实践 3.1 javax Validation原生基础用法 Maven地址
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;javax.validation&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;validation-api&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.1.0.Final&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 实体类
public class FlowLearning { @NotNull private Long id; //other properties } 校验方法
Validator validator = Validation.buildDefaultValidatorFactory().getValidator(); Set&amp;lt;ConstraintViolation&amp;lt;Object&amp;gt;&amp;gt; resultSet = validator.validate(flowLEarning,FlowLearning.class); if (!CollectionUtils.isEmpty(resultSet)) { throw new IllegalArgumentException(resultSet.toString()); } javax.validation.Validator为验证对象提供了三个方法 1)Set&amp;lt;ConstraintViolation&amp;gt; validate(T object, Class&amp;lt;?&amp;gt;&amp;hellip; groups) &amp;ndash;&amp;gt;验证一个给定的对象 2)Set&amp;lt;ConstraintViolation&amp;gt; validateProperty(T object, String propertyName, Class&amp;lt;?&amp;gt;&amp;hellip;groups) &amp;ndash;&amp;gt;验证给定对象中的字段或者属性 3)Set&amp;lt;ConstraintViolation&amp;gt; validateValue(ClassbeanType, String propertyName, Object value, Class&amp;lt;?&amp;gt;&amp;hellip; groups) &amp;ndash;&amp;gt;验证给定对象中的属性的具体值
3.2 spring validation 注解校验 略
3.3 @ScriptAssert校验复杂的业务逻辑 实体类
@Getter @Setter // @ScriptAssert的lang指脚本语言,script中的方法名需要完全限定名 @ScriptAssert(lang = &amp;quot;javascript&amp;quot;, script = &amp;quot;com.xdja.oa.nyingchi.admin.User.mockScriptAssert(_this.position,_this.amount)&amp;quot;) public class User { private String position; private Integer amount; public static boolean mockScriptAssert(String position,Integer amount){ if(StringUtils.isEmpty(position) || amount == null || amount &amp;lt;0){ return false; }else { return true; } } } 校验方法
@RequestMapping(value = &amp;quot;script&amp;quot;, method = RequestMethod.POST) public ResponseMsg scripts(@Validated @RequestBody User user, BindingResult bindingResult){ if(bindingResult.hasErrors()){ throw new IllegalArgumentException(&amp;quot;参数不合法&amp;quot;); }else{ //校验成功,处理业务逻辑 } return ResponseMsg.success(); } @ScriptAssert中的lang属性指的是哪种脚本语言,要查看当前jdk版本所支持的脚本语言,可以通过如下代码获取
ScriptEngineManager scriptEngineManager = new ScriptEngineManager(); List&amp;lt;ScriptEngineFactory&amp;gt; engineFactories = scriptEngineManager.getEngineFactories(); if(engineFactories.size() == 0) { System.out.println(&amp;quot;本JVM尚不支持任何脚本引擎&amp;quot;); return; } System.out.println(&amp;quot;本JVM支持的脚本引擎有:&amp;quot;); for(ScriptEngineFactory engineFactory : engineFactories) { System.out.println(&amp;quot;引擎名称:&amp;quot; + engineFactory.getEngineName()); System.out.println(&amp;quot;\t可被ScriptEngineManager识别的名称:&amp;quot; + engineFactory.getNames()); System.out.println(&amp;quot;\t该引擎支持的脚本语言名称:&amp;quot; + engineFactory.getLanguageName()); System.out.println(&amp;quot;\t是否线程安全:&amp;quot; + engineFactory.getParameter(&amp;quot;THREADING&amp;quot;)); } 3.4 原生自定义Validator 自定义注解
@Target( { METHOD, FIELD, ANNOTATION_TYPE }) @Retention(RUNTIME) @Documented @Constraint(validatedBy = CheckStringValidator.class) public @interface CheckString { String message() default &amp;quot;字符串校验失败！请少侠重新来过~&amp;quot;; Class&amp;lt;?&amp;gt;[] groups() default {}; Class&amp;lt;? extends Payload&amp;gt;[] payload() default {}; CheckType checkType() ; } 注解中的枚举
public enum CheckType { EMPTY,NOT_EMPTY } 注解校验器
public class CheckStringValidator implements ConstraintValidator&amp;lt;CheckString,String&amp;gt; { private CheckType checkType; @Override public void initialize(CheckString constraintAnnotation) { this.checkType = constraintAnnotation.checkType(); } @Override public boolean isValid(String string, ConstraintValidatorContext context) { if(string == null || checkType == null){ return false; }else{ boolean result = false; switch(checkType){ case NOT_EMPTY : result = !StringUtils.isEmpty(string); break; case EMPTY: result = StringUtils.isEmpty(string); break; default: break; } return result; } } } 实体类
@Getter @Setter public class User { @CheckString(checkType = CheckType.NOT_EMPTY) private String position; } 校验
@RequestMapping(value = &amp;quot;script&amp;quot;, method = RequestMethod.POST) public ResponseMsg scripts(@Valid @RequestBody User user, BindingResult bindingResult){ if(bindingResult.hasErrors()){ throw new IllegalArgumentException(&amp;quot;参数不合法&amp;quot;); }else{ System.out.println(&amp;quot;校验成功&amp;quot;); } } 3.5 校验模式 日常开发中进行的校验大多只要某字段校验失败就视为校验失败无需继续校验了，为此，可以设置校验模式为FastFail.
HibernateValidatorConfiguration configuration = Validation.byProvider( HibernateValidator.class ).configure(); ValidatorFactory factory = configuration.addProperty( &amp;quot;hibernate.validator.fail_fast&amp;quot;, &amp;quot;true&amp;quot; ).buildValidatorFactory(); Validator validator = factory.getValidator(); 级联验证目前使用较少,不再介绍。</content></entry><entry><title>汇报搜索优化历程</title><url>https://1162492411.github.io/docs/post/%E6%B1%87%E6%8A%A5%E6%90%9C%E7%B4%A2%E4%BC%98%E5%8C%96%E5%8E%86%E7%A8%8B/</url><categories><category>实战</category></categories><tags><tag>ES</tag><tag>搜索</tag></tags><content type="html"> 1.背景介绍 OA中存在工作汇报与汇报审批两个应用，前者用于员工填写汇报，如日报、周报、月报、会议纪要等，后者用于领导查阅员工填写的汇报，在查阅汇报时提供搜索功能，可根据关键字对汇报内容进行搜索。
1.1 表关系 搜索汇报相关的数据表表结构如下
t_report(汇报主表,存储员工填写的汇报记录,与t_report_value为一对多关系) 字段名 字段类型 字段说明 n_id bigint(20) 主键 n_account_id bigint(20) 人员id n_modify_time bigint(20) 修改时间 n_report_date bigint(20) 汇报日期 n_status bigint(20) 数据状态,0正常,1已删除 其他字段…. t_report_value(汇报子表,存储员工填写的汇报记录详情) 字段名 字段类型 字段说明 n_id bigint(20) 主键 n_report_id bigint(20) 汇报id，等同t_report的n_id n_moudle_id bigint(20) 模板id n_moudle_widget_id bigint(20) 模板的控件id c_widget_value varchar(12380) 汇报的内容 其他字段…. t_report_moudle(汇报模板表,存储汇报使用的模板,与t_report_widget为一对多关系) n_id bigint(20) 主键 c_name varchar(255) 模板名称 其他字段… t_report_widget(汇报控件表,存储汇报使用的模板中的控件) n_id bigint(20) 主键 n_moudle_id bigint(20) 模板id,等同t_report_moudle的n_id c_title varchar(20) 控件名,如标题、内容、本周总结、本月计划 n_value_limt int(8) 控件值长度限制,如标题最大长度、内容最大长度 其他字段…. 1.2 数据增长速度 模板表与模板控件表的数量增长较慢，数据增长主要为汇报主表与汇报子表，每天两表的数据增长速度大致如下
t_report：1 * 员工数
t_report_value : 1 * 员工数 * 模板数 * 控件数
1.3 搜索流程 搜索功能流程如下：
A接收请求
B查询当前人管辖的人员列表
C查询当前人能够查看的模板id列表
D将前两步的结果、关键词、分页参数等一起作为条件，搜索符合条件的汇报
E包装汇报的其他数据(如汇报的浏览数量、汇报的附件数量)
F返回数据
2.阶段A – MySQL like查询 在应用运行初期，由于汇报数据少，数据增长速度慢，且对搜索接口未提出其他方面的要求，因此采用like模糊查询符合条件的汇报数据，核心SQL如下
SELECT
DISTINCT t1.n_id AS id,
t1.n_create_time AS createTime,
t1.n_modify_time AS modifyTime,
t1.n_account_id AS accountId,
t1.c_coordinate AS coordinate,
t1.c_at_ids AS atIds,
t1.n_report_date AS reportDate
FROM
t_report t1
LEFT JOIN t_report_value brmwv ON t1.n_id = brmwv.n_report_id
WHERE
brmwv.c_widget_value like &amp;lsquo;%:1%&amp;rsquo;
AND brmwv.n_employee_id IN(:2)
AND brmwv.n_moudle_id IN(:3)
AND t1.n_delete_flag = 0
AND t1.n_modify_time &amp;lt; :4
ORDER BY
t1.n_report_date DESC,
t1.n_modify_time DESC LIMIT 0, :5
##:1为关键词,:2为人员列表,:3为模块列表,:4为分页参数,:5为分页条数
方案优点：无需额外改动
方案缺点：数据量多时效率低
3.阶段B - MySQL 全文索引 以公司环境为例，应用运行一年后，汇报主表的数据量大约为33w条(330天* 1000员工),汇报子表的数据量为33w条(330天 * 1000 员工 * 1个模板 * 1个控件),这时候搜索接口的查询汇报SQL平均速度为10S+，like搜索方案的主要瓶颈在于like搜索进行全表扫描，于是考虑在c_widget_value字段中建立索引来提高搜索速度，由于c_widget_value大部分为中文字符,因此需要在该字段建立全文索引并支持对中文的搜索。
经查阅资料，MySQL中的全文索引自v5.6.24开始支持InnoDB引擎，自v5.7开始增加ngram分词器以支持中日韩文，全文索引支持的数据库字段类型为char、varchar、text，于是此方案在MySQL中执行以下语句即可:
create fulltext index vfin on t_ report _value (c_widget_value) with parser ngram;
建立全文索引后SQL需要进行相应的改写，使用match against ，改写后的SQL如下
SELECT
DISTINCT t1.n_id AS id,
t1.n_create_time AS createTime,
t1.n_modify_time AS modifyTime,
t1.n_account_id AS accountId,
t1.c_coordinate AS coordinate,
t1.c_at_ids AS atIds,
t1.n_report_date AS reportDate
FROM
t _report t1
LEFT JOIN t_ report_ value brmwv ON t1.n_id = brmwv.n_report_id
WHERE
match(c_widget_value) against (':1&amp;rsquo;)
AND brmwv.n_employee_id IN(:2)
AND brmwv.n_moudle_id IN(:3)
AND t1.n_delete_flag = 0
AND t1.n_modify_time &amp;lt; :4
ORDER BY
t1.n_report_date DESC,
t1.n_modify_time DESC LIMIT 0, :5
##:1为关键词,:2为人员列表,:3为模块列表,:4为分页参数,:5为分页条数
在使用该方案时发现接口整体速度确实有了提升，但是当输入的关键词为单个字符或两个字符时无法查询到数据，于是继续查阅相关资料，得到以下信息：MySQL中的innodb_ft_min_token_size配置项表示全文索引最小分词长度,该值默认为3。于是将该配置项的值修改为1，重建了全文索引并重启MySQL，再次搜索时输入任意个字符均可搜索到相关数据。
方案时间：SQL平均时间2s，整体接口平均时间8s
方案优点：SQL查询效率提高
方案缺点：建立了索引额外占据了空间、对该表的CRUD都将降低响应速度、ngram分词粒度越细那么占据空间越大、修改MySQL的全文索引配置项后需要重建全文索引并重启MySQL才能生效、输入的关键词长度增加时SQL响应速度呈指数级增长。
4.阶段C - ElasticSearch + MySQL 全文索引方案与like方案相比，的确提升了SQL的响应速度，但是SQL响应速度受关键词影响极大，若输入的关键词长度过长，或输入的关键词几乎匹配了数据库中绝大部分数据，那么该接口的整体响应速度仍然堪忧；建立全文索引后，对t_ report_ value进行操作时响应速度将会有所降低；数据进一步增长时SQL速度将进一步变慢，在百万级以上时表现不佳；ngram分词器的分词规则不够灵活，导致分词后的索引占据空间很大。
一想到大数据量秒级响应，那么ElasticSearch会作为首选项。加上搜索接口对数据实时性要求不高，因此可以将汇报主表与汇报子表的数据存储在ElasticSearch中，对c_widget_value使用ik_smart进行分词存储，并定时更新ES数据，查询时从ES查询数据，然后再进行接口内其他业务操作。
由于ES并不擅长关联操作，于是该方案设计为OA后台执行定时任务，将汇报主表与汇报子表的数据增量整合为一张表t_report_sync_data,再通过LogStash将MySQL中表t_report_sync_data的数据增量同步到ES中。
记表t_ report为表A(汇报主表), t_report_value为表B(汇报子表), t _report_moudle(汇报模板表)为 C，以下是t_report_sync_data中各字段与这些表的对应关系
字段名 原始表 原始字段 说明 reportId report n_id 汇报id createTime report n_create_time 汇报创建时间 modifyTime report n_modify_time 汇报修改时间 accountId report n_account_id 人员id coordinate report c_coordinate 坐标 atds report c_at_ids 艾特的人员id集合 deleteFlag report n_delete_flag 删除标识 companyId report n_company_id 企业id reportDate report n_report_date 汇报日期 moduleId reportValue n_module_widget_id 模块id widgetValue reportValue c_widget_value 控件值 那么查询时的SQL就改变为了ES的查询语句，Java代码如下
// 条件构建
BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();
//widgetValue,模糊查询
BoolQueryBuilder builder = QueryBuilders.boolQuery();
builder.should(QueryBuilders.matchPhraseQuery(&amp;ldquo;widgetValue&amp;rdquo;, queryBean.getWidgetValue()));
boolQueryBuilder.must(builder);
//accountId,in
boolQueryBuilder.filter(inParamBuilder(queryBean.getAccountIds(), &amp;ldquo;accountId&amp;rdquo;));
//moudleId, in
boolQueryBuilder.filter(inParamBuilder(queryBean.getMoudleIds(), &amp;ldquo;moudleId&amp;rdquo;));
//deleteFlag,0
boolQueryBuilder.filter(QueryBuilders.termQuery(&amp;ldquo;deleteFlag&amp;rdquo;, 0));
//modifyTime,&amp;lt;
RangeQueryBuilder rangeQuery = QueryBuilders.rangeQuery(&amp;ldquo;modifyTime&amp;rdquo;);
rangeQuery.lt(queryBean.getModifyTime());
boolQueryBuilder.filter(rangeQuery);
//聚合请求构建
//按reportId分桶,按reportDate降序,按modifyTime降序,取前n桶 TermsBuilder termsBuilder = AggregationBuilders .terms(&amp;ldquo;group_by_reportId&amp;rdquo;) .field(&amp;ldquo;reportId&amp;rdquo;) .subAggregation(AggregationBuilders.max(&amp;ldquo;sortA&amp;rdquo;).field(&amp;ldquo;reportDate&amp;rdquo;)) .subAggregation(AggregationBuilders.max(&amp;ldquo;sortB&amp;rdquo;).field(&amp;ldquo;modifyTime&amp;rdquo;)) . order(Terms.Order.compound(Terms.Order.aggregation(&amp;ldquo;sortA&amp;rdquo;,false),Terms.Order.aggregation(&amp;ldquo;sortB&amp;rdquo;,false))) .size(queryBean.getPageSize());
//设置每组内取一条数据
TopHitsBuilder hitsBuilder = AggregationBuilders.topHits(&amp;ldquo;groupDataDetail&amp;rdquo;).setSize(1);
//每组内, 设置查询的字段
hitsBuilder.setFetchSource(WORK_REPORT_FIELDS, null);
termsBuilder.subAggregation(hitsBuilder);
//将分组挂靠在查询请求内,size设置es hit的原始数据,由于业务系统一般不需要,故设置不返回此项
requestBuilder.addAggregation(termsBuilder).setSize(0);
//设置Query并获取响应
SearchResponse searchResponse = requestBuilder.setQuery(boolQueryBuilder).execute().actionGet();
//处理响应,略
//inParamBuilder方法如下，用于解决同一个字段的terms的参数过多问题
private QueryBuilder inParamBuilder(List list, String field) { int count = 800; int len = list.size(); int size = len % count == 0 ? len / count : (len / count) + 1; BoolQueryBuilder shouldQuery = QueryBuilders.boolQuery(); for (int i = 0; i &amp;lt; size; i++) { int fromIndex = i * count; int toIndex = Math.min(fromIndex + count, len); List subList = list.subList(fromIndex, toIndex); TermsQueryBuilder termsQueryBuilder = QueryBuilders.termsQuery(field, subList); shouldQuery.should(termsQueryBuilder); } return shouldQuery; }
ES的DSL语句如下
{
&amp;ldquo;size&amp;rdquo; : 0,
&amp;ldquo;query&amp;rdquo; : {
&amp;ldquo;bool&amp;rdquo; : {
&amp;ldquo;must&amp;rdquo; : {&amp;ldquo;bool&amp;rdquo; : {&amp;ldquo;should&amp;rdquo; : {&amp;ldquo;match&amp;rdquo; : {&amp;ldquo;widgetValue&amp;rdquo; : {&amp;ldquo;query&amp;rdquo; : &amp;ldquo;工作汇报&amp;rdquo;,&amp;ldquo;type&amp;rdquo; : &amp;ldquo;phrase&amp;rdquo;}}}}},
&amp;ldquo;filter&amp;rdquo; : [
​ {&amp;ldquo;bool&amp;rdquo; : {&amp;ldquo;should&amp;rdquo; : {&amp;ldquo;terms&amp;rdquo; : {&amp;ldquo;moudleId&amp;rdquo; : [ 1, 2, 3, 4, 5 ]}}}},
​ {&amp;ldquo;term&amp;rdquo; : {&amp;ldquo;deleteFlag&amp;rdquo; : 0}},
​ {&amp;ldquo;range&amp;rdquo; : {&amp;ldquo;modifyTime&amp;rdquo; : {&amp;ldquo;from&amp;rdquo; : null,&amp;ldquo;to&amp;rdquo; : 1554180590383,&amp;ldquo;include_lower&amp;rdquo; : true,&amp;ldquo;include_upper&amp;rdquo; : false}} } ]}
},
&amp;ldquo;aggregations&amp;rdquo; : {
&amp;ldquo;group_by_reportId&amp;rdquo; : {
&amp;ldquo;terms&amp;rdquo; : {
​ &amp;ldquo;field&amp;rdquo; : &amp;ldquo;reportId&amp;rdquo;,&amp;ldquo;size&amp;rdquo; : 200,&amp;ldquo;order&amp;rdquo; : [ {&amp;ldquo;sortA&amp;rdquo; : &amp;ldquo;desc&amp;rdquo;}, {&amp;ldquo;sortB&amp;rdquo; : &amp;ldquo;desc&amp;rdquo;}, {&amp;quot;_term&amp;rdquo; : &amp;ldquo;asc&amp;rdquo;} ]},
&amp;ldquo;aggregations&amp;rdquo; : {
​ &amp;ldquo;sortA&amp;rdquo; : {&amp;ldquo;max&amp;rdquo; : {&amp;ldquo;field&amp;rdquo; : &amp;ldquo;reportDate&amp;rdquo;}},
​ &amp;ldquo;sortB&amp;rdquo; : {&amp;ldquo;max&amp;rdquo; : {&amp;ldquo;field&amp;rdquo; : &amp;ldquo;modifyTime&amp;rdquo;}},
​ &amp;ldquo;groupDataDetail&amp;rdquo; : {&amp;ldquo;top_hits&amp;rdquo; : {&amp;ldquo;size&amp;rdquo; : 1, &amp;ldquo;_source&amp;rdquo; : {&amp;ldquo;includes&amp;rdquo; : [ &amp;ldquo;accountId&amp;rdquo;, &amp;ldquo;atIds&amp;rdquo;, &amp;ldquo;companyId&amp;rdquo;, &amp;ldquo;coordinate&amp;rdquo;, &amp;ldquo;createTime&amp;rdquo;, &amp;ldquo;deleteFlag&amp;rdquo;, &amp;ldquo;modifyTime&amp;rdquo;, &amp;ldquo;moudleId&amp;rdquo;, &amp;ldquo;reportDate&amp;rdquo;, &amp;ldquo;widgetValue&amp;rdquo; ],
​ &amp;ldquo;excludes&amp;rdquo; : [ ]} }}}}}}
经测试，该方案中ES环节所需时间稳定在0.8s左右，接口整体速度在1-6s。
优点：速度进一步提升且响应时间比较稳定
缺点：汇报相关的其他数据仍存储在MySQL中，整体接口瓶颈变为查询汇报其他数据时的速度过慢。
5.阶段D - ElasticSearch + Redis + MySQL 对整体接口相关数据进一步分析，根据数据修改频繁程度，可以将数据进行冷热分离，将修改频率较低的数据,如汇报主表与汇报子表，存储在ES中，并通过LogStash定时增量更新；将高频修改数据(如汇报的浏览数量)存储在Redis中，这样将会进一步提升搜索的响应速度。</content></entry><entry><title>MySQL全文索引使用</title><url>https://1162492411.github.io/docs/post/mysql%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95/</url><categories><category>MySQL</category></categories><tags><tag>MySQL</tag><tag>数据库</tag><tag>全文索引</tag></tags><content type="html"> 1.简介 在Web应用中,经常会遇到按照关键字进行模糊搜索的需求,当参数搜索的数据量较少时,我们一般使用like进行搜索,但是当数据量达到一定程度后,like方式的速度就会很慢很慢,这时候我们可以借助一些全文搜索的组件来实现需求.MySQL就提供了全文索引来支持模糊搜索.
2.限制条件 2.1引擎限制 MySQL 5.6 以前的版本，只有 MyISAM 存储引擎支持全文索引；
MySQL 5.6 及以后的版本，MyISAM 和 InnoDB 存储引擎均支持全文索引
2.2版本号限制 Mysql自v5.6.24版本开始在InnoDB引擎中增加全文索引功能，支持对英文的全文搜索,默认以空格作为分隔符;自v5.7版本开始增加ngram分词器以支持中文
2.3字段类型限制 全文索引支持的字段类型为char、varchar、text等这些基于文本的列
2.4连表限制 全文搜索仅支持在同一张表中进行,不支持对多张表中的关键字进行全文搜索
3.准备索引 我们以report表为例
-- 准备表 create table report ( id int auto_increment primary key, content varchar(1000) null ); -- 在content字段创建普通的全文索引 create fulltext index content_fti on report(content); -- 在content字段创建支持中文的全文索引 create fulltext index content_fti on report(content) WITH PARSER ngram; -- 删除索引,方式一 drop index content_fti on report; -- 删除索引,方式二 alter table report drop index content_fti; 4.准备配置 ​ 使用全文索引搜索时,搜索引擎受全文搜索的单词长度影响,如果关键词长度小于该配置项,那么将无法搜索出相匹配的结果,通过命令可以查看出相关配置项
-- 查看全文搜索配置 show variables like &amp;#39;%ft%&amp;#39;; -- 命令执行结果 // MyISAM:关键词最小长度默认4字符,最大长度84字符 ft_min_word_len = 4; ft_max_word_len = 84; // InnoDB:关键词最小长度默认3字符,最大长度84字符 innodb_ft_min_token_size = 3; innodb_ft_max_token_size = 84; 我们以常用的Innodb引擎为例,在MySQL的配置文件中修改配置项
[mysqld] innodb_ft_min_token_size = 1 ft_min_word_len = 1 修改后需要重启MySQL,然后修复全文索引(可以删除索引然后重新建立索引,如果是MyIsam引擎,也可以执行repair命令修复)
然而对于使用了ngram的全文索引来讲,它的全文搜索单词长度配置会忽略上述四个配置项,真正生效的为配置项ngram_token_size(默认2),可以通过在MySQL的配置文件中修改以下配置项或启动时追加参数&amp;ndash;ngram_token_size=1来实现对该配置项的修改
[mysqld] ngram_token_size=1 同样的,修改此项后需要重建全文索引
5.准备数据 略
6.使用索引 与like不同,全文索引的搜索需要使用match agnist,示例如下
select * from report where match(content) against('测试关键词'); match agnist本身还会返回非负浮点数作为搜索的结果行与关键词的相关度.除了match agnist的基础使用,全文搜索还支持以不同的检索模式进行搜索,常用的全文检索模式有两种： 1、自然语言模式(NATURAL LANGUAGE MODE) ， 自然语言模式是MySQL 默认的全文检索模式。自然语言模式不能使用操作符，不能指定关键词必须出现或者必须不能出现等复杂查询。当sql中指定了IN NATURAL LANGUAGE MODE修饰符或未给出修饰符，则全文搜索是自然语言搜索模式 。 2、BOOLEAN模式(BOOLEAN MODE) BOOLEAN模式可以使用操作符，可以支持指定关键词必须出现或者必须不能出现或者关键词的权重高还是低等复杂查询。
6.1自然语言检索模式 ​ 在该模式下,可以指定IN NATURAL LANGUAGE MOD,也可以不指定修饰符,下面给出一个按照结果行相关度倒序排列的SQL示例
select *,match(content) against(&amp;#39;一切&amp;#39;) as score from report where match(content) against(&amp;#39;一切&amp;#39;) order by score desc; 6.2布尔检索模式 MySQL可以使用IN BOOLEAN MODE修饰符执行布尔型全文本搜索 。在这种模式下,支持通过一些正则来进行高级搜索,布尔模式下支持以下操作符：
“+”表示必须包含 “-”表示必须排除 “&amp;gt;”表示出现该单词时增加相关性 “&amp;lt;”表示出现该单词时降低相关性 “*”表示通配符 “~”允许出现该单词，但是出现时相关性为负 “&amp;quot;&amp;quot;”表示短语 下面给出一些示例 'apple banana' ## 无操作符，表示或，要么包含apple，要么包含banana '+apple +juice' ## 必须同时包含两个词apple和juice '+apple macintosh' ## 必须包含apple，但是如果也包含macintosh的话，相关性会更高。 '+apple -macintosh' ## 必须包含apple，同时不能包含macintosh。 '+apple ~macintosh' ## 必须包含apple，但是如果也包含macintosh的话，相关性要比不包含macintosh的记录低。 '+apple +(&amp;gt;juice &amp;lt;pie)' ## 查询必须包含apple和juice或者apple和pie的记录，但是apple juice的相关性要比apple pie高。 'apple*' ## 查询包含以apple开头的单词的记录，如apple、apples、applet。 '&amp;quot;some words&amp;quot;' ## 使用双引号把要搜素的词括起来，效果类似于like '%some words%'， 例如“some words of wisdom”会被匹配到，而“some noise words”就不会被匹配。 7.InnoDB引擎的相关性 InnoDB引擎的全文索引基于Sphinx,算法基于BM-25和TF-IDF,InnoDB使用“术语频率-逆文档频率” （TF-IDF）加权系统的变体对给定的全文搜索查询对文档的相关性进行排名,单词出现在文档中的频率越高，单词出现在文档集合中的频率越低，文档的排名就越高。
7.1相关性排名的计算方式 术语频率（TF）值是单词在文档中出现的次数。IDF单词的逆文档频率（）值是使用以下公式计算的，其中 total_records是集合中matching_records的记录数，并且是搜索词出现的记录数。
${IDF} = log10( ${total_records} / ${matching_records} ) 当文档多次包含一个单词时，IDF值将乘以TF值：
${TF} * ${IDF} 使用TF和IDF 值，使用以下公式计算文档的相关性等级：
${rank} = ${TF} * ${IDF} * ${IDF} 8.停止词 可以通过配置停止词来禁止某些词语参与全文索引,详细使用见全文停用词
9.InnoDB分词原理 InnoDB 全文索引具有倒排索引设计。倒排索引存储一个单词列表，对于每个单词，存储单词出现的文档列表。为了支持邻近搜索，每个单词的位置信息也作为字节偏移量存储。
创建全文索引时,MySQL将创建一组表用于辅助
## 查看索引表 SELECT table_id, name, space from INFORMATION_SCHEMA.INNODB_SYS_TABLES WHERE name LIKE 'test/%'; ## 命令执行结果 424 test/FTS_000000000000006b_0000000000000388_INDEX_1 423 425 test/FTS_000000000000006b_0000000000000388_INDEX_2 424 426 test/FTS_000000000000006b_0000000000000388_INDEX_3 425 427 test/FTS_000000000000006b_0000000000000388_INDEX_4 426 428 test/FTS_000000000000006b_0000000000000388_INDEX_5 427 429 test/FTS_000000000000006b_0000000000000388_INDEX_6 428 430 test/FTS_000000000000006b_BEING_DELETED 429 431 test/FTS_000000000000006b_BEING_DELETED_CACHE 430 432 test/FTS_000000000000006b_CONFIG 431 433 test/FTS_000000000000006b_DELETED 432 434 test/FTS_000000000000006b_DELETED_CACHE 433 107 test/report 93 前六个表代表反向索引，并称为辅助索引表。对传入文档进行标记时，各个单词（也称为 “标记”）与位置信息和关联的文档ID（DOC_ID）一起插入索引表中。根据单词第一个字符的字符集排序权重，单词在六个索引表中得到完全排序和分区。
倒排索引分为六个辅助索引表，以支持并行索引创建。默认情况下，两个线程对索引表中的单词和相关数据进行标记化，排序和插入。线程数可以使用该innodb_ft_sort_pll_degree 选项配置 。FULLTEXT在大型表上创建索引时，请考虑增加线程数 。
辅助索引表名称以前缀 FTS_和后缀 INDEX_*。每个索引表通过索引表名称中与table_id索引表的匹配的十六进制值与索引表相关联。例如，table_id所述的 test/opening_lines表是 327，为此，十六进制值是0x147。如前面的示例所示，十六进制值“ 147 ”出现在与该test/opening_lines表关联的索引表的名称中。</content></entry><entry><title>Docker MySQL部署</title><url>https://1162492411.github.io/docs/post/docker-mysql%E9%83%A8%E7%BD%B2/</url><categories><category>MySQL</category></categories><tags><tag>MySQL</tag><tag>Docker</tag></tags><content type="html"> 前提 安装docker,mac环境下可直接安装docker Desktop
拉取 #拉取5.7版本的mysql镜像 docker push mysql:5.7
运行 docker run -p 13306:3306 \ --name d-mysql-57 \ -e MYSQL_ROOT_PASSWORD=Mo20100528 \ -v /Users/zyg/softs/docker/mysql57/data:/var/lib/mysql \ -v /Users/zyg/softs/docker/mysql57/logs:/var/log/mysql \ -v /Users/zyg/softs/docker/mysql57/conf/my.cnf:/etc/my.cnf \ -d mysql:5.7 参数说明:
run　run 是运行一个容器 -d　表示后台运行 -p　表示容器内部端口和服务器端口映射关联 &amp;ndash;privileged=true　设值MySQL 的root用户权限, 否则外部不能使用root用户登陆 -v 容器内的路径(如/etc/mysql)挂载到宿主机 -e MYSQL_ROOT_PASSWORD=xxx 设置MySQL数据库root用户的密码 &amp;ndash;name 设值容器名称为mysql mysql:5.7 表示从docker镜像mysql:5.7中启动一个容器 &amp;ndash;character-set-server=utf8mb4 &amp;ndash;collation-server=utf8mb4_general_ci 设值数据库默认编码 停止上面启动的容器,容器名字为&amp;quot;d-mysql-57&amp;rdquo;
docker stop d-mysql-57 配置账户 ##进入容器
docker exec -it d-mysql-57 bash ##登录MySQL
``mysql -uroot -p` ##创建用户,名叫test,密码是test123,开启远程访问权限
GRANT ALL PRIVILEGES ON *.* TO 'test'@'%' IDENTIFIED BY 'test123' WITH GRANT OPTION; ##创建数据库,名叫xxx
create database xxx; 之后便可以通过该用户执行业务脚本</content></entry></search>